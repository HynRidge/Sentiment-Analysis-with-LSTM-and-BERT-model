{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SETUP**"
      ],
      "metadata": {
        "id": "5zXJrSMo495u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "8_7Yx7y-5BOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tYF42jlo5Q4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0d9185-4622-4bb2-a519-6f31b1cd2c92"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSD-dTWt28EH",
        "outputId": "842f35bd-32e7-4926-86eb-fa8697e95949"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import spacy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eTERBhPf5Ykk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import strftime, localtime\n",
        "from transformers import BertTokenizer\n",
        "from sklearn import metrics\n",
        "from xml.etree.ElementTree import parse\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "ZEPQFq-e6gVp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "6674vF4n6le1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASSES**"
      ],
      "metadata": {
        "id": "IDenOuxLpJFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**"
      ],
      "metadata": {
        "id": "USX4iChw67d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, fname, tokenizer):\n",
        "        fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "        lines = fin.readlines()\n",
        "        fin.close()\n",
        "        fin = open(fname+'.graph', 'rb')\n",
        "        idx2graph = pickle.load(fin)\n",
        "        fin.close()\n",
        "\n",
        "        all_data = []\n",
        "        for i in range(0, len(lines), 3):\n",
        "            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "            aspect = lines[i + 1].lower().strip()\n",
        "            polarity = lines[i + 2].strip()\n",
        "\n",
        "            text_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
        "            context_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
        "            left_indices = tokenizer.text_to_sequence(text_left)\n",
        "            left_with_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect)\n",
        "            right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
        "            right_with_aspect_indices = tokenizer.text_to_sequence(aspect + \" \" + text_right, reverse=True)\n",
        "            aspect_indices = tokenizer.text_to_sequence(aspect)\n",
        "            left_len = np.sum(left_indices != 0)\n",
        "            aspect_len = np.sum(aspect_indices != 0)\n",
        "            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype=np.int64)\n",
        "            polarity = int(polarity) + 1\n",
        "\n",
        "            text_len = np.sum(text_indices != 0)\n",
        "            concat_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\")\n",
        "            concat_segments_indices = [0] * (text_len + 2) + [1] * (aspect_len + 1)\n",
        "            concat_segments_indices = pad_and_truncate(concat_segments_indices, tokenizer.max_seq_len)\n",
        "\n",
        "            text_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text_left + \" \" + aspect + \" \" + text_right + \" [SEP]\")\n",
        "            aspect_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n",
        "\n",
        "            dependency_graph = np.pad(idx2graph[i], \\\n",
        "                ((0,tokenizer.max_seq_len-idx2graph[i].shape[0]),(0,tokenizer.max_seq_len-idx2graph[i].shape[0])), 'constant')\n",
        "\n",
        "            data = {\n",
        "                'concat_bert_indices': concat_bert_indices,\n",
        "                'concat_segments_indices': concat_segments_indices,\n",
        "                'text_bert_indices': text_bert_indices,\n",
        "                'aspect_bert_indices': aspect_bert_indices,\n",
        "                'text_indices': text_indices,\n",
        "                'context_indices': context_indices,\n",
        "                'left_indices': left_indices,\n",
        "                'left_with_aspect_indices': left_with_aspect_indices,\n",
        "                'right_indices': right_indices,\n",
        "                'right_with_aspect_indices': right_with_aspect_indices,\n",
        "                'aspect_indices': aspect_indices,\n",
        "                'aspect_boundary': aspect_boundary,\n",
        "                'dependency_graph': dependency_graph,\n",
        "                'polarity': polarity,\n",
        "            }\n",
        "\n",
        "            all_data.append(data)\n",
        "        self.data = all_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "julOof-E-0Ry"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, max_seq_len, lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 1\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unknownidx = len(self.word2idx)+1\n",
        "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)"
      ],
      "metadata": {
        "id": "t_BKpmMdqNuW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split()\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "H3kxztDgDc2t"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "fuiAR0ng6vID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n",
        "                 bidirectional=False, only_use_last_hidden_state=False, rnn_type = 'LSTM'):\n",
        "        \"\"\"\n",
        "        LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, length...).\n",
        "        :param input_size:The number of expected features in the input x\n",
        "        :param hidden_size:The number of features in the hidden state h\n",
        "        :param num_layers:Number of recurrent layers.\n",
        "        :param bias:If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "        :param batch_first:If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "        :param dropout:If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
        "        :param bidirectional:If True, becomes a bidirectional RNN. Default: False\n",
        "        :param rnn_type: {LSTM, GRU, RNN}\n",
        "        \"\"\"\n",
        "        super(DynamicLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.batch_first = batch_first\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.only_use_last_hidden_state = only_use_last_hidden_state\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        if self.rnn_type == 'LSTM': \n",
        "            self.RNN = nn.LSTM(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)  \n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.RNN = nn.GRU(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
        "        elif self.rnn_type == 'RNN':\n",
        "            self.RNN = nn.RNN(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack ->process using RNN -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
        "        \n",
        "        # process using the selected RNN\n",
        "        if self.rnn_type == 'LSTM': \n",
        "            out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n",
        "        else: \n",
        "            out_pack, ht = self.RNN(x_emb_p, None)\n",
        "            ct = None\n",
        "        \"\"\"unsort: h\"\"\"\n",
        "        ht = torch.transpose(ht, 0, 1)[\n",
        "            x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n",
        "        ht = torch.transpose(ht, 0, 1)\n",
        "\n",
        "        if self.only_use_last_hidden_state:\n",
        "            return ht\n",
        "        else:\n",
        "            \"\"\"unpack: out\"\"\"\n",
        "            out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "            out = out[0]  #\n",
        "            out = out[x_unsort_idx]\n",
        "            \"\"\"unsort: out c\"\"\"\n",
        "            if self.rnn_type =='LSTM':\n",
        "                ct = torch.transpose(ct, 0, 1)[\n",
        "                    x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n",
        "                ct = torch.transpose(ct, 0, 1)\n",
        "\n",
        "            return out, (ht, ct)"
      ],
      "metadata": {
        "id": "qQsLY_w-8Bep"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ATAE_LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(ATAE_LSTM, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.squeeze_embedding = SqueezeEmbedding()\n",
        "        self.lstm = DynamicLSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=1, batch_first=True)\n",
        "        self.attention = NoQueryAttention(config2[\"hidden_dim\"]+config2[\"embed_dim\"], score_function='bi_linear')\n",
        "        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        text_indices, aspect_indices = inputs[0], inputs[1]\n",
        "        x_len = torch.sum(text_indices != 0, dim=-1)\n",
        "        x_len_max = torch.max(x_len)\n",
        "        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n",
        "\n",
        "        x = self.embed(text_indices)\n",
        "        x = self.squeeze_embedding(x, x_len)\n",
        "        aspect = self.embed(aspect_indices)\n",
        "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n",
        "        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n",
        "        x = torch.cat((aspect, x), dim=-1)\n",
        "\n",
        "        h, (_, _) = self.lstm(x, x_len)\n",
        "        ha = torch.cat((h, aspect), dim=-1)\n",
        "        _, score = self.attention(ha)\n",
        "        output = torch.squeeze(torch.bmm(score, h), dim=1)\n",
        "\n",
        "        out = self.dense(output)\n",
        "        return out"
      ],
      "metadata": {
        "id": "V3WeKKJK8ES1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vDMVKKkHmnJP"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):\n",
        "        ''' Attention Mechanism\n",
        "        :param embed_dim:\n",
        "        :param hidden_dim:\n",
        "        :param out_dim:\n",
        "        :param n_head: num of head (Multi-Head Attention)\n",
        "        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)\n",
        "        :return (?, q_len, out_dim,)\n",
        "        '''\n",
        "        super(Attention, self).__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = embed_dim // n_head\n",
        "        if out_dim is None:\n",
        "            out_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_head = n_head\n",
        "        self.score_function = score_function\n",
        "        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if score_function == 'mlp':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        else:  # dot_product / scaled_dot_product\n",
        "            self.register_parameter('weight', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.hidden_dim)\n",
        "        if self.weight is not None:\n",
        "            self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, q):\n",
        "        if len(q.shape) == 2:  # q_len missing\n",
        "            q = torch.unsqueeze(q, dim=1)\n",
        "        if len(k.shape) == 2:  # k_len missing\n",
        "            k = torch.unsqueeze(k, dim=1)\n",
        "        mb_size = k.shape[0]  # ?\n",
        "        k_len = k.shape[1]\n",
        "        q_len = q.shape[1]\n",
        "        # k: (?, k_len, embed_dim,)\n",
        "        # q: (?, q_len, embed_dim,)\n",
        "        # kx: (n_head*?, k_len, hidden_dim)\n",
        "        # qx: (n_head*?, q_len, hidden_dim)\n",
        "        # score: (n_head*?, q_len, k_len,)\n",
        "        # output: (?, q_len, out_dim,)\n",
        "        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n",
        "        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n",
        "        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n",
        "        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n",
        "        if self.score_function == 'dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qx, kt)\n",
        "        elif self.score_function == 'scaled_dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            qkt = torch.bmm(qx, kt)\n",
        "            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n",
        "        elif self.score_function == 'mlp':\n",
        "            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)\n",
        "            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)\n",
        "            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)\n",
        "            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)\n",
        "            score = F.tanh(torch.matmul(kq, self.weight))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            qw = torch.matmul(qx, self.weight)\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qw, kt)\n",
        "        else:\n",
        "            raise RuntimeError('invalid score_function')\n",
        "        score = F.softmax(score, dim=-1)\n",
        "        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)\n",
        "        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)\n",
        "        output = self.proj(output)  # (?, q_len, out_dim)\n",
        "        output = self.dropout(output)\n",
        "        return output, score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoQueryAttention(Attention):\n",
        "    '''q is a parameter'''\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', q_len=1, dropout=0):\n",
        "        super(NoQueryAttention, self).__init__(embed_dim, hidden_dim, out_dim, n_head, score_function, dropout)\n",
        "        self.q_len = q_len\n",
        "        self.q = nn.Parameter(torch.Tensor(q_len, embed_dim))\n",
        "        self.reset_q()\n",
        "\n",
        "    def reset_q(self):\n",
        "        stdv = 1. / math.sqrt(self.embed_dim)\n",
        "        self.q.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, **kwargs):\n",
        "        mb_size = k.shape[0]\n",
        "        q = self.q.expand(mb_size, -1, -1)\n",
        "        return super(NoQueryAttention, self).forward(k, q)"
      ],
      "metadata": {
        "id": "aOhD2c_x8MeN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze sequence embedding length to the longest one in the batch\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_first=True):\n",
        "        super(SqueezeEmbedding, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n",
        "        \"\"\"unpack: out\"\"\"\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "        out = out[0]  #\n",
        "        \"\"\"unsort\"\"\"\n",
        "        out = out[x_unsort_idx]\n",
        "        return out"
      ],
      "metadata": {
        "id": "7TlMbCBN8HdH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main**"
      ],
      "metadata": {
        "id": "6BmgF1FSp32r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Instructor:\n",
        "    def __init__(self):\n",
        "        tokenizer = build_tokenizer(\n",
        "            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n",
        "            max_seq_len=config2[\"max_seq_len\"],\n",
        "            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n",
        "        embedding_matrix = build_embedding_matrix(\n",
        "            word2idx=tokenizer.word2idx,\n",
        "            embed_dim=config2[\"embed_dim\"],\n",
        "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n",
        "        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n",
        "\n",
        "        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n",
        "        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n",
        "        assert 0 <= config2[\"valset_ratio\"] < 1\n",
        "        if config2[\"valset_ratio\"] > 0:\n",
        "            valset_len = int(len(self.trainset) * config2[\"valset_ratio\"])\n",
        "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
        "        else:\n",
        "            self.valset = self.testset\n",
        "\n",
        "        if config2[\"device\"].type == 'cuda':\n",
        "            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n",
        "        self._print_args()\n",
        "\n",
        "    def _print_args(self):\n",
        "        n_trainable_params, n_nontrainable_params = 0, 0\n",
        "        for p in self.model.parameters():\n",
        "            n_params = torch.prod(torch.tensor(p.shape))\n",
        "            if p.requires_grad:\n",
        "                n_trainable_params += n_params\n",
        "            else:\n",
        "                n_nontrainable_params += n_params\n",
        "        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
        "\n",
        "\n",
        "    def _reset_params(self):\n",
        "        for child in self.model.children():\n",
        "            for p in child.parameters():\n",
        "                if p.requires_grad:\n",
        "                    if len(p.shape) > 1:\n",
        "                        config2[\"initializer\"](p)\n",
        "                    else:\n",
        "                        stdv = 1. / math.sqrt(p.shape[0])\n",
        "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "\n",
        "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
        "        max_val_acc = 0\n",
        "        max_val_f1 = 0\n",
        "        max_val_epoch = 0\n",
        "        global_step = 0\n",
        "        path = None\n",
        "        for i_epoch in range(config2[\"num_epoch\"]):\n",
        "            print('>' * 100)\n",
        "            print('epoch: {}'.format(i_epoch))\n",
        "            n_correct, n_total, loss_total = 0, 0, 0\n",
        "            # switch model to training mode\n",
        "            self.model.train()\n",
        "            for i_batch, batch in enumerate(train_data_loader):\n",
        "                global_step += 1\n",
        "                # clear gradient accumulators\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                outputs = self.model(inputs)\n",
        "                targets = batch['polarity'].to(config2[\"device\"])\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
        "                n_total += len(outputs)\n",
        "                loss_total += loss.item() * len(outputs)\n",
        "                if global_step % config2[\"log_step\"] == 0:\n",
        "                    train_acc = n_correct / n_total\n",
        "                    train_loss = loss_total / n_total\n",
        "                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n",
        "\n",
        "            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n",
        "            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                max_val_epoch = i_epoch\n",
        "                if not os.path.exists('state_dict'):\n",
        "                    os.mkdir('state_dict')\n",
        "                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n",
        "                torch.save(self.model.state_dict(), path)\n",
        "                print('>> saved: {}'.format(path))\n",
        "            if val_f1 > max_val_f1:\n",
        "                max_val_f1 = val_f1\n",
        "            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n",
        "                print('>> early stop.')\n",
        "                break\n",
        "\n",
        "        return path\n",
        "\n",
        "    def _evaluate_acc_f1(self, data_loader):\n",
        "        n_correct, n_total = 0, 0\n",
        "        t_targets_all, t_outputs_all = None, None\n",
        "        # switch model to evaluation mode\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i_batch, t_batch in enumerate(data_loader):\n",
        "                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                t_targets = t_batch['polarity'].to(config2[\"device\"])\n",
        "                t_outputs = self.model(t_inputs)\n",
        "\n",
        "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
        "                n_total += len(t_outputs)\n",
        "\n",
        "                if t_targets_all is None:\n",
        "                    t_targets_all = t_targets\n",
        "                    t_outputs_all = t_outputs\n",
        "                else:\n",
        "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
        "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
        "\n",
        "        acc = n_correct / n_total\n",
        "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
        "        return acc, f1\n",
        "\n",
        "    def run(self):\n",
        "        # Loss and Optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n",
        "\n",
        "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n",
        "        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "\n",
        "        self._reset_params()\n",
        "        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n",
        "        print('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
      ],
      "metadata": {
        "id": "mPRxQS-Jp5vP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "bv09Mo677CkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_format_sentences(source_path, target_path):\n",
        "    f = open(target_path, \"w\")\n",
        "\n",
        "    sentences = parse(source_path).getroot()\n",
        "    preprocessed = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            if polarity == \"positive\":\n",
        "                polarity = '1'\n",
        "            elif polarity == \"neutral\":\n",
        "                polarity = '0'\n",
        "            elif polarity == \"negative\":\n",
        "                polarity = '-1'\n",
        "            else:\n",
        "                raise Exception(\"invalid polarity!\")\n",
        "            start = int(aspectTerm.get('from'))\n",
        "            end = int(aspectTerm.get('to'))\n",
        "            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n",
        "            preprocessed.append(text[start:end])\n",
        "            preprocessed.append(polarity)\n",
        "    f.write(\"\\n\".join(preprocessed))"
      ],
      "metadata": {
        "id": "Mlhjev_wrkDF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading tokenizer:', dat_fname)\n",
        "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        text = ''\n",
        "        for fname in fnames:\n",
        "            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "            lines = fin.readlines()\n",
        "            fin.close()\n",
        "            for i in range(0, len(lines), 3):\n",
        "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "                aspect = lines[i + 1].lower().strip()\n",
        "                text_raw = text_left + \" \" + aspect + \" \" + text_right\n",
        "                text += text_raw + \" \"\n",
        "\n",
        "        tokenizer = Tokenizer(max_seq_len)\n",
        "        tokenizer.fit_on_text(text)\n",
        "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "pV174mP37UZy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_word_vec(path, word2idx=None, embed_dim=300):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n",
        "        if word in word2idx.keys():\n",
        "            word_vec[word] = np.asarray(vec, dtype='float32')\n",
        "    return word_vec"
      ],
      "metadata": {
        "id": "2h-2UjGs7fu5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix:', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = config[\"glove_path\"]\n",
        "        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "nB3jz5Fk7dZd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "7JH84-Kx7Y-_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dependency_adj_matrix(text):\n",
        "    # https://spacy.io/docs/usage/processing-text\n",
        "    tokens = nlp(text)\n",
        "    words = text.split()\n",
        "    matrix = np.zeros((len(words), len(words))).astype('float32')\n",
        "    assert len(words) == len(list(tokens))\n",
        "\n",
        "    for token in tokens:\n",
        "        matrix[token.i][token.i] = 1\n",
        "        for child in token.children:\n",
        "            matrix[token.i][child.i] = 1\n",
        "            matrix[child.i][token.i] = 1\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "VemwD4FB7sY1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n",
        "    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n",
        "    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n",
        "    process(config[\"processed_train_path\"])\n",
        "    process(config[\"processed_val_path\"])\n",
        "    process(config[\"processed_test_path\"])"
      ],
      "metadata": {
        "id": "vC45U4k21oPd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(filename):\n",
        "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    lines = fin.readlines()\n",
        "    fin.close()\n",
        "    idx2graph = {}\n",
        "    fout = open(filename+'.graph', 'wb')\n",
        "    for i in range(0, len(lines), 3):\n",
        "        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n",
        "        aspect = lines[i + 1].strip()\n",
        "        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n",
        "        idx2graph[i] = adj_matrix\n",
        "    pickle.dump(idx2graph, fout)        \n",
        "    fout.close() "
      ],
      "metadata": {
        "id": "HlLsFiUJ7mPH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    ins = Instructor()\n",
        "    ins.run()"
      ],
      "metadata": {
        "id": "ANtAieVU8lwS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN**"
      ],
      "metadata": {
        "id": "cxb8D05u8UN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "5QyUuFxj8XuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "AF3DYep7AW8k"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"base_path\": \"drive/MyDrive/CS4248/MAMS-ATSA\",\n",
        "    \"glove_path\": \"drive/MyDrive/CS4248/glove.42B.300d.txt\"\n",
        "}\n",
        "\n",
        "config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n",
        "config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n",
        "config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n",
        "config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n",
        "config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n",
        "config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n",
        "config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"
      ],
      "metadata": {
        "id": "9y9MO-RW8bcS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_files = {\n",
        "    'train': config[\"processed_train_path\"],\n",
        "    'test': config[\"processed_test_path\"]\n",
        "}\n",
        "\n",
        "config2 = {\n",
        "    \"model_name\" : \"ATAE_LSTM\",\n",
        "    \"lr\" : 2e-5,\n",
        "    \"dropout\" : 0.1,\n",
        "    \"l2reg\" : 0.01,\n",
        "    \"num_epoch\" : 20,\n",
        "    \"batch_size\" : 16,\n",
        "    \"log_step\" : 10,\n",
        "    \"embed_dim\" : 300,\n",
        "    \"hidden_dim\" : 300,\n",
        "    \"model_class\" : ATAE_LSTM,\n",
        "    \"dataset\": \"MAMS\",\n",
        "    \"dataset_file\" : dataset_files,\n",
        "    \"inputs_cols\" : ['text_indices', 'aspect_indices'],\n",
        "    \"initializer\" : torch.nn.init.xavier_uniform_,\n",
        "    \"optimizer\" : torch.optim.Adam,\n",
        "    \"max_seq_len\" : 85,\n",
        "    \"polarities_dim\" : 3,\n",
        "    \"hops\" : 3,\n",
        "    \"patience\" : 5,\n",
        "    \"device\" : None,\n",
        "    \"seed\" : 1234,\n",
        "    \"valset_ratio\" : 0\n",
        "}\n",
        "\n",
        "config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
        "        if config2[\"device\"] is None else torch.device(config2[\"device\"])\n",
        "\n",
        "if config2[\"seed\"] is not None:\n",
        "    random.seed(config2[\"seed\"])\n",
        "    np.random.seed(config2[\"seed\"])\n",
        "    torch.manual_seed(config2[\"seed\"])\n",
        "    torch.cuda.manual_seed(config2[\"seed\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"
      ],
      "metadata": {
        "id": "clOIqT523tWy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**"
      ],
      "metadata": {
        "id": "3fT6RqeiAGmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#process_data()"
      ],
      "metadata": {
        "id": "vT4xfKSC-Xpx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "id": "8gEIkSn38pHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29a1344-f5c8-4a00-dd87-e6cba0d8a887"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading tokenizer: MAMS_tokenizer.dat\n",
            "loading embedding_matrix: 300_MAMS_embedding_matrix.dat\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 3994500\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 0\n",
            "[epoch 0] loss: 1.1509, acc: 0.2500\n",
            "[epoch 0] loss: 1.1485, acc: 0.2562\n",
            "[epoch 0] loss: 1.1415, acc: 0.2583\n",
            "[epoch 0] loss: 1.1266, acc: 0.2859\n",
            "[epoch 0] loss: 1.1180, acc: 0.2975\n",
            "[epoch 0] loss: 1.1113, acc: 0.3198\n",
            "[epoch 0] loss: 1.1050, acc: 0.3402\n",
            "[epoch 0] loss: 1.1018, acc: 0.3523\n",
            "[epoch 0] loss: 1.0990, acc: 0.3597\n",
            "[epoch 0] loss: 1.0935, acc: 0.3731\n",
            "[epoch 0] loss: 1.0885, acc: 0.3812\n",
            "[epoch 0] loss: 1.0828, acc: 0.3891\n",
            "[epoch 0] loss: 1.0805, acc: 0.3913\n",
            "[epoch 0] loss: 1.0761, acc: 0.3978\n",
            "[epoch 0] loss: 1.0707, acc: 0.4058\n",
            "[epoch 0] loss: 1.0659, acc: 0.4125\n",
            "[epoch 0] loss: 1.0624, acc: 0.4176\n",
            "[epoch 0] loss: 1.0592, acc: 0.4233\n",
            "[epoch 0] loss: 1.0553, acc: 0.4289\n",
            "[epoch 0] loss: 1.0516, acc: 0.4338\n",
            "[epoch 0] loss: 1.0473, acc: 0.4393\n",
            "[epoch 0] loss: 1.0457, acc: 0.4412\n",
            "[epoch 0] loss: 1.0407, acc: 0.4473\n",
            "[epoch 0] loss: 1.0395, acc: 0.4487\n",
            "[epoch 0] loss: 1.0363, acc: 0.4512\n",
            "[epoch 0] loss: 1.0342, acc: 0.4543\n",
            "[epoch 0] loss: 1.0302, acc: 0.4606\n",
            "[epoch 0] loss: 1.0269, acc: 0.4636\n",
            "[epoch 0] loss: 1.0241, acc: 0.4666\n",
            "[epoch 0] loss: 1.0218, acc: 0.4690\n",
            "[epoch 0] loss: 1.0198, acc: 0.4702\n",
            "[epoch 0] loss: 1.0159, acc: 0.4750\n",
            "[epoch 0] loss: 1.0125, acc: 0.4784\n",
            "[epoch 0] loss: 1.0090, acc: 0.4831\n",
            "[epoch 0] loss: 1.0063, acc: 0.4854\n",
            "[epoch 0] loss: 1.0036, acc: 0.4877\n",
            "[epoch 0] loss: 1.0022, acc: 0.4899\n",
            "[epoch 0] loss: 1.0015, acc: 0.4903\n",
            "[epoch 0] loss: 0.9993, acc: 0.4918\n",
            "[epoch 0] loss: 0.9963, acc: 0.4945\n",
            "[epoch 0] loss: 0.9940, acc: 0.4973\n",
            "[epoch 0] loss: 0.9922, acc: 0.4997\n",
            "[epoch 0] loss: 0.9903, acc: 0.5025\n",
            "[epoch 0] loss: 0.9896, acc: 0.5028\n",
            "[epoch 0] loss: 0.9868, acc: 0.5056\n",
            "[epoch 0] loss: 0.9844, acc: 0.5073\n",
            "[epoch 0] loss: 0.9821, acc: 0.5102\n",
            "[epoch 0] loss: 0.9800, acc: 0.5116\n",
            "[epoch 0] loss: 0.9789, acc: 0.5125\n",
            "[epoch 0] loss: 0.9778, acc: 0.5138\n",
            "[epoch 0] loss: 0.9757, acc: 0.5146\n",
            "[epoch 0] loss: 0.9739, acc: 0.5161\n",
            "[epoch 0] loss: 0.9725, acc: 0.5176\n",
            "[epoch 0] loss: 0.9698, acc: 0.5203\n",
            "[epoch 0] loss: 0.9680, acc: 0.5209\n",
            "[epoch 0] loss: 0.9670, acc: 0.5221\n",
            "[epoch 0] loss: 0.9651, acc: 0.5237\n",
            "[epoch 0] loss: 0.9643, acc: 0.5244\n",
            "[epoch 0] loss: 0.9636, acc: 0.5250\n",
            "[epoch 0] loss: 0.9623, acc: 0.5260\n",
            "[epoch 0] loss: 0.9604, acc: 0.5279\n",
            "[epoch 0] loss: 0.9590, acc: 0.5295\n",
            "[epoch 0] loss: 0.9573, acc: 0.5304\n",
            "[epoch 0] loss: 0.9554, acc: 0.5321\n",
            "[epoch 0] loss: 0.9547, acc: 0.5327\n",
            "[epoch 0] loss: 0.9541, acc: 0.5332\n",
            "[epoch 0] loss: 0.9526, acc: 0.5345\n",
            "[epoch 0] loss: 0.9527, acc: 0.5343\n",
            "[epoch 0] loss: 0.9512, acc: 0.5351\n",
            "[epoch 0] loss: 0.9498, acc: 0.5358\n",
            "> val_acc: 0.6010, val_f1: 0.5548\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.601\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 1\n",
            "[epoch 1] loss: 0.9641, acc: 0.5375\n",
            "[epoch 1] loss: 0.8841, acc: 0.5969\n",
            "[epoch 1] loss: 0.8527, acc: 0.6333\n",
            "[epoch 1] loss: 0.8420, acc: 0.6438\n",
            "[epoch 1] loss: 0.8571, acc: 0.6338\n",
            "[epoch 1] loss: 0.8586, acc: 0.6271\n",
            "[epoch 1] loss: 0.8661, acc: 0.6134\n",
            "[epoch 1] loss: 0.8674, acc: 0.6133\n",
            "[epoch 1] loss: 0.8712, acc: 0.6056\n",
            "[epoch 1] loss: 0.8772, acc: 0.6044\n",
            "[epoch 1] loss: 0.8728, acc: 0.6057\n",
            "[epoch 1] loss: 0.8708, acc: 0.6073\n",
            "[epoch 1] loss: 0.8672, acc: 0.6087\n",
            "[epoch 1] loss: 0.8720, acc: 0.6027\n",
            "[epoch 1] loss: 0.8735, acc: 0.5992\n",
            "[epoch 1] loss: 0.8741, acc: 0.5996\n",
            "[epoch 1] loss: 0.8712, acc: 0.6011\n",
            "[epoch 1] loss: 0.8737, acc: 0.5979\n",
            "[epoch 1] loss: 0.8731, acc: 0.6003\n",
            "[epoch 1] loss: 0.8748, acc: 0.5994\n",
            "[epoch 1] loss: 0.8731, acc: 0.6000\n",
            "[epoch 1] loss: 0.8736, acc: 0.6003\n",
            "[epoch 1] loss: 0.8709, acc: 0.6016\n",
            "[epoch 1] loss: 0.8754, acc: 0.5992\n",
            "[epoch 1] loss: 0.8752, acc: 0.5990\n",
            "[epoch 1] loss: 0.8749, acc: 0.5990\n",
            "[epoch 1] loss: 0.8746, acc: 0.5991\n",
            "[epoch 1] loss: 0.8756, acc: 0.5978\n",
            "[epoch 1] loss: 0.8765, acc: 0.5972\n",
            "[epoch 1] loss: 0.8753, acc: 0.5973\n",
            "[epoch 1] loss: 0.8742, acc: 0.5990\n",
            "[epoch 1] loss: 0.8719, acc: 0.6002\n",
            "[epoch 1] loss: 0.8722, acc: 0.5992\n",
            "[epoch 1] loss: 0.8718, acc: 0.5989\n",
            "[epoch 1] loss: 0.8733, acc: 0.5977\n",
            "[epoch 1] loss: 0.8712, acc: 0.5986\n",
            "[epoch 1] loss: 0.8700, acc: 0.5997\n",
            "[epoch 1] loss: 0.8713, acc: 0.5995\n",
            "[epoch 1] loss: 0.8699, acc: 0.5998\n",
            "[epoch 1] loss: 0.8695, acc: 0.5997\n",
            "[epoch 1] loss: 0.8695, acc: 0.5997\n",
            "[epoch 1] loss: 0.8706, acc: 0.5976\n",
            "[epoch 1] loss: 0.8714, acc: 0.5969\n",
            "[epoch 1] loss: 0.8685, acc: 0.5994\n",
            "[epoch 1] loss: 0.8659, acc: 0.6011\n",
            "[epoch 1] loss: 0.8650, acc: 0.6026\n",
            "[epoch 1] loss: 0.8637, acc: 0.6035\n",
            "[epoch 1] loss: 0.8637, acc: 0.6035\n",
            "[epoch 1] loss: 0.8643, acc: 0.6029\n",
            "[epoch 1] loss: 0.8638, acc: 0.6039\n",
            "[epoch 1] loss: 0.8644, acc: 0.6031\n",
            "[epoch 1] loss: 0.8645, acc: 0.6030\n",
            "[epoch 1] loss: 0.8630, acc: 0.6040\n",
            "[epoch 1] loss: 0.8624, acc: 0.6046\n",
            "[epoch 1] loss: 0.8630, acc: 0.6045\n",
            "[epoch 1] loss: 0.8639, acc: 0.6052\n",
            "[epoch 1] loss: 0.8630, acc: 0.6064\n",
            "[epoch 1] loss: 0.8627, acc: 0.6060\n",
            "[epoch 1] loss: 0.8635, acc: 0.6054\n",
            "[epoch 1] loss: 0.8630, acc: 0.6061\n",
            "[epoch 1] loss: 0.8623, acc: 0.6064\n",
            "[epoch 1] loss: 0.8636, acc: 0.6055\n",
            "[epoch 1] loss: 0.8637, acc: 0.6052\n",
            "[epoch 1] loss: 0.8638, acc: 0.6052\n",
            "[epoch 1] loss: 0.8631, acc: 0.6054\n",
            "[epoch 1] loss: 0.8624, acc: 0.6057\n",
            "[epoch 1] loss: 0.8624, acc: 0.6054\n",
            "[epoch 1] loss: 0.8626, acc: 0.6051\n",
            "[epoch 1] loss: 0.8625, acc: 0.6053\n",
            "[epoch 1] loss: 0.8612, acc: 0.6060\n",
            "> val_acc: 0.6168, val_f1: 0.5796\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 2\n",
            "[epoch 2] loss: 0.8455, acc: 0.6375\n",
            "[epoch 2] loss: 0.8632, acc: 0.6344\n",
            "[epoch 2] loss: 0.8775, acc: 0.6000\n",
            "[epoch 2] loss: 0.8668, acc: 0.6031\n",
            "[epoch 2] loss: 0.8612, acc: 0.6112\n",
            "[epoch 2] loss: 0.8651, acc: 0.5979\n",
            "[epoch 2] loss: 0.8640, acc: 0.6000\n",
            "[epoch 2] loss: 0.8663, acc: 0.6008\n",
            "[epoch 2] loss: 0.8601, acc: 0.6007\n",
            "[epoch 2] loss: 0.8531, acc: 0.6044\n",
            "[epoch 2] loss: 0.8563, acc: 0.6006\n",
            "[epoch 2] loss: 0.8518, acc: 0.6068\n",
            "[epoch 2] loss: 0.8523, acc: 0.6135\n",
            "[epoch 2] loss: 0.8478, acc: 0.6156\n",
            "[epoch 2] loss: 0.8461, acc: 0.6167\n",
            "[epoch 2] loss: 0.8474, acc: 0.6164\n",
            "[epoch 2] loss: 0.8493, acc: 0.6188\n",
            "[epoch 2] loss: 0.8488, acc: 0.6184\n",
            "[epoch 2] loss: 0.8505, acc: 0.6168\n",
            "[epoch 2] loss: 0.8469, acc: 0.6175\n",
            "[epoch 2] loss: 0.8411, acc: 0.6214\n",
            "[epoch 2] loss: 0.8436, acc: 0.6213\n",
            "[epoch 2] loss: 0.8427, acc: 0.6212\n",
            "[epoch 2] loss: 0.8419, acc: 0.6216\n",
            "[epoch 2] loss: 0.8453, acc: 0.6205\n",
            "[epoch 2] loss: 0.8451, acc: 0.6207\n",
            "[epoch 2] loss: 0.8475, acc: 0.6194\n",
            "[epoch 2] loss: 0.8453, acc: 0.6210\n",
            "[epoch 2] loss: 0.8452, acc: 0.6224\n",
            "[epoch 2] loss: 0.8425, acc: 0.6248\n",
            "[epoch 2] loss: 0.8422, acc: 0.6232\n",
            "[epoch 2] loss: 0.8423, acc: 0.6232\n",
            "[epoch 2] loss: 0.8408, acc: 0.6235\n",
            "[epoch 2] loss: 0.8420, acc: 0.6215\n",
            "[epoch 2] loss: 0.8408, acc: 0.6221\n",
            "[epoch 2] loss: 0.8392, acc: 0.6240\n",
            "[epoch 2] loss: 0.8400, acc: 0.6228\n",
            "[epoch 2] loss: 0.8404, acc: 0.6230\n",
            "[epoch 2] loss: 0.8400, acc: 0.6232\n",
            "[epoch 2] loss: 0.8408, acc: 0.6216\n",
            "[epoch 2] loss: 0.8418, acc: 0.6213\n",
            "[epoch 2] loss: 0.8430, acc: 0.6204\n",
            "[epoch 2] loss: 0.8442, acc: 0.6199\n",
            "[epoch 2] loss: 0.8436, acc: 0.6196\n",
            "[epoch 2] loss: 0.8444, acc: 0.6178\n",
            "[epoch 2] loss: 0.8429, acc: 0.6185\n",
            "[epoch 2] loss: 0.8406, acc: 0.6195\n",
            "[epoch 2] loss: 0.8396, acc: 0.6198\n",
            "[epoch 2] loss: 0.8397, acc: 0.6200\n",
            "[epoch 2] loss: 0.8411, acc: 0.6194\n",
            "[epoch 2] loss: 0.8406, acc: 0.6197\n",
            "[epoch 2] loss: 0.8399, acc: 0.6202\n",
            "[epoch 2] loss: 0.8397, acc: 0.6202\n",
            "[epoch 2] loss: 0.8381, acc: 0.6209\n",
            "[epoch 2] loss: 0.8374, acc: 0.6214\n",
            "[epoch 2] loss: 0.8384, acc: 0.6201\n",
            "[epoch 2] loss: 0.8389, acc: 0.6206\n",
            "[epoch 2] loss: 0.8390, acc: 0.6204\n",
            "[epoch 2] loss: 0.8388, acc: 0.6202\n",
            "[epoch 2] loss: 0.8385, acc: 0.6203\n",
            "[epoch 2] loss: 0.8390, acc: 0.6190\n",
            "[epoch 2] loss: 0.8401, acc: 0.6174\n",
            "[epoch 2] loss: 0.8401, acc: 0.6180\n",
            "[epoch 2] loss: 0.8394, acc: 0.6187\n",
            "[epoch 2] loss: 0.8383, acc: 0.6198\n",
            "[epoch 2] loss: 0.8373, acc: 0.6205\n",
            "[epoch 2] loss: 0.8384, acc: 0.6196\n",
            "[epoch 2] loss: 0.8382, acc: 0.6196\n",
            "[epoch 2] loss: 0.8381, acc: 0.6202\n",
            "[epoch 2] loss: 0.8373, acc: 0.6209\n",
            "> val_acc: 0.6235, val_f1: 0.5939\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6235\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 3\n",
            "[epoch 3] loss: 0.8389, acc: 0.5938\n",
            "[epoch 3] loss: 0.7908, acc: 0.6375\n",
            "[epoch 3] loss: 0.8175, acc: 0.6250\n",
            "[epoch 3] loss: 0.8114, acc: 0.6328\n",
            "[epoch 3] loss: 0.8211, acc: 0.6288\n",
            "[epoch 3] loss: 0.8150, acc: 0.6344\n",
            "[epoch 3] loss: 0.8139, acc: 0.6321\n",
            "[epoch 3] loss: 0.8108, acc: 0.6328\n",
            "[epoch 3] loss: 0.8129, acc: 0.6326\n",
            "[epoch 3] loss: 0.8120, acc: 0.6369\n",
            "[epoch 3] loss: 0.8183, acc: 0.6335\n",
            "[epoch 3] loss: 0.8178, acc: 0.6339\n",
            "[epoch 3] loss: 0.8200, acc: 0.6341\n",
            "[epoch 3] loss: 0.8200, acc: 0.6317\n",
            "[epoch 3] loss: 0.8225, acc: 0.6296\n",
            "[epoch 3] loss: 0.8180, acc: 0.6336\n",
            "[epoch 3] loss: 0.8141, acc: 0.6375\n",
            "[epoch 3] loss: 0.8177, acc: 0.6351\n",
            "[epoch 3] loss: 0.8200, acc: 0.6342\n",
            "[epoch 3] loss: 0.8186, acc: 0.6338\n",
            "[epoch 3] loss: 0.8177, acc: 0.6342\n",
            "[epoch 3] loss: 0.8201, acc: 0.6338\n",
            "[epoch 3] loss: 0.8204, acc: 0.6326\n",
            "[epoch 3] loss: 0.8215, acc: 0.6318\n",
            "[epoch 3] loss: 0.8191, acc: 0.6348\n",
            "[epoch 3] loss: 0.8191, acc: 0.6349\n",
            "[epoch 3] loss: 0.8177, acc: 0.6352\n",
            "[epoch 3] loss: 0.8166, acc: 0.6355\n",
            "[epoch 3] loss: 0.8153, acc: 0.6362\n",
            "[epoch 3] loss: 0.8151, acc: 0.6367\n",
            "[epoch 3] loss: 0.8163, acc: 0.6353\n",
            "[epoch 3] loss: 0.8164, acc: 0.6346\n",
            "[epoch 3] loss: 0.8169, acc: 0.6339\n",
            "[epoch 3] loss: 0.8165, acc: 0.6344\n",
            "[epoch 3] loss: 0.8143, acc: 0.6357\n",
            "[epoch 3] loss: 0.8131, acc: 0.6370\n",
            "[epoch 3] loss: 0.8145, acc: 0.6367\n",
            "[epoch 3] loss: 0.8137, acc: 0.6375\n",
            "[epoch 3] loss: 0.8124, acc: 0.6386\n",
            "[epoch 3] loss: 0.8135, acc: 0.6381\n",
            "[epoch 3] loss: 0.8155, acc: 0.6360\n",
            "[epoch 3] loss: 0.8134, acc: 0.6376\n",
            "[epoch 3] loss: 0.8133, acc: 0.6376\n",
            "[epoch 3] loss: 0.8154, acc: 0.6364\n",
            "[epoch 3] loss: 0.8150, acc: 0.6365\n",
            "[epoch 3] loss: 0.8156, acc: 0.6359\n",
            "[epoch 3] loss: 0.8164, acc: 0.6351\n",
            "[epoch 3] loss: 0.8176, acc: 0.6336\n",
            "[epoch 3] loss: 0.8163, acc: 0.6342\n",
            "[epoch 3] loss: 0.8162, acc: 0.6338\n",
            "[epoch 3] loss: 0.8167, acc: 0.6325\n",
            "[epoch 3] loss: 0.8158, acc: 0.6332\n",
            "[epoch 3] loss: 0.8155, acc: 0.6328\n",
            "[epoch 3] loss: 0.8160, acc: 0.6322\n",
            "[epoch 3] loss: 0.8167, acc: 0.6324\n",
            "[epoch 3] loss: 0.8169, acc: 0.6324\n",
            "[epoch 3] loss: 0.8173, acc: 0.6325\n",
            "[epoch 3] loss: 0.8175, acc: 0.6328\n",
            "[epoch 3] loss: 0.8184, acc: 0.6322\n",
            "[epoch 3] loss: 0.8185, acc: 0.6320\n",
            "[epoch 3] loss: 0.8194, acc: 0.6314\n",
            "[epoch 3] loss: 0.8206, acc: 0.6305\n",
            "[epoch 3] loss: 0.8212, acc: 0.6304\n",
            "[epoch 3] loss: 0.8205, acc: 0.6305\n",
            "[epoch 3] loss: 0.8206, acc: 0.6306\n",
            "[epoch 3] loss: 0.8207, acc: 0.6311\n",
            "[epoch 3] loss: 0.8210, acc: 0.6306\n",
            "[epoch 3] loss: 0.8213, acc: 0.6303\n",
            "[epoch 3] loss: 0.8212, acc: 0.6303\n",
            "[epoch 3] loss: 0.8208, acc: 0.6303\n",
            "> val_acc: 0.6415, val_f1: 0.6185\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 4\n",
            "[epoch 4] loss: 0.8472, acc: 0.6062\n",
            "[epoch 4] loss: 0.8213, acc: 0.6219\n",
            "[epoch 4] loss: 0.7911, acc: 0.6500\n",
            "[epoch 4] loss: 0.7988, acc: 0.6516\n",
            "[epoch 4] loss: 0.7988, acc: 0.6462\n",
            "[epoch 4] loss: 0.7999, acc: 0.6438\n",
            "[epoch 4] loss: 0.7995, acc: 0.6464\n",
            "[epoch 4] loss: 0.7932, acc: 0.6469\n",
            "[epoch 4] loss: 0.7920, acc: 0.6458\n",
            "[epoch 4] loss: 0.7964, acc: 0.6475\n",
            "[epoch 4] loss: 0.7960, acc: 0.6460\n",
            "[epoch 4] loss: 0.7935, acc: 0.6448\n",
            "[epoch 4] loss: 0.7985, acc: 0.6413\n",
            "[epoch 4] loss: 0.8015, acc: 0.6402\n",
            "[epoch 4] loss: 0.8038, acc: 0.6388\n",
            "[epoch 4] loss: 0.8028, acc: 0.6387\n",
            "[epoch 4] loss: 0.8010, acc: 0.6415\n",
            "[epoch 4] loss: 0.7963, acc: 0.6455\n",
            "[epoch 4] loss: 0.7999, acc: 0.6431\n",
            "[epoch 4] loss: 0.8044, acc: 0.6394\n",
            "[epoch 4] loss: 0.8037, acc: 0.6387\n",
            "[epoch 4] loss: 0.8055, acc: 0.6384\n",
            "[epoch 4] loss: 0.8074, acc: 0.6367\n",
            "[epoch 4] loss: 0.8049, acc: 0.6367\n",
            "[epoch 4] loss: 0.8032, acc: 0.6372\n",
            "[epoch 4] loss: 0.8034, acc: 0.6370\n",
            "[epoch 4] loss: 0.8081, acc: 0.6347\n",
            "[epoch 4] loss: 0.8092, acc: 0.6348\n",
            "[epoch 4] loss: 0.8123, acc: 0.6345\n",
            "[epoch 4] loss: 0.8150, acc: 0.6331\n",
            "[epoch 4] loss: 0.8141, acc: 0.6339\n",
            "[epoch 4] loss: 0.8131, acc: 0.6346\n",
            "[epoch 4] loss: 0.8120, acc: 0.6364\n",
            "[epoch 4] loss: 0.8115, acc: 0.6371\n",
            "[epoch 4] loss: 0.8105, acc: 0.6377\n",
            "[epoch 4] loss: 0.8106, acc: 0.6368\n",
            "[epoch 4] loss: 0.8102, acc: 0.6373\n",
            "[epoch 4] loss: 0.8121, acc: 0.6354\n",
            "[epoch 4] loss: 0.8119, acc: 0.6351\n",
            "[epoch 4] loss: 0.8115, acc: 0.6369\n",
            "[epoch 4] loss: 0.8109, acc: 0.6364\n",
            "[epoch 4] loss: 0.8108, acc: 0.6362\n",
            "[epoch 4] loss: 0.8113, acc: 0.6360\n",
            "[epoch 4] loss: 0.8121, acc: 0.6358\n",
            "[epoch 4] loss: 0.8111, acc: 0.6361\n",
            "[epoch 4] loss: 0.8101, acc: 0.6364\n",
            "[epoch 4] loss: 0.8100, acc: 0.6356\n",
            "[epoch 4] loss: 0.8121, acc: 0.6345\n",
            "[epoch 4] loss: 0.8154, acc: 0.6323\n",
            "[epoch 4] loss: 0.8155, acc: 0.6318\n",
            "[epoch 4] loss: 0.8159, acc: 0.6312\n",
            "[epoch 4] loss: 0.8158, acc: 0.6315\n",
            "[epoch 4] loss: 0.8154, acc: 0.6314\n",
            "[epoch 4] loss: 0.8140, acc: 0.6324\n",
            "[epoch 4] loss: 0.8139, acc: 0.6323\n",
            "[epoch 4] loss: 0.8142, acc: 0.6320\n",
            "[epoch 4] loss: 0.8115, acc: 0.6334\n",
            "[epoch 4] loss: 0.8105, acc: 0.6344\n",
            "[epoch 4] loss: 0.8109, acc: 0.6347\n",
            "[epoch 4] loss: 0.8105, acc: 0.6350\n",
            "[epoch 4] loss: 0.8097, acc: 0.6353\n",
            "[epoch 4] loss: 0.8092, acc: 0.6354\n",
            "[epoch 4] loss: 0.8100, acc: 0.6349\n",
            "[epoch 4] loss: 0.8096, acc: 0.6350\n",
            "[epoch 4] loss: 0.8090, acc: 0.6350\n",
            "[epoch 4] loss: 0.8074, acc: 0.6354\n",
            "[epoch 4] loss: 0.8083, acc: 0.6351\n",
            "[epoch 4] loss: 0.8080, acc: 0.6355\n",
            "[epoch 4] loss: 0.8089, acc: 0.6348\n",
            "[epoch 4] loss: 0.8094, acc: 0.6344\n",
            "> val_acc: 0.6250, val_f1: 0.5925\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 5\n",
            "[epoch 5] loss: 0.8561, acc: 0.5687\n",
            "[epoch 5] loss: 0.8312, acc: 0.6281\n",
            "[epoch 5] loss: 0.8217, acc: 0.6354\n",
            "[epoch 5] loss: 0.8471, acc: 0.6266\n",
            "[epoch 5] loss: 0.8426, acc: 0.6250\n",
            "[epoch 5] loss: 0.8405, acc: 0.6229\n",
            "[epoch 5] loss: 0.8418, acc: 0.6179\n",
            "[epoch 5] loss: 0.8304, acc: 0.6258\n",
            "[epoch 5] loss: 0.8258, acc: 0.6326\n",
            "[epoch 5] loss: 0.8164, acc: 0.6356\n",
            "[epoch 5] loss: 0.8111, acc: 0.6392\n",
            "[epoch 5] loss: 0.8111, acc: 0.6349\n",
            "[epoch 5] loss: 0.8049, acc: 0.6380\n",
            "[epoch 5] loss: 0.8086, acc: 0.6326\n",
            "[epoch 5] loss: 0.8061, acc: 0.6338\n",
            "[epoch 5] loss: 0.8008, acc: 0.6398\n",
            "[epoch 5] loss: 0.8006, acc: 0.6415\n",
            "[epoch 5] loss: 0.8003, acc: 0.6431\n",
            "[epoch 5] loss: 0.7990, acc: 0.6414\n",
            "[epoch 5] loss: 0.7983, acc: 0.6434\n",
            "[epoch 5] loss: 0.7960, acc: 0.6455\n",
            "[epoch 5] loss: 0.8014, acc: 0.6423\n",
            "[epoch 5] loss: 0.7988, acc: 0.6443\n",
            "[epoch 5] loss: 0.7976, acc: 0.6427\n",
            "[epoch 5] loss: 0.7983, acc: 0.6432\n",
            "[epoch 5] loss: 0.8012, acc: 0.6409\n",
            "[epoch 5] loss: 0.8011, acc: 0.6412\n",
            "[epoch 5] loss: 0.8021, acc: 0.6382\n",
            "[epoch 5] loss: 0.8034, acc: 0.6364\n",
            "[epoch 5] loss: 0.8034, acc: 0.6365\n",
            "[epoch 5] loss: 0.8057, acc: 0.6353\n",
            "[epoch 5] loss: 0.8057, acc: 0.6355\n",
            "[epoch 5] loss: 0.8077, acc: 0.6335\n",
            "[epoch 5] loss: 0.8072, acc: 0.6336\n",
            "[epoch 5] loss: 0.8063, acc: 0.6338\n",
            "[epoch 5] loss: 0.8038, acc: 0.6352\n",
            "[epoch 5] loss: 0.8037, acc: 0.6358\n",
            "[epoch 5] loss: 0.8029, acc: 0.6363\n",
            "[epoch 5] loss: 0.8020, acc: 0.6375\n",
            "[epoch 5] loss: 0.8030, acc: 0.6369\n",
            "[epoch 5] loss: 0.8031, acc: 0.6369\n",
            "[epoch 5] loss: 0.8042, acc: 0.6354\n",
            "[epoch 5] loss: 0.8045, acc: 0.6349\n",
            "[epoch 5] loss: 0.8031, acc: 0.6365\n",
            "[epoch 5] loss: 0.8027, acc: 0.6367\n",
            "[epoch 5] loss: 0.8044, acc: 0.6357\n",
            "[epoch 5] loss: 0.8041, acc: 0.6364\n",
            "[epoch 5] loss: 0.8056, acc: 0.6355\n",
            "[epoch 5] loss: 0.8066, acc: 0.6352\n",
            "[epoch 5] loss: 0.8060, acc: 0.6352\n",
            "[epoch 5] loss: 0.8062, acc: 0.6349\n",
            "[epoch 5] loss: 0.8062, acc: 0.6346\n",
            "[epoch 5] loss: 0.8065, acc: 0.6343\n",
            "[epoch 5] loss: 0.8057, acc: 0.6347\n",
            "[epoch 5] loss: 0.8052, acc: 0.6355\n",
            "[epoch 5] loss: 0.8050, acc: 0.6359\n",
            "[epoch 5] loss: 0.8049, acc: 0.6365\n",
            "[epoch 5] loss: 0.8055, acc: 0.6358\n",
            "[epoch 5] loss: 0.8053, acc: 0.6362\n",
            "[epoch 5] loss: 0.8049, acc: 0.6365\n",
            "[epoch 5] loss: 0.8053, acc: 0.6360\n",
            "[epoch 5] loss: 0.8046, acc: 0.6363\n",
            "[epoch 5] loss: 0.8035, acc: 0.6369\n",
            "[epoch 5] loss: 0.8039, acc: 0.6368\n",
            "[epoch 5] loss: 0.8025, acc: 0.6379\n",
            "[epoch 5] loss: 0.8009, acc: 0.6392\n",
            "[epoch 5] loss: 0.8008, acc: 0.6396\n",
            "[epoch 5] loss: 0.8014, acc: 0.6392\n",
            "[epoch 5] loss: 0.8008, acc: 0.6397\n",
            "[epoch 5] loss: 0.7999, acc: 0.6408\n",
            "> val_acc: 0.6385, val_f1: 0.6122\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 6\n",
            "[epoch 6] loss: 0.7723, acc: 0.6500\n",
            "[epoch 6] loss: 0.7758, acc: 0.6531\n",
            "[epoch 6] loss: 0.7820, acc: 0.6667\n",
            "[epoch 6] loss: 0.7721, acc: 0.6703\n",
            "[epoch 6] loss: 0.7625, acc: 0.6737\n",
            "[epoch 6] loss: 0.7668, acc: 0.6708\n",
            "[epoch 6] loss: 0.7710, acc: 0.6687\n",
            "[epoch 6] loss: 0.7698, acc: 0.6695\n",
            "[epoch 6] loss: 0.7742, acc: 0.6660\n",
            "[epoch 6] loss: 0.7717, acc: 0.6681\n",
            "[epoch 6] loss: 0.7692, acc: 0.6716\n",
            "[epoch 6] loss: 0.7757, acc: 0.6656\n",
            "[epoch 6] loss: 0.7729, acc: 0.6673\n",
            "[epoch 6] loss: 0.7785, acc: 0.6629\n",
            "[epoch 6] loss: 0.7811, acc: 0.6617\n",
            "[epoch 6] loss: 0.7789, acc: 0.6598\n",
            "[epoch 6] loss: 0.7790, acc: 0.6607\n",
            "[epoch 6] loss: 0.7756, acc: 0.6615\n",
            "[epoch 6] loss: 0.7822, acc: 0.6589\n",
            "[epoch 6] loss: 0.7812, acc: 0.6587\n",
            "[epoch 6] loss: 0.7834, acc: 0.6577\n",
            "[epoch 6] loss: 0.7874, acc: 0.6554\n",
            "[epoch 6] loss: 0.7887, acc: 0.6543\n",
            "[epoch 6] loss: 0.7878, acc: 0.6562\n",
            "[epoch 6] loss: 0.7884, acc: 0.6555\n",
            "[epoch 6] loss: 0.7869, acc: 0.6550\n",
            "[epoch 6] loss: 0.7924, acc: 0.6530\n",
            "[epoch 6] loss: 0.7914, acc: 0.6531\n",
            "[epoch 6] loss: 0.7893, acc: 0.6539\n",
            "[epoch 6] loss: 0.7892, acc: 0.6538\n",
            "[epoch 6] loss: 0.7886, acc: 0.6540\n",
            "[epoch 6] loss: 0.7871, acc: 0.6559\n",
            "[epoch 6] loss: 0.7856, acc: 0.6561\n",
            "[epoch 6] loss: 0.7859, acc: 0.6555\n",
            "[epoch 6] loss: 0.7870, acc: 0.6545\n",
            "[epoch 6] loss: 0.7862, acc: 0.6550\n",
            "[epoch 6] loss: 0.7888, acc: 0.6530\n",
            "[epoch 6] loss: 0.7875, acc: 0.6546\n",
            "[epoch 6] loss: 0.7878, acc: 0.6543\n",
            "[epoch 6] loss: 0.7860, acc: 0.6547\n",
            "[epoch 6] loss: 0.7863, acc: 0.6558\n",
            "[epoch 6] loss: 0.7854, acc: 0.6562\n",
            "[epoch 6] loss: 0.7846, acc: 0.6552\n",
            "[epoch 6] loss: 0.7849, acc: 0.6551\n",
            "[epoch 6] loss: 0.7852, acc: 0.6542\n",
            "[epoch 6] loss: 0.7863, acc: 0.6530\n",
            "[epoch 6] loss: 0.7869, acc: 0.6528\n",
            "[epoch 6] loss: 0.7850, acc: 0.6535\n",
            "[epoch 6] loss: 0.7838, acc: 0.6541\n",
            "[epoch 6] loss: 0.7847, acc: 0.6538\n",
            "[epoch 6] loss: 0.7863, acc: 0.6525\n",
            "[epoch 6] loss: 0.7888, acc: 0.6508\n",
            "[epoch 6] loss: 0.7886, acc: 0.6507\n",
            "[epoch 6] loss: 0.7885, acc: 0.6506\n",
            "[epoch 6] loss: 0.7879, acc: 0.6503\n",
            "[epoch 6] loss: 0.7890, acc: 0.6499\n",
            "[epoch 6] loss: 0.7888, acc: 0.6498\n",
            "[epoch 6] loss: 0.7891, acc: 0.6496\n",
            "[epoch 6] loss: 0.7906, acc: 0.6483\n",
            "[epoch 6] loss: 0.7906, acc: 0.6480\n",
            "[epoch 6] loss: 0.7903, acc: 0.6477\n",
            "[epoch 6] loss: 0.7905, acc: 0.6477\n",
            "[epoch 6] loss: 0.7920, acc: 0.6469\n",
            "[epoch 6] loss: 0.7915, acc: 0.6470\n",
            "[epoch 6] loss: 0.7914, acc: 0.6470\n",
            "[epoch 6] loss: 0.7911, acc: 0.6470\n",
            "[epoch 6] loss: 0.7901, acc: 0.6482\n",
            "[epoch 6] loss: 0.7913, acc: 0.6468\n",
            "[epoch 6] loss: 0.7907, acc: 0.6468\n",
            "[epoch 6] loss: 0.7909, acc: 0.6463\n",
            "> val_acc: 0.6415, val_f1: 0.6155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 7\n",
            "[epoch 7] loss: 0.8122, acc: 0.6937\n",
            "[epoch 7] loss: 0.7753, acc: 0.6937\n",
            "[epoch 7] loss: 0.7491, acc: 0.7063\n",
            "[epoch 7] loss: 0.7558, acc: 0.6922\n",
            "[epoch 7] loss: 0.7706, acc: 0.6687\n",
            "[epoch 7] loss: 0.7634, acc: 0.6698\n",
            "[epoch 7] loss: 0.7648, acc: 0.6625\n",
            "[epoch 7] loss: 0.7729, acc: 0.6570\n",
            "[epoch 7] loss: 0.7761, acc: 0.6562\n",
            "[epoch 7] loss: 0.7811, acc: 0.6506\n",
            "[epoch 7] loss: 0.7843, acc: 0.6466\n",
            "[epoch 7] loss: 0.7832, acc: 0.6474\n",
            "[epoch 7] loss: 0.7732, acc: 0.6548\n",
            "[epoch 7] loss: 0.7736, acc: 0.6540\n",
            "[epoch 7] loss: 0.7692, acc: 0.6575\n",
            "[epoch 7] loss: 0.7689, acc: 0.6594\n",
            "[epoch 7] loss: 0.7666, acc: 0.6614\n",
            "[epoch 7] loss: 0.7688, acc: 0.6618\n",
            "[epoch 7] loss: 0.7700, acc: 0.6615\n",
            "[epoch 7] loss: 0.7737, acc: 0.6603\n",
            "[epoch 7] loss: 0.7779, acc: 0.6580\n",
            "[epoch 7] loss: 0.7772, acc: 0.6568\n",
            "[epoch 7] loss: 0.7780, acc: 0.6549\n",
            "[epoch 7] loss: 0.7811, acc: 0.6510\n",
            "[epoch 7] loss: 0.7792, acc: 0.6528\n",
            "[epoch 7] loss: 0.7796, acc: 0.6522\n",
            "[epoch 7] loss: 0.7796, acc: 0.6530\n",
            "[epoch 7] loss: 0.7809, acc: 0.6527\n",
            "[epoch 7] loss: 0.7809, acc: 0.6528\n",
            "[epoch 7] loss: 0.7813, acc: 0.6529\n",
            "[epoch 7] loss: 0.7811, acc: 0.6544\n",
            "[epoch 7] loss: 0.7807, acc: 0.6545\n",
            "[epoch 7] loss: 0.7811, acc: 0.6540\n",
            "[epoch 7] loss: 0.7799, acc: 0.6546\n",
            "[epoch 7] loss: 0.7791, acc: 0.6555\n",
            "[epoch 7] loss: 0.7798, acc: 0.6540\n",
            "[epoch 7] loss: 0.7794, acc: 0.6535\n",
            "[epoch 7] loss: 0.7798, acc: 0.6531\n",
            "[epoch 7] loss: 0.7799, acc: 0.6534\n",
            "[epoch 7] loss: 0.7819, acc: 0.6520\n",
            "[epoch 7] loss: 0.7834, acc: 0.6512\n",
            "[epoch 7] loss: 0.7824, acc: 0.6516\n",
            "[epoch 7] loss: 0.7827, acc: 0.6516\n",
            "[epoch 7] loss: 0.7828, acc: 0.6516\n",
            "[epoch 7] loss: 0.7831, acc: 0.6511\n",
            "[epoch 7] loss: 0.7822, acc: 0.6527\n",
            "[epoch 7] loss: 0.7825, acc: 0.6523\n",
            "[epoch 7] loss: 0.7835, acc: 0.6517\n",
            "[epoch 7] loss: 0.7836, acc: 0.6514\n",
            "[epoch 7] loss: 0.7851, acc: 0.6508\n",
            "[epoch 7] loss: 0.7839, acc: 0.6523\n",
            "[epoch 7] loss: 0.7829, acc: 0.6525\n",
            "[epoch 7] loss: 0.7839, acc: 0.6521\n",
            "[epoch 7] loss: 0.7845, acc: 0.6525\n",
            "[epoch 7] loss: 0.7852, acc: 0.6518\n",
            "[epoch 7] loss: 0.7844, acc: 0.6515\n",
            "[epoch 7] loss: 0.7846, acc: 0.6513\n",
            "[epoch 7] loss: 0.7848, acc: 0.6517\n",
            "[epoch 7] loss: 0.7833, acc: 0.6530\n",
            "[epoch 7] loss: 0.7832, acc: 0.6531\n",
            "[epoch 7] loss: 0.7836, acc: 0.6532\n",
            "[epoch 7] loss: 0.7840, acc: 0.6527\n",
            "[epoch 7] loss: 0.7844, acc: 0.6522\n",
            "[epoch 7] loss: 0.7830, acc: 0.6530\n",
            "[epoch 7] loss: 0.7841, acc: 0.6527\n",
            "[epoch 7] loss: 0.7845, acc: 0.6520\n",
            "[epoch 7] loss: 0.7849, acc: 0.6521\n",
            "[epoch 7] loss: 0.7856, acc: 0.6517\n",
            "[epoch 7] loss: 0.7854, acc: 0.6522\n",
            "[epoch 7] loss: 0.7855, acc: 0.6522\n",
            "> val_acc: 0.6377, val_f1: 0.6171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 8\n",
            "[epoch 8] loss: 0.8176, acc: 0.5875\n",
            "[epoch 8] loss: 0.8064, acc: 0.6125\n",
            "[epoch 8] loss: 0.7870, acc: 0.6396\n",
            "[epoch 8] loss: 0.8057, acc: 0.6250\n",
            "[epoch 8] loss: 0.8097, acc: 0.6225\n",
            "[epoch 8] loss: 0.8047, acc: 0.6250\n",
            "[epoch 8] loss: 0.8121, acc: 0.6277\n",
            "[epoch 8] loss: 0.8057, acc: 0.6383\n",
            "[epoch 8] loss: 0.7990, acc: 0.6424\n",
            "[epoch 8] loss: 0.7979, acc: 0.6381\n",
            "[epoch 8] loss: 0.7920, acc: 0.6426\n",
            "[epoch 8] loss: 0.7951, acc: 0.6391\n",
            "[epoch 8] loss: 0.7969, acc: 0.6361\n",
            "[epoch 8] loss: 0.7908, acc: 0.6388\n",
            "[epoch 8] loss: 0.7925, acc: 0.6388\n",
            "[epoch 8] loss: 0.7953, acc: 0.6375\n",
            "[epoch 8] loss: 0.7964, acc: 0.6382\n",
            "[epoch 8] loss: 0.7968, acc: 0.6375\n",
            "[epoch 8] loss: 0.7975, acc: 0.6388\n",
            "[epoch 8] loss: 0.7968, acc: 0.6409\n",
            "[epoch 8] loss: 0.7972, acc: 0.6420\n",
            "[epoch 8] loss: 0.7966, acc: 0.6418\n",
            "[epoch 8] loss: 0.7949, acc: 0.6424\n",
            "[epoch 8] loss: 0.7921, acc: 0.6456\n",
            "[epoch 8] loss: 0.7920, acc: 0.6465\n",
            "[epoch 8] loss: 0.7905, acc: 0.6457\n",
            "[epoch 8] loss: 0.7858, acc: 0.6479\n",
            "[epoch 8] loss: 0.7860, acc: 0.6475\n",
            "[epoch 8] loss: 0.7852, acc: 0.6478\n",
            "[epoch 8] loss: 0.7841, acc: 0.6479\n",
            "[epoch 8] loss: 0.7819, acc: 0.6496\n",
            "[epoch 8] loss: 0.7831, acc: 0.6480\n",
            "[epoch 8] loss: 0.7825, acc: 0.6487\n",
            "[epoch 8] loss: 0.7809, acc: 0.6487\n",
            "[epoch 8] loss: 0.7807, acc: 0.6491\n",
            "[epoch 8] loss: 0.7815, acc: 0.6484\n",
            "[epoch 8] loss: 0.7821, acc: 0.6486\n",
            "[epoch 8] loss: 0.7800, acc: 0.6505\n",
            "[epoch 8] loss: 0.7819, acc: 0.6498\n",
            "[epoch 8] loss: 0.7822, acc: 0.6502\n",
            "[epoch 8] loss: 0.7830, acc: 0.6508\n",
            "[epoch 8] loss: 0.7812, acc: 0.6531\n",
            "[epoch 8] loss: 0.7825, acc: 0.6526\n",
            "[epoch 8] loss: 0.7825, acc: 0.6523\n",
            "[epoch 8] loss: 0.7850, acc: 0.6508\n",
            "[epoch 8] loss: 0.7859, acc: 0.6508\n",
            "[epoch 8] loss: 0.7860, acc: 0.6508\n",
            "[epoch 8] loss: 0.7872, acc: 0.6496\n",
            "[epoch 8] loss: 0.7857, acc: 0.6503\n",
            "[epoch 8] loss: 0.7850, acc: 0.6506\n",
            "[epoch 8] loss: 0.7843, acc: 0.6511\n",
            "[epoch 8] loss: 0.7850, acc: 0.6504\n",
            "[epoch 8] loss: 0.7840, acc: 0.6509\n",
            "[epoch 8] loss: 0.7859, acc: 0.6506\n",
            "[epoch 8] loss: 0.7869, acc: 0.6493\n",
            "[epoch 8] loss: 0.7853, acc: 0.6507\n",
            "[epoch 8] loss: 0.7842, acc: 0.6505\n",
            "[epoch 8] loss: 0.7845, acc: 0.6502\n",
            "[epoch 8] loss: 0.7855, acc: 0.6492\n",
            "[epoch 8] loss: 0.7841, acc: 0.6500\n",
            "[epoch 8] loss: 0.7828, acc: 0.6517\n",
            "[epoch 8] loss: 0.7809, acc: 0.6526\n",
            "[epoch 8] loss: 0.7817, acc: 0.6528\n",
            "[epoch 8] loss: 0.7811, acc: 0.6529\n",
            "[epoch 8] loss: 0.7807, acc: 0.6529\n",
            "[epoch 8] loss: 0.7813, acc: 0.6527\n",
            "[epoch 8] loss: 0.7816, acc: 0.6522\n",
            "[epoch 8] loss: 0.7811, acc: 0.6528\n",
            "[epoch 8] loss: 0.7810, acc: 0.6520\n",
            "[epoch 8] loss: 0.7808, acc: 0.6522\n",
            "> val_acc: 0.6422, val_f1: 0.6231\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 9\n",
            "[epoch 9] loss: 0.7332, acc: 0.6937\n",
            "[epoch 9] loss: 0.7603, acc: 0.6781\n",
            "[epoch 9] loss: 0.7718, acc: 0.6750\n",
            "[epoch 9] loss: 0.7653, acc: 0.6797\n",
            "[epoch 9] loss: 0.7741, acc: 0.6775\n",
            "[epoch 9] loss: 0.7829, acc: 0.6771\n",
            "[epoch 9] loss: 0.7751, acc: 0.6732\n",
            "[epoch 9] loss: 0.7754, acc: 0.6695\n",
            "[epoch 9] loss: 0.7763, acc: 0.6687\n",
            "[epoch 9] loss: 0.7701, acc: 0.6700\n",
            "[epoch 9] loss: 0.7722, acc: 0.6682\n",
            "[epoch 9] loss: 0.7755, acc: 0.6635\n",
            "[epoch 9] loss: 0.7768, acc: 0.6644\n",
            "[epoch 9] loss: 0.7746, acc: 0.6665\n",
            "[epoch 9] loss: 0.7716, acc: 0.6667\n",
            "[epoch 9] loss: 0.7699, acc: 0.6660\n",
            "[epoch 9] loss: 0.7711, acc: 0.6654\n",
            "[epoch 9] loss: 0.7697, acc: 0.6635\n",
            "[epoch 9] loss: 0.7697, acc: 0.6628\n",
            "[epoch 9] loss: 0.7690, acc: 0.6644\n",
            "[epoch 9] loss: 0.7688, acc: 0.6640\n",
            "[epoch 9] loss: 0.7695, acc: 0.6639\n",
            "[epoch 9] loss: 0.7677, acc: 0.6639\n",
            "[epoch 9] loss: 0.7667, acc: 0.6638\n",
            "[epoch 9] loss: 0.7624, acc: 0.6670\n",
            "[epoch 9] loss: 0.7619, acc: 0.6661\n",
            "[epoch 9] loss: 0.7639, acc: 0.6646\n",
            "[epoch 9] loss: 0.7677, acc: 0.6605\n",
            "[epoch 9] loss: 0.7668, acc: 0.6601\n",
            "[epoch 9] loss: 0.7676, acc: 0.6581\n",
            "[epoch 9] loss: 0.7659, acc: 0.6591\n",
            "[epoch 9] loss: 0.7655, acc: 0.6594\n",
            "[epoch 9] loss: 0.7646, acc: 0.6595\n",
            "[epoch 9] loss: 0.7673, acc: 0.6583\n",
            "[epoch 9] loss: 0.7637, acc: 0.6602\n",
            "[epoch 9] loss: 0.7644, acc: 0.6604\n",
            "[epoch 9] loss: 0.7634, acc: 0.6610\n",
            "[epoch 9] loss: 0.7653, acc: 0.6590\n",
            "[epoch 9] loss: 0.7654, acc: 0.6598\n",
            "[epoch 9] loss: 0.7658, acc: 0.6586\n",
            "[epoch 9] loss: 0.7664, acc: 0.6581\n",
            "[epoch 9] loss: 0.7686, acc: 0.6561\n",
            "[epoch 9] loss: 0.7674, acc: 0.6573\n",
            "[epoch 9] loss: 0.7696, acc: 0.6565\n",
            "[epoch 9] loss: 0.7710, acc: 0.6549\n",
            "[epoch 9] loss: 0.7730, acc: 0.6545\n",
            "[epoch 9] loss: 0.7737, acc: 0.6539\n",
            "[epoch 9] loss: 0.7738, acc: 0.6543\n",
            "[epoch 9] loss: 0.7748, acc: 0.6540\n",
            "[epoch 9] loss: 0.7743, acc: 0.6544\n",
            "[epoch 9] loss: 0.7735, acc: 0.6551\n",
            "[epoch 9] loss: 0.7738, acc: 0.6550\n",
            "[epoch 9] loss: 0.7732, acc: 0.6561\n",
            "[epoch 9] loss: 0.7722, acc: 0.6572\n",
            "[epoch 9] loss: 0.7724, acc: 0.6568\n",
            "[epoch 9] loss: 0.7708, acc: 0.6579\n",
            "[epoch 9] loss: 0.7712, acc: 0.6569\n",
            "[epoch 9] loss: 0.7736, acc: 0.6562\n",
            "[epoch 9] loss: 0.7731, acc: 0.6568\n",
            "[epoch 9] loss: 0.7731, acc: 0.6572\n",
            "[epoch 9] loss: 0.7736, acc: 0.6569\n",
            "[epoch 9] loss: 0.7735, acc: 0.6558\n",
            "[epoch 9] loss: 0.7745, acc: 0.6550\n",
            "[epoch 9] loss: 0.7750, acc: 0.6546\n",
            "[epoch 9] loss: 0.7741, acc: 0.6550\n",
            "[epoch 9] loss: 0.7742, acc: 0.6547\n",
            "[epoch 9] loss: 0.7745, acc: 0.6548\n",
            "[epoch 9] loss: 0.7748, acc: 0.6536\n",
            "[epoch 9] loss: 0.7745, acc: 0.6540\n",
            "[epoch 9] loss: 0.7753, acc: 0.6536\n",
            "> val_acc: 0.6475, val_f1: 0.6312\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 10\n",
            "[epoch 10] loss: 0.7250, acc: 0.6937\n",
            "[epoch 10] loss: 0.7536, acc: 0.6687\n",
            "[epoch 10] loss: 0.7446, acc: 0.6729\n",
            "[epoch 10] loss: 0.7491, acc: 0.6719\n",
            "[epoch 10] loss: 0.7642, acc: 0.6587\n",
            "[epoch 10] loss: 0.7639, acc: 0.6552\n",
            "[epoch 10] loss: 0.7724, acc: 0.6545\n",
            "[epoch 10] loss: 0.7702, acc: 0.6641\n",
            "[epoch 10] loss: 0.7687, acc: 0.6674\n",
            "[epoch 10] loss: 0.7721, acc: 0.6606\n",
            "[epoch 10] loss: 0.7681, acc: 0.6625\n",
            "[epoch 10] loss: 0.7635, acc: 0.6635\n",
            "[epoch 10] loss: 0.7666, acc: 0.6635\n",
            "[epoch 10] loss: 0.7700, acc: 0.6580\n",
            "[epoch 10] loss: 0.7663, acc: 0.6596\n",
            "[epoch 10] loss: 0.7649, acc: 0.6602\n",
            "[epoch 10] loss: 0.7691, acc: 0.6592\n",
            "[epoch 10] loss: 0.7718, acc: 0.6566\n",
            "[epoch 10] loss: 0.7716, acc: 0.6582\n",
            "[epoch 10] loss: 0.7760, acc: 0.6553\n",
            "[epoch 10] loss: 0.7737, acc: 0.6551\n",
            "[epoch 10] loss: 0.7727, acc: 0.6534\n",
            "[epoch 10] loss: 0.7688, acc: 0.6549\n",
            "[epoch 10] loss: 0.7698, acc: 0.6547\n",
            "[epoch 10] loss: 0.7680, acc: 0.6560\n",
            "[epoch 10] loss: 0.7721, acc: 0.6524\n",
            "[epoch 10] loss: 0.7709, acc: 0.6519\n",
            "[epoch 10] loss: 0.7722, acc: 0.6507\n",
            "[epoch 10] loss: 0.7720, acc: 0.6509\n",
            "[epoch 10] loss: 0.7706, acc: 0.6521\n",
            "[epoch 10] loss: 0.7695, acc: 0.6532\n",
            "[epoch 10] loss: 0.7672, acc: 0.6543\n",
            "[epoch 10] loss: 0.7657, acc: 0.6547\n",
            "[epoch 10] loss: 0.7666, acc: 0.6546\n",
            "[epoch 10] loss: 0.7680, acc: 0.6539\n",
            "[epoch 10] loss: 0.7674, acc: 0.6552\n",
            "[epoch 10] loss: 0.7702, acc: 0.6530\n",
            "[epoch 10] loss: 0.7707, acc: 0.6536\n",
            "[epoch 10] loss: 0.7700, acc: 0.6532\n",
            "[epoch 10] loss: 0.7697, acc: 0.6547\n",
            "[epoch 10] loss: 0.7692, acc: 0.6556\n",
            "[epoch 10] loss: 0.7679, acc: 0.6549\n",
            "[epoch 10] loss: 0.7712, acc: 0.6538\n",
            "[epoch 10] loss: 0.7697, acc: 0.6550\n",
            "[epoch 10] loss: 0.7695, acc: 0.6543\n",
            "[epoch 10] loss: 0.7676, acc: 0.6549\n",
            "[epoch 10] loss: 0.7670, acc: 0.6556\n",
            "[epoch 10] loss: 0.7672, acc: 0.6564\n",
            "[epoch 10] loss: 0.7694, acc: 0.6556\n",
            "[epoch 10] loss: 0.7683, acc: 0.6565\n",
            "[epoch 10] loss: 0.7684, acc: 0.6566\n",
            "[epoch 10] loss: 0.7680, acc: 0.6573\n",
            "[epoch 10] loss: 0.7686, acc: 0.6574\n",
            "[epoch 10] loss: 0.7686, acc: 0.6578\n",
            "[epoch 10] loss: 0.7670, acc: 0.6587\n",
            "[epoch 10] loss: 0.7681, acc: 0.6577\n",
            "[epoch 10] loss: 0.7674, acc: 0.6575\n",
            "[epoch 10] loss: 0.7687, acc: 0.6566\n",
            "[epoch 10] loss: 0.7692, acc: 0.6562\n",
            "[epoch 10] loss: 0.7698, acc: 0.6572\n",
            "[epoch 10] loss: 0.7701, acc: 0.6582\n",
            "[epoch 10] loss: 0.7702, acc: 0.6591\n",
            "[epoch 10] loss: 0.7701, acc: 0.6583\n",
            "[epoch 10] loss: 0.7705, acc: 0.6580\n",
            "[epoch 10] loss: 0.7707, acc: 0.6581\n",
            "[epoch 10] loss: 0.7724, acc: 0.6568\n",
            "[epoch 10] loss: 0.7723, acc: 0.6570\n",
            "[epoch 10] loss: 0.7729, acc: 0.6573\n",
            "[epoch 10] loss: 0.7733, acc: 0.6573\n",
            "[epoch 10] loss: 0.7724, acc: 0.6578\n",
            "> val_acc: 0.6542, val_f1: 0.6242\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 11\n",
            "[epoch 11] loss: 0.7707, acc: 0.6312\n",
            "[epoch 11] loss: 0.7874, acc: 0.6375\n",
            "[epoch 11] loss: 0.7991, acc: 0.6312\n",
            "[epoch 11] loss: 0.8048, acc: 0.6234\n",
            "[epoch 11] loss: 0.7913, acc: 0.6350\n",
            "[epoch 11] loss: 0.7900, acc: 0.6375\n",
            "[epoch 11] loss: 0.7919, acc: 0.6384\n",
            "[epoch 11] loss: 0.7901, acc: 0.6453\n",
            "[epoch 11] loss: 0.7818, acc: 0.6521\n",
            "[epoch 11] loss: 0.7816, acc: 0.6519\n",
            "[epoch 11] loss: 0.7779, acc: 0.6517\n",
            "[epoch 11] loss: 0.7704, acc: 0.6573\n",
            "[epoch 11] loss: 0.7661, acc: 0.6591\n",
            "[epoch 11] loss: 0.7735, acc: 0.6567\n",
            "[epoch 11] loss: 0.7778, acc: 0.6558\n",
            "[epoch 11] loss: 0.7811, acc: 0.6547\n",
            "[epoch 11] loss: 0.7781, acc: 0.6548\n",
            "[epoch 11] loss: 0.7774, acc: 0.6562\n",
            "[epoch 11] loss: 0.7790, acc: 0.6520\n",
            "[epoch 11] loss: 0.7796, acc: 0.6525\n",
            "[epoch 11] loss: 0.7802, acc: 0.6533\n",
            "[epoch 11] loss: 0.7784, acc: 0.6560\n",
            "[epoch 11] loss: 0.7786, acc: 0.6573\n",
            "[epoch 11] loss: 0.7798, acc: 0.6562\n",
            "[epoch 11] loss: 0.7805, acc: 0.6552\n",
            "[epoch 11] loss: 0.7770, acc: 0.6567\n",
            "[epoch 11] loss: 0.7788, acc: 0.6551\n",
            "[epoch 11] loss: 0.7765, acc: 0.6545\n",
            "[epoch 11] loss: 0.7733, acc: 0.6580\n",
            "[epoch 11] loss: 0.7723, acc: 0.6590\n",
            "[epoch 11] loss: 0.7746, acc: 0.6571\n",
            "[epoch 11] loss: 0.7737, acc: 0.6578\n",
            "[epoch 11] loss: 0.7712, acc: 0.6595\n",
            "[epoch 11] loss: 0.7691, acc: 0.6607\n",
            "[epoch 11] loss: 0.7681, acc: 0.6613\n",
            "[epoch 11] loss: 0.7689, acc: 0.6613\n",
            "[epoch 11] loss: 0.7708, acc: 0.6603\n",
            "[epoch 11] loss: 0.7710, acc: 0.6599\n",
            "[epoch 11] loss: 0.7702, acc: 0.6591\n",
            "[epoch 11] loss: 0.7686, acc: 0.6605\n",
            "[epoch 11] loss: 0.7678, acc: 0.6604\n",
            "[epoch 11] loss: 0.7695, acc: 0.6598\n",
            "[epoch 11] loss: 0.7699, acc: 0.6599\n",
            "[epoch 11] loss: 0.7704, acc: 0.6598\n",
            "[epoch 11] loss: 0.7721, acc: 0.6593\n",
            "[epoch 11] loss: 0.7707, acc: 0.6599\n",
            "[epoch 11] loss: 0.7718, acc: 0.6598\n",
            "[epoch 11] loss: 0.7714, acc: 0.6596\n",
            "[epoch 11] loss: 0.7714, acc: 0.6592\n",
            "[epoch 11] loss: 0.7711, acc: 0.6591\n",
            "[epoch 11] loss: 0.7719, acc: 0.6589\n",
            "[epoch 11] loss: 0.7723, acc: 0.6576\n",
            "[epoch 11] loss: 0.7712, acc: 0.6574\n",
            "[epoch 11] loss: 0.7708, acc: 0.6573\n",
            "[epoch 11] loss: 0.7706, acc: 0.6570\n",
            "[epoch 11] loss: 0.7695, acc: 0.6571\n",
            "[epoch 11] loss: 0.7695, acc: 0.6571\n",
            "[epoch 11] loss: 0.7711, acc: 0.6567\n",
            "[epoch 11] loss: 0.7716, acc: 0.6559\n",
            "[epoch 11] loss: 0.7712, acc: 0.6560\n",
            "[epoch 11] loss: 0.7706, acc: 0.6568\n",
            "[epoch 11] loss: 0.7697, acc: 0.6567\n",
            "[epoch 11] loss: 0.7697, acc: 0.6563\n",
            "[epoch 11] loss: 0.7696, acc: 0.6565\n",
            "[epoch 11] loss: 0.7690, acc: 0.6566\n",
            "[epoch 11] loss: 0.7689, acc: 0.6564\n",
            "[epoch 11] loss: 0.7684, acc: 0.6569\n",
            "[epoch 11] loss: 0.7693, acc: 0.6567\n",
            "[epoch 11] loss: 0.7694, acc: 0.6566\n",
            "[epoch 11] loss: 0.7685, acc: 0.6570\n",
            "> val_acc: 0.6392, val_f1: 0.6144\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 12\n",
            "[epoch 12] loss: 0.7747, acc: 0.6875\n",
            "[epoch 12] loss: 0.7752, acc: 0.6656\n",
            "[epoch 12] loss: 0.7686, acc: 0.6604\n",
            "[epoch 12] loss: 0.7864, acc: 0.6594\n",
            "[epoch 12] loss: 0.7815, acc: 0.6550\n",
            "[epoch 12] loss: 0.7715, acc: 0.6594\n",
            "[epoch 12] loss: 0.7705, acc: 0.6598\n",
            "[epoch 12] loss: 0.7698, acc: 0.6602\n",
            "[epoch 12] loss: 0.7773, acc: 0.6514\n",
            "[epoch 12] loss: 0.7685, acc: 0.6587\n",
            "[epoch 12] loss: 0.7656, acc: 0.6631\n",
            "[epoch 12] loss: 0.7735, acc: 0.6589\n",
            "[epoch 12] loss: 0.7725, acc: 0.6601\n",
            "[epoch 12] loss: 0.7695, acc: 0.6625\n",
            "[epoch 12] loss: 0.7712, acc: 0.6600\n",
            "[epoch 12] loss: 0.7700, acc: 0.6598\n",
            "[epoch 12] loss: 0.7693, acc: 0.6585\n",
            "[epoch 12] loss: 0.7687, acc: 0.6573\n",
            "[epoch 12] loss: 0.7646, acc: 0.6582\n",
            "[epoch 12] loss: 0.7634, acc: 0.6591\n",
            "[epoch 12] loss: 0.7662, acc: 0.6568\n",
            "[epoch 12] loss: 0.7665, acc: 0.6554\n",
            "[epoch 12] loss: 0.7639, acc: 0.6568\n",
            "[epoch 12] loss: 0.7638, acc: 0.6544\n",
            "[epoch 12] loss: 0.7631, acc: 0.6550\n",
            "[epoch 12] loss: 0.7621, acc: 0.6562\n",
            "[epoch 12] loss: 0.7601, acc: 0.6581\n",
            "[epoch 12] loss: 0.7637, acc: 0.6576\n",
            "[epoch 12] loss: 0.7634, acc: 0.6578\n",
            "[epoch 12] loss: 0.7618, acc: 0.6590\n",
            "[epoch 12] loss: 0.7618, acc: 0.6599\n",
            "[epoch 12] loss: 0.7594, acc: 0.6619\n",
            "[epoch 12] loss: 0.7586, acc: 0.6634\n",
            "[epoch 12] loss: 0.7568, acc: 0.6653\n",
            "[epoch 12] loss: 0.7576, acc: 0.6630\n",
            "[epoch 12] loss: 0.7586, acc: 0.6637\n",
            "[epoch 12] loss: 0.7599, acc: 0.6632\n",
            "[epoch 12] loss: 0.7600, acc: 0.6625\n",
            "[epoch 12] loss: 0.7619, acc: 0.6617\n",
            "[epoch 12] loss: 0.7634, acc: 0.6602\n",
            "[epoch 12] loss: 0.7636, acc: 0.6595\n",
            "[epoch 12] loss: 0.7642, acc: 0.6585\n",
            "[epoch 12] loss: 0.7652, acc: 0.6584\n",
            "[epoch 12] loss: 0.7651, acc: 0.6594\n",
            "[epoch 12] loss: 0.7649, acc: 0.6600\n",
            "[epoch 12] loss: 0.7636, acc: 0.6607\n",
            "[epoch 12] loss: 0.7638, acc: 0.6606\n",
            "[epoch 12] loss: 0.7643, acc: 0.6603\n",
            "[epoch 12] loss: 0.7633, acc: 0.6603\n",
            "[epoch 12] loss: 0.7629, acc: 0.6603\n",
            "[epoch 12] loss: 0.7634, acc: 0.6594\n",
            "[epoch 12] loss: 0.7636, acc: 0.6601\n",
            "[epoch 12] loss: 0.7644, acc: 0.6594\n",
            "[epoch 12] loss: 0.7642, acc: 0.6595\n",
            "[epoch 12] loss: 0.7659, acc: 0.6586\n",
            "[epoch 12] loss: 0.7668, acc: 0.6584\n",
            "[epoch 12] loss: 0.7673, acc: 0.6587\n",
            "[epoch 12] loss: 0.7673, acc: 0.6591\n",
            "[epoch 12] loss: 0.7675, acc: 0.6592\n",
            "[epoch 12] loss: 0.7671, acc: 0.6598\n",
            "[epoch 12] loss: 0.7669, acc: 0.6596\n",
            "[epoch 12] loss: 0.7666, acc: 0.6602\n",
            "[epoch 12] loss: 0.7673, acc: 0.6594\n",
            "[epoch 12] loss: 0.7679, acc: 0.6585\n",
            "[epoch 12] loss: 0.7675, acc: 0.6588\n",
            "[epoch 12] loss: 0.7670, acc: 0.6595\n",
            "[epoch 12] loss: 0.7649, acc: 0.6610\n",
            "[epoch 12] loss: 0.7639, acc: 0.6612\n",
            "[epoch 12] loss: 0.7641, acc: 0.6609\n",
            "[epoch 12] loss: 0.7652, acc: 0.6606\n",
            "> val_acc: 0.6542, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 13\n",
            "[epoch 13] loss: 0.7159, acc: 0.6875\n",
            "[epoch 13] loss: 0.7396, acc: 0.6781\n",
            "[epoch 13] loss: 0.7850, acc: 0.6667\n",
            "[epoch 13] loss: 0.7618, acc: 0.6719\n",
            "[epoch 13] loss: 0.7607, acc: 0.6725\n",
            "[epoch 13] loss: 0.7617, acc: 0.6635\n",
            "[epoch 13] loss: 0.7646, acc: 0.6643\n",
            "[epoch 13] loss: 0.7636, acc: 0.6664\n",
            "[epoch 13] loss: 0.7706, acc: 0.6597\n",
            "[epoch 13] loss: 0.7690, acc: 0.6606\n",
            "[epoch 13] loss: 0.7698, acc: 0.6614\n",
            "[epoch 13] loss: 0.7731, acc: 0.6557\n",
            "[epoch 13] loss: 0.7703, acc: 0.6543\n",
            "[epoch 13] loss: 0.7678, acc: 0.6576\n",
            "[epoch 13] loss: 0.7675, acc: 0.6587\n",
            "[epoch 13] loss: 0.7626, acc: 0.6605\n",
            "[epoch 13] loss: 0.7608, acc: 0.6621\n",
            "[epoch 13] loss: 0.7594, acc: 0.6642\n",
            "[epoch 13] loss: 0.7606, acc: 0.6648\n",
            "[epoch 13] loss: 0.7606, acc: 0.6653\n",
            "[epoch 13] loss: 0.7621, acc: 0.6637\n",
            "[epoch 13] loss: 0.7600, acc: 0.6653\n",
            "[epoch 13] loss: 0.7576, acc: 0.6666\n",
            "[epoch 13] loss: 0.7576, acc: 0.6677\n",
            "[epoch 13] loss: 0.7550, acc: 0.6685\n",
            "[epoch 13] loss: 0.7542, acc: 0.6692\n",
            "[epoch 13] loss: 0.7530, acc: 0.6692\n",
            "[epoch 13] loss: 0.7509, acc: 0.6728\n",
            "[epoch 13] loss: 0.7537, acc: 0.6716\n",
            "[epoch 13] loss: 0.7528, acc: 0.6719\n",
            "[epoch 13] loss: 0.7516, acc: 0.6724\n",
            "[epoch 13] loss: 0.7511, acc: 0.6727\n",
            "[epoch 13] loss: 0.7519, acc: 0.6725\n",
            "[epoch 13] loss: 0.7501, acc: 0.6730\n",
            "[epoch 13] loss: 0.7511, acc: 0.6720\n",
            "[epoch 13] loss: 0.7535, acc: 0.6696\n",
            "[epoch 13] loss: 0.7533, acc: 0.6693\n",
            "[epoch 13] loss: 0.7541, acc: 0.6679\n",
            "[epoch 13] loss: 0.7543, acc: 0.6671\n",
            "[epoch 13] loss: 0.7545, acc: 0.6680\n",
            "[epoch 13] loss: 0.7557, acc: 0.6672\n",
            "[epoch 13] loss: 0.7557, acc: 0.6673\n",
            "[epoch 13] loss: 0.7588, acc: 0.6657\n",
            "[epoch 13] loss: 0.7601, acc: 0.6656\n",
            "[epoch 13] loss: 0.7602, acc: 0.6664\n",
            "[epoch 13] loss: 0.7622, acc: 0.6660\n",
            "[epoch 13] loss: 0.7633, acc: 0.6646\n",
            "[epoch 13] loss: 0.7632, acc: 0.6656\n",
            "[epoch 13] loss: 0.7625, acc: 0.6665\n",
            "[epoch 13] loss: 0.7608, acc: 0.6677\n",
            "[epoch 13] loss: 0.7605, acc: 0.6670\n",
            "[epoch 13] loss: 0.7594, acc: 0.6677\n",
            "[epoch 13] loss: 0.7607, acc: 0.6666\n",
            "[epoch 13] loss: 0.7587, acc: 0.6679\n",
            "[epoch 13] loss: 0.7582, acc: 0.6687\n",
            "[epoch 13] loss: 0.7586, acc: 0.6684\n",
            "[epoch 13] loss: 0.7593, acc: 0.6679\n",
            "[epoch 13] loss: 0.7598, acc: 0.6678\n",
            "[epoch 13] loss: 0.7609, acc: 0.6663\n",
            "[epoch 13] loss: 0.7612, acc: 0.6661\n",
            "[epoch 13] loss: 0.7618, acc: 0.6666\n",
            "[epoch 13] loss: 0.7617, acc: 0.6671\n",
            "[epoch 13] loss: 0.7606, acc: 0.6676\n",
            "[epoch 13] loss: 0.7604, acc: 0.6674\n",
            "[epoch 13] loss: 0.7590, acc: 0.6673\n",
            "[epoch 13] loss: 0.7601, acc: 0.6662\n",
            "[epoch 13] loss: 0.7589, acc: 0.6670\n",
            "[epoch 13] loss: 0.7597, acc: 0.6665\n",
            "[epoch 13] loss: 0.7599, acc: 0.6658\n",
            "[epoch 13] loss: 0.7602, acc: 0.6653\n",
            "> val_acc: 0.6437, val_f1: 0.6294\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 14\n",
            "[epoch 14] loss: 0.7545, acc: 0.6750\n",
            "[epoch 14] loss: 0.7468, acc: 0.6844\n",
            "[epoch 14] loss: 0.7619, acc: 0.6771\n",
            "[epoch 14] loss: 0.7674, acc: 0.6703\n",
            "[epoch 14] loss: 0.7628, acc: 0.6750\n",
            "[epoch 14] loss: 0.7578, acc: 0.6813\n",
            "[epoch 14] loss: 0.7570, acc: 0.6839\n",
            "[epoch 14] loss: 0.7606, acc: 0.6797\n",
            "[epoch 14] loss: 0.7535, acc: 0.6882\n",
            "[epoch 14] loss: 0.7532, acc: 0.6837\n",
            "[epoch 14] loss: 0.7514, acc: 0.6813\n",
            "[epoch 14] loss: 0.7554, acc: 0.6776\n",
            "[epoch 14] loss: 0.7518, acc: 0.6784\n",
            "[epoch 14] loss: 0.7576, acc: 0.6741\n",
            "[epoch 14] loss: 0.7592, acc: 0.6713\n",
            "[epoch 14] loss: 0.7583, acc: 0.6707\n",
            "[epoch 14] loss: 0.7533, acc: 0.6732\n",
            "[epoch 14] loss: 0.7546, acc: 0.6729\n",
            "[epoch 14] loss: 0.7564, acc: 0.6714\n",
            "[epoch 14] loss: 0.7558, acc: 0.6719\n",
            "[epoch 14] loss: 0.7559, acc: 0.6717\n",
            "[epoch 14] loss: 0.7555, acc: 0.6719\n",
            "[epoch 14] loss: 0.7543, acc: 0.6717\n",
            "[epoch 14] loss: 0.7552, acc: 0.6703\n",
            "[epoch 14] loss: 0.7558, acc: 0.6703\n",
            "[epoch 14] loss: 0.7602, acc: 0.6675\n",
            "[epoch 14] loss: 0.7591, acc: 0.6683\n",
            "[epoch 14] loss: 0.7572, acc: 0.6690\n",
            "[epoch 14] loss: 0.7562, acc: 0.6698\n",
            "[epoch 14] loss: 0.7572, acc: 0.6692\n",
            "[epoch 14] loss: 0.7557, acc: 0.6702\n",
            "[epoch 14] loss: 0.7566, acc: 0.6686\n",
            "[epoch 14] loss: 0.7575, acc: 0.6682\n",
            "[epoch 14] loss: 0.7548, acc: 0.6700\n",
            "[epoch 14] loss: 0.7537, acc: 0.6709\n",
            "[epoch 14] loss: 0.7522, acc: 0.6707\n",
            "[epoch 14] loss: 0.7523, acc: 0.6696\n",
            "[epoch 14] loss: 0.7505, acc: 0.6702\n",
            "[epoch 14] loss: 0.7523, acc: 0.6676\n",
            "[epoch 14] loss: 0.7524, acc: 0.6663\n",
            "[epoch 14] loss: 0.7523, acc: 0.6654\n",
            "[epoch 14] loss: 0.7542, acc: 0.6644\n",
            "[epoch 14] loss: 0.7549, acc: 0.6634\n",
            "[epoch 14] loss: 0.7563, acc: 0.6631\n",
            "[epoch 14] loss: 0.7557, acc: 0.6629\n",
            "[epoch 14] loss: 0.7564, acc: 0.6626\n",
            "[epoch 14] loss: 0.7561, acc: 0.6630\n",
            "[epoch 14] loss: 0.7557, acc: 0.6630\n",
            "[epoch 14] loss: 0.7555, acc: 0.6626\n",
            "[epoch 14] loss: 0.7558, acc: 0.6621\n",
            "[epoch 14] loss: 0.7543, acc: 0.6631\n",
            "[epoch 14] loss: 0.7549, acc: 0.6631\n",
            "[epoch 14] loss: 0.7546, acc: 0.6643\n",
            "[epoch 14] loss: 0.7543, acc: 0.6646\n",
            "[epoch 14] loss: 0.7538, acc: 0.6649\n",
            "[epoch 14] loss: 0.7543, acc: 0.6646\n",
            "[epoch 14] loss: 0.7541, acc: 0.6647\n",
            "[epoch 14] loss: 0.7553, acc: 0.6649\n",
            "[epoch 14] loss: 0.7548, acc: 0.6657\n",
            "[epoch 14] loss: 0.7565, acc: 0.6646\n",
            "[epoch 14] loss: 0.7569, acc: 0.6643\n",
            "[epoch 14] loss: 0.7567, acc: 0.6642\n",
            "[epoch 14] loss: 0.7565, acc: 0.6646\n",
            "[epoch 14] loss: 0.7569, acc: 0.6645\n",
            "[epoch 14] loss: 0.7567, acc: 0.6645\n",
            "[epoch 14] loss: 0.7566, acc: 0.6645\n",
            "[epoch 14] loss: 0.7571, acc: 0.6648\n",
            "[epoch 14] loss: 0.7577, acc: 0.6640\n",
            "[epoch 14] loss: 0.7574, acc: 0.6644\n",
            "[epoch 14] loss: 0.7587, acc: 0.6634\n",
            "> val_acc: 0.6407, val_f1: 0.6032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 15\n",
            "[epoch 15] loss: 0.7697, acc: 0.6375\n",
            "[epoch 15] loss: 0.7307, acc: 0.6687\n",
            "[epoch 15] loss: 0.7429, acc: 0.6687\n",
            "[epoch 15] loss: 0.7670, acc: 0.6547\n",
            "[epoch 15] loss: 0.7703, acc: 0.6550\n",
            "[epoch 15] loss: 0.7719, acc: 0.6531\n",
            "[epoch 15] loss: 0.7714, acc: 0.6598\n",
            "[epoch 15] loss: 0.7713, acc: 0.6602\n",
            "[epoch 15] loss: 0.7744, acc: 0.6625\n",
            "[epoch 15] loss: 0.7680, acc: 0.6687\n",
            "[epoch 15] loss: 0.7682, acc: 0.6625\n",
            "[epoch 15] loss: 0.7664, acc: 0.6620\n",
            "[epoch 15] loss: 0.7618, acc: 0.6654\n",
            "[epoch 15] loss: 0.7670, acc: 0.6634\n",
            "[epoch 15] loss: 0.7696, acc: 0.6621\n",
            "[epoch 15] loss: 0.7706, acc: 0.6629\n",
            "[epoch 15] loss: 0.7719, acc: 0.6629\n",
            "[epoch 15] loss: 0.7681, acc: 0.6639\n",
            "[epoch 15] loss: 0.7727, acc: 0.6602\n",
            "[epoch 15] loss: 0.7730, acc: 0.6591\n",
            "[epoch 15] loss: 0.7730, acc: 0.6583\n",
            "[epoch 15] loss: 0.7700, acc: 0.6577\n",
            "[epoch 15] loss: 0.7736, acc: 0.6554\n",
            "[epoch 15] loss: 0.7743, acc: 0.6557\n",
            "[epoch 15] loss: 0.7734, acc: 0.6555\n",
            "[epoch 15] loss: 0.7736, acc: 0.6550\n",
            "[epoch 15] loss: 0.7714, acc: 0.6560\n",
            "[epoch 15] loss: 0.7692, acc: 0.6580\n",
            "[epoch 15] loss: 0.7688, acc: 0.6588\n",
            "[epoch 15] loss: 0.7665, acc: 0.6600\n",
            "[epoch 15] loss: 0.7688, acc: 0.6599\n",
            "[epoch 15] loss: 0.7667, acc: 0.6611\n",
            "[epoch 15] loss: 0.7649, acc: 0.6631\n",
            "[epoch 15] loss: 0.7647, acc: 0.6631\n",
            "[epoch 15] loss: 0.7633, acc: 0.6648\n",
            "[epoch 15] loss: 0.7637, acc: 0.6646\n",
            "[epoch 15] loss: 0.7633, acc: 0.6649\n",
            "[epoch 15] loss: 0.7628, acc: 0.6648\n",
            "[epoch 15] loss: 0.7638, acc: 0.6643\n",
            "[epoch 15] loss: 0.7614, acc: 0.6650\n",
            "[epoch 15] loss: 0.7617, acc: 0.6648\n",
            "[epoch 15] loss: 0.7633, acc: 0.6640\n",
            "[epoch 15] loss: 0.7636, acc: 0.6637\n",
            "[epoch 15] loss: 0.7635, acc: 0.6634\n",
            "[epoch 15] loss: 0.7645, acc: 0.6628\n",
            "[epoch 15] loss: 0.7631, acc: 0.6633\n",
            "[epoch 15] loss: 0.7627, acc: 0.6633\n",
            "[epoch 15] loss: 0.7630, acc: 0.6637\n",
            "[epoch 15] loss: 0.7639, acc: 0.6633\n",
            "[epoch 15] loss: 0.7646, acc: 0.6625\n",
            "[epoch 15] loss: 0.7653, acc: 0.6613\n",
            "[epoch 15] loss: 0.7642, acc: 0.6613\n",
            "[epoch 15] loss: 0.7637, acc: 0.6623\n",
            "[epoch 15] loss: 0.7633, acc: 0.6627\n",
            "[epoch 15] loss: 0.7640, acc: 0.6620\n",
            "[epoch 15] loss: 0.7639, acc: 0.6619\n",
            "[epoch 15] loss: 0.7624, acc: 0.6628\n",
            "[epoch 15] loss: 0.7613, acc: 0.6639\n",
            "[epoch 15] loss: 0.7599, acc: 0.6642\n",
            "[epoch 15] loss: 0.7597, acc: 0.6646\n",
            "[epoch 15] loss: 0.7575, acc: 0.6659\n",
            "[epoch 15] loss: 0.7574, acc: 0.6661\n",
            "[epoch 15] loss: 0.7565, acc: 0.6665\n",
            "[epoch 15] loss: 0.7564, acc: 0.6665\n",
            "[epoch 15] loss: 0.7567, acc: 0.6658\n",
            "[epoch 15] loss: 0.7571, acc: 0.6652\n",
            "[epoch 15] loss: 0.7572, acc: 0.6644\n",
            "[epoch 15] loss: 0.7568, acc: 0.6644\n",
            "[epoch 15] loss: 0.7558, acc: 0.6649\n",
            "[epoch 15] loss: 0.7569, acc: 0.6646\n",
            "> val_acc: 0.6490, val_f1: 0.6249\n",
            ">> early stop.\n",
            ">> test_acc: 0.6542, test_f1: 0.6242\n"
          ]
        }
      ]
    }
  ]
}