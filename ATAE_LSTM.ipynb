{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SETUP**"
      ],
      "metadata": {
        "id": "5zXJrSMo495u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "8_7Yx7y-5BOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tYF42jlo5Q4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a067822b-1f96-4d31-9889-18fd00f19032"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSD-dTWt28EH",
        "outputId": "e3557324-5c37-4ca6-dff8-8e685505bfea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "eTERBhPf5Ykk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import strftime, localtime\n",
        "from transformers import BertTokenizer\n",
        "from sklearn import metrics\n",
        "from xml.etree.ElementTree import parse\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "ZEPQFq-e6gVp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "6674vF4n6le1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASSES**"
      ],
      "metadata": {
        "id": "IDenOuxLpJFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**"
      ],
      "metadata": {
        "id": "USX4iChw67d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, fname, tokenizer):\n",
        "        fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "        lines = fin.readlines()\n",
        "        fin.close()\n",
        "        fin = open(fname+'.graph', 'rb')\n",
        "        idx2graph = pickle.load(fin)\n",
        "        fin.close()\n",
        "\n",
        "        all_data = []\n",
        "        for i in range(0, len(lines), 3):\n",
        "            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "            aspect = lines[i + 1].lower().strip()\n",
        "            polarity = lines[i + 2].strip()\n",
        "\n",
        "            text_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
        "            context_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
        "            left_indices = tokenizer.text_to_sequence(text_left)\n",
        "            left_with_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect)\n",
        "            right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
        "            right_with_aspect_indices = tokenizer.text_to_sequence(aspect + \" \" + text_right, reverse=True)\n",
        "            aspect_indices = tokenizer.text_to_sequence(aspect)\n",
        "            left_len = np.sum(left_indices != 0)\n",
        "            aspect_len = np.sum(aspect_indices != 0)\n",
        "            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype=np.int64)\n",
        "            polarity = int(polarity) + 1\n",
        "\n",
        "            text_len = np.sum(text_indices != 0)\n",
        "            concat_segments_indices = [0] * (text_len + 2) + [1] * (aspect_len + 1)\n",
        "            concat_segments_indices = pad_and_truncate(concat_segments_indices, tokenizer.max_seq_len)\n",
        "\n",
        "            dependency_graph = np.pad(idx2graph[i], \\\n",
        "                ((0,tokenizer.max_seq_len-idx2graph[i].shape[0]),(0,tokenizer.max_seq_len-idx2graph[i].shape[0])), 'constant')\n",
        "\n",
        "            data = {\n",
        "                'text_indices': text_indices,\n",
        "                'context_indices': context_indices,\n",
        "                'left_indices': left_indices,\n",
        "                'left_with_aspect_indices': left_with_aspect_indices,\n",
        "                'right_indices': right_indices,\n",
        "                'right_with_aspect_indices': right_with_aspect_indices,\n",
        "                'aspect_indices': aspect_indices,\n",
        "                'aspect_boundary': aspect_boundary,\n",
        "                'dependency_graph': dependency_graph,\n",
        "                'polarity': polarity,\n",
        "            }\n",
        "\n",
        "            all_data.append(data)\n",
        "        self.data = all_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "julOof-E-0Ry"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, max_seq_len, lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 1\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unknownidx = len(self.word2idx)+1\n",
        "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)"
      ],
      "metadata": {
        "id": "t_BKpmMdqNuW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split()\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "H3kxztDgDc2t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "fuiAR0ng6vID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n",
        "                 bidirectional=False, only_use_last_hidden_state=False, rnn_type = 'LSTM'):\n",
        "        \"\"\"\n",
        "        LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, length...).\n",
        "        :param input_size:The number of expected features in the input x\n",
        "        :param hidden_size:The number of features in the hidden state h\n",
        "        :param num_layers:Number of recurrent layers.\n",
        "        :param bias:If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "        :param batch_first:If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "        :param dropout:If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
        "        :param bidirectional:If True, becomes a bidirectional RNN. Default: False\n",
        "        :param rnn_type: {LSTM, GRU, RNN}\n",
        "        \"\"\"\n",
        "        super(DynamicLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.batch_first = batch_first\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.only_use_last_hidden_state = only_use_last_hidden_state\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        if self.rnn_type == 'LSTM': \n",
        "            self.RNN = nn.LSTM(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)  \n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.RNN = nn.GRU(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
        "        elif self.rnn_type == 'RNN':\n",
        "            self.RNN = nn.RNN(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack ->process using RNN -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
        "        \n",
        "        # process using the selected RNN\n",
        "        if self.rnn_type == 'LSTM': \n",
        "            out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n",
        "        else: \n",
        "            out_pack, ht = self.RNN(x_emb_p, None)\n",
        "            ct = None\n",
        "        \"\"\"unsort: h\"\"\"\n",
        "        ht = torch.transpose(ht, 0, 1)[\n",
        "            x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n",
        "        ht = torch.transpose(ht, 0, 1)\n",
        "\n",
        "        if self.only_use_last_hidden_state:\n",
        "            return ht\n",
        "        else:\n",
        "            \"\"\"unpack: out\"\"\"\n",
        "            out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "            out = out[0]  #\n",
        "            out = out[x_unsort_idx]\n",
        "            \"\"\"unsort: out c\"\"\"\n",
        "            if self.rnn_type =='LSTM':\n",
        "                ct = torch.transpose(ct, 0, 1)[\n",
        "                    x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n",
        "                ct = torch.transpose(ct, 0, 1)\n",
        "\n",
        "            return out, (ht, ct)"
      ],
      "metadata": {
        "id": "qQsLY_w-8Bep"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ATAE_LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(ATAE_LSTM, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.squeeze_embedding = SqueezeEmbedding()\n",
        "        self.lstm = DynamicLSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=3, batch_first=True)\n",
        "        self.attention = NoQueryAttention(config2[\"hidden_dim\"]+config2[\"embed_dim\"], score_function='bi_linear')\n",
        "        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        text_indices, aspect_indices = inputs[0], inputs[1]\n",
        "        x_len = torch.sum(text_indices != 0, dim=-1)\n",
        "        x_len_max = torch.max(x_len)\n",
        "        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n",
        "\n",
        "        x = self.embed(text_indices)\n",
        "        x = self.squeeze_embedding(x, x_len)\n",
        "        aspect = self.embed(aspect_indices)\n",
        "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n",
        "        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n",
        "        x = torch.cat((aspect, x), dim=-1)\n",
        "\n",
        "        h, (_, _) = self.lstm(x, x_len)\n",
        "        ha = torch.cat((h, aspect), dim=-1)\n",
        "        _, score = self.attention(ha)\n",
        "        output = torch.squeeze(torch.bmm(score, h), dim=1)\n",
        "\n",
        "        out = self.dense(output)\n",
        "        return out"
      ],
      "metadata": {
        "id": "V3WeKKJK8ES1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vDMVKKkHmnJP"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):\n",
        "        ''' Attention Mechanism\n",
        "        :param embed_dim:\n",
        "        :param hidden_dim:\n",
        "        :param out_dim:\n",
        "        :param n_head: num of head (Multi-Head Attention)\n",
        "        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)\n",
        "        :return (?, q_len, out_dim,)\n",
        "        '''\n",
        "        super(Attention, self).__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = embed_dim // n_head\n",
        "        if out_dim is None:\n",
        "            out_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_head = n_head\n",
        "        self.score_function = score_function\n",
        "        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if score_function == 'mlp':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        else:  # dot_product / scaled_dot_product\n",
        "            self.register_parameter('weight', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.hidden_dim)\n",
        "        if self.weight is not None:\n",
        "            self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, q):\n",
        "        if len(q.shape) == 2:  # q_len missing\n",
        "            q = torch.unsqueeze(q, dim=1)\n",
        "        if len(k.shape) == 2:  # k_len missing\n",
        "            k = torch.unsqueeze(k, dim=1)\n",
        "        mb_size = k.shape[0]  # ?\n",
        "        k_len = k.shape[1]\n",
        "        q_len = q.shape[1]\n",
        "        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n",
        "        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n",
        "        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n",
        "        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n",
        "        if self.score_function == 'dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qx, kt)\n",
        "        elif self.score_function == 'scaled_dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            qkt = torch.bmm(qx, kt)\n",
        "            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n",
        "        elif self.score_function == 'mlp':\n",
        "            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)\n",
        "            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)\n",
        "            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)\n",
        "            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)\n",
        "            score = F.tanh(torch.matmul(kq, self.weight))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            qw = torch.matmul(qx, self.weight)\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qw, kt)\n",
        "        else:\n",
        "            raise RuntimeError('invalid score_function')\n",
        "        score = F.softmax(score, dim=-1)\n",
        "        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)\n",
        "        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)\n",
        "        output = self.proj(output)  # (?, q_len, out_dim)\n",
        "        output = self.dropout(output)\n",
        "        return output, score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoQueryAttention(Attention):\n",
        "    '''q is a parameter'''\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', q_len=1, dropout=0):\n",
        "        super(NoQueryAttention, self).__init__(embed_dim, hidden_dim, out_dim, n_head, score_function, dropout)\n",
        "        self.q_len = q_len\n",
        "        self.q = nn.Parameter(torch.Tensor(q_len, embed_dim))\n",
        "        self.reset_q()\n",
        "\n",
        "    def reset_q(self):\n",
        "        stdv = 1. / math.sqrt(self.embed_dim)\n",
        "        self.q.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, **kwargs):\n",
        "        mb_size = k.shape[0]\n",
        "        q = self.q.expand(mb_size, -1, -1)\n",
        "        return super(NoQueryAttention, self).forward(k, q)"
      ],
      "metadata": {
        "id": "aOhD2c_x8MeN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze sequence embedding length to the longest one in the batch\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_first=True):\n",
        "        super(SqueezeEmbedding, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n",
        "        \"\"\"unpack: out\"\"\"\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "        out = out[0]  #\n",
        "        \"\"\"unsort\"\"\"\n",
        "        out = out[x_unsort_idx]\n",
        "        return out"
      ],
      "metadata": {
        "id": "7TlMbCBN8HdH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main**"
      ],
      "metadata": {
        "id": "6BmgF1FSp32r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Instructor:\n",
        "    def __init__(self):\n",
        "        tokenizer = build_tokenizer(\n",
        "            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n",
        "            max_seq_len=config2[\"max_seq_len\"],\n",
        "            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n",
        "        embedding_matrix = build_embedding_matrix(\n",
        "            word2idx=tokenizer.word2idx,\n",
        "            embed_dim=config2[\"embed_dim\"],\n",
        "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n",
        "        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n",
        "\n",
        "        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n",
        "        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n",
        "        assert 0 <= config2[\"valset_ratio\"] < 1\n",
        "        if config2[\"valset_ratio\"] > 0:\n",
        "            valset_len = int(len(self.trainset) * config2[\"valset_ratio\"])\n",
        "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
        "        else:\n",
        "            self.valset = self.testset\n",
        "\n",
        "        if config2[\"device\"].type == 'cuda':\n",
        "            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n",
        "        self._print_args()\n",
        "\n",
        "    def _print_args(self):\n",
        "        n_trainable_params, n_nontrainable_params = 0, 0\n",
        "        for p in self.model.parameters():\n",
        "            n_params = torch.prod(torch.tensor(p.shape))\n",
        "            if p.requires_grad:\n",
        "                n_trainable_params += n_params\n",
        "            else:\n",
        "                n_nontrainable_params += n_params\n",
        "        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
        "\n",
        "\n",
        "    def _reset_params(self):\n",
        "        for child in self.model.children():\n",
        "            for p in child.parameters():\n",
        "                if p.requires_grad:\n",
        "                    if len(p.shape) > 1:\n",
        "                        config2[\"initializer\"](p)\n",
        "                    else:\n",
        "                        stdv = 1. / math.sqrt(p.shape[0])\n",
        "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "\n",
        "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
        "        max_val_acc = 0\n",
        "        max_val_f1 = 0\n",
        "        max_val_epoch = 0\n",
        "        global_step = 0\n",
        "        path = None\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        for i_epoch in range(config2[\"num_epoch\"]):\n",
        "            print('>' * 100)\n",
        "            print('epoch: {}'.format(i_epoch))\n",
        "            n_correct, n_total, loss_total = 0, 0, 0\n",
        "            # switch model to training mode\n",
        "            self.model.train()\n",
        "            for i_batch, batch in enumerate(train_data_loader):\n",
        "                global_step += 1\n",
        "                # clear gradient accumulators\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                global outputs, targets\n",
        "                outputs = self.model(inputs)\n",
        "                targets = batch['polarity'].to(config2[\"device\"])\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
        "                n_total += len(outputs)\n",
        "                loss_total += loss.item() * len(outputs)\n",
        "                if global_step % config2[\"log_step\"] == 0:\n",
        "                    train_acc = n_correct / n_total\n",
        "                    train_loss = loss_total / n_total\n",
        "                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n",
        "\n",
        "            val_acc, val_loss, _, val_f1, _, _, _, _, _, _, _, _ = self._evaluate(criterion, val_data_loader)\n",
        "            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                max_val_epoch = i_epoch\n",
        "                if not os.path.exists('state_dict'):\n",
        "                    os.mkdir('state_dict')\n",
        "                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n",
        "                torch.save(self.model.state_dict(), path)\n",
        "                print('>> saved: {}'.format(path))\n",
        "            if val_f1 > max_val_f1:\n",
        "                max_val_f1 = val_f1\n",
        "            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n",
        "                print('>> early stop.')\n",
        "                break\n",
        "            train_losses.append(loss_total / n_total)\n",
        "            train_accuracies.append(n_correct / n_total)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        return path, train_losses, train_accuracies, val_losses, val_accuracies\n",
        "\n",
        "    def _evaluate(self, criterion, data_loader):\n",
        "        n_correct, n_total, loss_total = 0, 0, 0\n",
        "        t_targets_all, t_outputs_all = None, None\n",
        "        # switch model to evaluation mode\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i_batch, t_batch in enumerate(data_loader):\n",
        "                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                t_targets = t_batch['polarity'].to(config2[\"device\"])\n",
        "                t_outputs = self.model(t_inputs)\n",
        "\n",
        "                loss = criterion(t_outputs, t_targets)\n",
        "\n",
        "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
        "                n_total += len(t_outputs)\n",
        "                loss_total += loss.item() * len(t_outputs)\n",
        "\n",
        "                if t_targets_all is None:\n",
        "                    t_targets_all = t_targets\n",
        "                    t_outputs_all = t_outputs\n",
        "                else:\n",
        "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
        "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
        "\n",
        "        acc = n_correct / n_total\n",
        "        loss = loss_total / n_total\n",
        "        f1_macro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
        "        f1_micro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='micro')\n",
        "        f1_weighted = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='weighted')\n",
        "        precision_macro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        precision_micro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        precision_weighted = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        recall_macro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        recall_micro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        recall_weighted = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        confusion_matrix = metrics.confusion_matrix(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2])\n",
        "        return acc, loss, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix\n",
        "\n",
        "    def run(self):\n",
        "        # Loss and Optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n",
        "\n",
        "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n",
        "        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "\n",
        "        self._reset_params()\n",
        "        best_model_path, train_losses, train_accuracies, val_losses, val_accuracies = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "        acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = self._evaluate(nn.CrossEntropyLoss(), test_data_loader)\n",
        "        print('>> test_acc: {:.4f}'.format(acc))\n",
        "        print('>> test_f1_macro: {:.4f}'.format(f1_macro))\n",
        "        print('>> test_f1_micro: {:.4f}'.format(f1_micro))\n",
        "        print('>> test_f1_weighted: {:.4f}'.format(f1_weighted))\n",
        "        print('>> test_precision_macro: {:.4f}'.format(precision_macro))\n",
        "        print('>> test_precision_micro: {:.4f}'.format(precision_micro))\n",
        "        print('>> test_precision_weighted: {:.4f}'.format(precision_weighted))\n",
        "        print('>> test_recall_macro: {:.4f}'.format(recall_macro))\n",
        "        print('>> test_recall_micro: {:.4f}'.format(recall_micro))\n",
        "        print('>> test_recall_weighted: {:.4f}'.format(recall_weighted))\n",
        "        print('confusion matrix:')\n",
        "        print(confusion_matrix)\n",
        "\n",
        "        epochs = len(train_losses)\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        for i, metrics in enumerate(zip([train_losses, train_accuracies], [val_losses, val_accuracies], ['Loss', 'Accuracy'])):\n",
        "            plt.subplot(1, 2, i + 1)\n",
        "            plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "            plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "            plt.legend()\n",
        "        plt.show()\n",
        "        plt.savefig('lstm_accuracy.png')\n",
        "        return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "mPRxQS-Jp5vP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "bv09Mo677CkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_format_sentences(source_path, target_path):\n",
        "    f = open(target_path, \"w\")\n",
        "\n",
        "    sentences = parse(source_path).getroot()\n",
        "    preprocessed = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            if polarity == \"positive\":\n",
        "                polarity = '1'\n",
        "            elif polarity == \"neutral\":\n",
        "                polarity = '0'\n",
        "            elif polarity == \"negative\":\n",
        "                polarity = '-1'\n",
        "            else:\n",
        "                raise Exception(\"invalid polarity!\")\n",
        "            start = int(aspectTerm.get('from'))\n",
        "            end = int(aspectTerm.get('to'))\n",
        "            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n",
        "            preprocessed.append(text[start:end])\n",
        "            preprocessed.append(polarity)\n",
        "    f.write(\"\\n\".join(preprocessed))"
      ],
      "metadata": {
        "id": "Mlhjev_wrkDF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading tokenizer:', dat_fname)\n",
        "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        text = ''\n",
        "        for fname in fnames:\n",
        "            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "            lines = fin.readlines()\n",
        "            fin.close()\n",
        "            for i in range(0, len(lines), 3):\n",
        "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "                aspect = lines[i + 1].lower().strip()\n",
        "                text_raw = text_left + \" \" + aspect + \" \" + text_right\n",
        "                text += text_raw + \" \"\n",
        "\n",
        "        tokenizer = Tokenizer(max_seq_len)\n",
        "        tokenizer.fit_on_text(text)\n",
        "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "pV174mP37UZy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_word_vec(path, word2idx=None, embed_dim=300):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n",
        "        if word in word2idx.keys():\n",
        "            word_vec[word] = np.asarray(vec, dtype='float32')\n",
        "    return word_vec"
      ],
      "metadata": {
        "id": "2h-2UjGs7fu5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix:', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = config[\"glove_path\"]\n",
        "        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "nB3jz5Fk7dZd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "7JH84-Kx7Y-_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dependency_adj_matrix(text):\n",
        "    # https://spacy.io/docs/usage/processing-text\n",
        "    tokens = nlp(text)\n",
        "    words = text.split()\n",
        "    matrix = np.zeros((len(words), len(words))).astype('float32')\n",
        "    assert len(words) == len(list(tokens))\n",
        "\n",
        "    for token in tokens:\n",
        "        matrix[token.i][token.i] = 1\n",
        "        for child in token.children:\n",
        "            matrix[token.i][child.i] = 1\n",
        "            matrix[child.i][token.i] = 1\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "VemwD4FB7sY1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n",
        "    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n",
        "    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n",
        "    process(config[\"processed_train_path\"])\n",
        "    process(config[\"processed_val_path\"])\n",
        "    process(config[\"processed_test_path\"])"
      ],
      "metadata": {
        "id": "vC45U4k21oPd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(filename):\n",
        "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    lines = fin.readlines()\n",
        "    fin.close()\n",
        "    idx2graph = {}\n",
        "    fout = open(filename+'.graph', 'wb')\n",
        "    for i in range(0, len(lines), 3):\n",
        "        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n",
        "        aspect = lines[i + 1].strip()\n",
        "        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n",
        "        idx2graph[i] = adj_matrix\n",
        "    pickle.dump(idx2graph, fout)        \n",
        "    fout.close() "
      ],
      "metadata": {
        "id": "HlLsFiUJ7mPH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    ins = Instructor()\n",
        "    best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = ins.run()\n",
        "    return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "ANtAieVU8lwS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN**"
      ],
      "metadata": {
        "id": "cxb8D05u8UN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "5QyUuFxj8XuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "AF3DYep7AW8k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"base_path\": \"drive/MyDrive/CS4248/MAMS-ATSA\",\n",
        "    \"glove_path\": \"drive/MyDrive/CS4248/glove.42B.300d.txt\"\n",
        "}\n",
        "\n",
        "config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n",
        "config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n",
        "config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n",
        "config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n",
        "config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n",
        "config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n",
        "config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"
      ],
      "metadata": {
        "id": "9y9MO-RW8bcS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_files = {\n",
        "    'train': config[\"processed_train_path\"],\n",
        "    'test': config[\"processed_test_path\"]\n",
        "}\n",
        "\n",
        "config2 = {\n",
        "    \"model_name\" : \"ATAE_LSTM\",\n",
        "    \"lr\" : 3e-5,\n",
        "    \"dropout\" : 0.1,\n",
        "    \"l2reg\" : 0.001,\n",
        "    \"num_epoch\" : 20,\n",
        "    \"batch_size\" : 25,\n",
        "    \"log_step\" : 10,\n",
        "    \"embed_dim\" : 300,\n",
        "    \"hidden_dim\" : 300,\n",
        "    \"model_class\" : ATAE_LSTM,\n",
        "    \"dataset\": \"MAMS\",\n",
        "    \"dataset_file\" : dataset_files,\n",
        "    \"inputs_cols\" : ['text_indices', 'aspect_indices'],\n",
        "    \"initializer\" : torch.nn.init.xavier_uniform_,\n",
        "    \"optimizer\" : torch.optim.Adam,\n",
        "    \"max_seq_len\" : 85,\n",
        "    \"polarities_dim\" : 3,\n",
        "    \"patience\" : 5, # for early stopping\n",
        "    \"device\" : None,\n",
        "    \"seed\" : 1234,\n",
        "    \"valset_ratio\" : 0\n",
        "}\n",
        "\n",
        "config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if config2[\"seed\"] is not None:\n",
        "    random.seed(config2[\"seed\"])\n",
        "    np.random.seed(config2[\"seed\"])\n",
        "    torch.manual_seed(config2[\"seed\"])\n",
        "    torch.cuda.manual_seed(config2[\"seed\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"
      ],
      "metadata": {
        "id": "clOIqT523tWy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**"
      ],
      "metadata": {
        "id": "3fT6RqeiAGmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this line only for the first time you run this file\n",
        "# process_data()"
      ],
      "metadata": {
        "id": "vT4xfKSC-Xpx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = run()"
      ],
      "metadata": {
        "id": "8gEIkSn38pHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5e4d8ef-5196-4339-e6b4-8745f9026521"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word vectors...\n",
            "building embedding_matrix: 300_MAMS_embedding_matrix.dat\n",
            "> n_trainable_params: 3970503, n_nontrainable_params: 3994500\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 0\n",
            "[epoch 0] loss: 1.1607, acc: 0.1920\n",
            "[epoch 0] loss: 1.1414, acc: 0.2240\n",
            "[epoch 0] loss: 1.1356, acc: 0.2320\n",
            "[epoch 0] loss: 1.1282, acc: 0.2420\n",
            "[epoch 0] loss: 1.1202, acc: 0.2688\n",
            "[epoch 0] loss: 1.1158, acc: 0.2860\n",
            "[epoch 0] loss: 1.1108, acc: 0.3023\n",
            "[epoch 0] loss: 1.1037, acc: 0.3185\n",
            "[epoch 0] loss: 1.0970, acc: 0.3329\n",
            "[epoch 0] loss: 1.0900, acc: 0.3428\n",
            "[epoch 0] loss: 1.0858, acc: 0.3495\n",
            "[epoch 0] loss: 1.0796, acc: 0.3613\n",
            "[epoch 0] loss: 1.0728, acc: 0.3726\n",
            "[epoch 0] loss: 1.0658, acc: 0.3843\n",
            "[epoch 0] loss: 1.0587, acc: 0.3949\n",
            "[epoch 0] loss: 1.0531, acc: 0.4027\n",
            "[epoch 0] loss: 1.0461, acc: 0.4118\n",
            "[epoch 0] loss: 1.0416, acc: 0.4187\n",
            "[epoch 0] loss: 1.0346, acc: 0.4272\n",
            "[epoch 0] loss: 1.0310, acc: 0.4328\n",
            "[epoch 0] loss: 1.0285, acc: 0.4370\n",
            "[epoch 0] loss: 1.0209, acc: 0.4445\n",
            "[epoch 0] loss: 1.0155, acc: 0.4494\n",
            "[epoch 0] loss: 1.0136, acc: 0.4515\n",
            "[epoch 0] loss: 1.0100, acc: 0.4558\n",
            "[epoch 0] loss: 1.0061, acc: 0.4617\n",
            "[epoch 0] loss: 1.0016, acc: 0.4674\n",
            "[epoch 0] loss: 0.9985, acc: 0.4727\n",
            "[epoch 0] loss: 0.9953, acc: 0.4770\n",
            "[epoch 0] loss: 0.9928, acc: 0.4803\n",
            "[epoch 0] loss: 0.9890, acc: 0.4844\n",
            "[epoch 0] loss: 0.9854, acc: 0.4895\n",
            "[epoch 0] loss: 0.9821, acc: 0.4935\n",
            "[epoch 0] loss: 0.9771, acc: 0.4988\n",
            "[epoch 0] loss: 0.9746, acc: 0.5010\n",
            "[epoch 0] loss: 0.9719, acc: 0.5036\n",
            "[epoch 0] loss: 0.9705, acc: 0.5050\n",
            "[epoch 0] loss: 0.9683, acc: 0.5064\n",
            "[epoch 0] loss: 0.9646, acc: 0.5092\n",
            "[epoch 0] loss: 0.9611, acc: 0.5120\n",
            "[epoch 0] loss: 0.9604, acc: 0.5132\n",
            "[epoch 0] loss: 0.9580, acc: 0.5158\n",
            "[epoch 0] loss: 0.9557, acc: 0.5173\n",
            "[epoch 0] loss: 0.9545, acc: 0.5185\n",
            "> val_acc: 0.6048, val_f1: 0.6048\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 1\n",
            "[epoch 1] loss: 0.8068, acc: 0.6800\n",
            "[epoch 1] loss: 0.8125, acc: 0.6333\n",
            "[epoch 1] loss: 0.8295, acc: 0.6273\n",
            "[epoch 1] loss: 0.8345, acc: 0.6250\n",
            "[epoch 1] loss: 0.8392, acc: 0.6171\n",
            "[epoch 1] loss: 0.8472, acc: 0.6062\n",
            "[epoch 1] loss: 0.8309, acc: 0.6194\n",
            "[epoch 1] loss: 0.8358, acc: 0.6233\n",
            "[epoch 1] loss: 0.8387, acc: 0.6224\n",
            "[epoch 1] loss: 0.8377, acc: 0.6230\n",
            "[epoch 1] loss: 0.8399, acc: 0.6227\n",
            "[epoch 1] loss: 0.8407, acc: 0.6211\n",
            "[epoch 1] loss: 0.8401, acc: 0.6213\n",
            "[epoch 1] loss: 0.8409, acc: 0.6206\n",
            "[epoch 1] loss: 0.8432, acc: 0.6180\n",
            "[epoch 1] loss: 0.8378, acc: 0.6216\n",
            "[epoch 1] loss: 0.8385, acc: 0.6202\n",
            "[epoch 1] loss: 0.8386, acc: 0.6207\n",
            "[epoch 1] loss: 0.8381, acc: 0.6204\n",
            "[epoch 1] loss: 0.8385, acc: 0.6206\n",
            "[epoch 1] loss: 0.8387, acc: 0.6192\n",
            "[epoch 1] loss: 0.8398, acc: 0.6168\n",
            "[epoch 1] loss: 0.8396, acc: 0.6160\n",
            "[epoch 1] loss: 0.8383, acc: 0.6166\n",
            "[epoch 1] loss: 0.8381, acc: 0.6169\n",
            "[epoch 1] loss: 0.8379, acc: 0.6170\n",
            "[epoch 1] loss: 0.8384, acc: 0.6165\n",
            "[epoch 1] loss: 0.8407, acc: 0.6146\n",
            "[epoch 1] loss: 0.8432, acc: 0.6121\n",
            "[epoch 1] loss: 0.8431, acc: 0.6126\n",
            "[epoch 1] loss: 0.8424, acc: 0.6132\n",
            "[epoch 1] loss: 0.8402, acc: 0.6149\n",
            "[epoch 1] loss: 0.8372, acc: 0.6166\n",
            "[epoch 1] loss: 0.8383, acc: 0.6172\n",
            "[epoch 1] loss: 0.8364, acc: 0.6188\n",
            "[epoch 1] loss: 0.8366, acc: 0.6183\n",
            "[epoch 1] loss: 0.8374, acc: 0.6170\n",
            "[epoch 1] loss: 0.8375, acc: 0.6160\n",
            "[epoch 1] loss: 0.8400, acc: 0.6142\n",
            "[epoch 1] loss: 0.8397, acc: 0.6139\n",
            "[epoch 1] loss: 0.8382, acc: 0.6147\n",
            "[epoch 1] loss: 0.8384, acc: 0.6155\n",
            "[epoch 1] loss: 0.8373, acc: 0.6172\n",
            "[epoch 1] loss: 0.8374, acc: 0.6171\n",
            "[epoch 1] loss: 0.8383, acc: 0.6166\n",
            "> val_acc: 0.6287, val_f1: 0.6287\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6287\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 2\n",
            "[epoch 2] loss: 0.8177, acc: 0.6200\n",
            "[epoch 2] loss: 0.7853, acc: 0.6600\n",
            "[epoch 2] loss: 0.8232, acc: 0.6200\n",
            "[epoch 2] loss: 0.8346, acc: 0.6129\n",
            "[epoch 2] loss: 0.8270, acc: 0.6236\n",
            "[epoch 2] loss: 0.8279, acc: 0.6222\n",
            "[epoch 2] loss: 0.8220, acc: 0.6225\n",
            "[epoch 2] loss: 0.8272, acc: 0.6124\n",
            "[epoch 2] loss: 0.8185, acc: 0.6195\n",
            "[epoch 2] loss: 0.8206, acc: 0.6226\n",
            "[epoch 2] loss: 0.8154, acc: 0.6250\n",
            "[epoch 2] loss: 0.8082, acc: 0.6277\n",
            "[epoch 2] loss: 0.8052, acc: 0.6303\n",
            "[epoch 2] loss: 0.8072, acc: 0.6313\n",
            "[epoch 2] loss: 0.8013, acc: 0.6347\n",
            "[epoch 2] loss: 0.8014, acc: 0.6335\n",
            "[epoch 2] loss: 0.7980, acc: 0.6349\n",
            "[epoch 2] loss: 0.7968, acc: 0.6338\n",
            "[epoch 2] loss: 0.7965, acc: 0.6330\n",
            "[epoch 2] loss: 0.7995, acc: 0.6311\n",
            "[epoch 2] loss: 0.7994, acc: 0.6324\n",
            "[epoch 2] loss: 0.7977, acc: 0.6342\n",
            "[epoch 2] loss: 0.7958, acc: 0.6384\n",
            "[epoch 2] loss: 0.7923, acc: 0.6393\n",
            "[epoch 2] loss: 0.7897, acc: 0.6418\n",
            "[epoch 2] loss: 0.7928, acc: 0.6398\n",
            "[epoch 2] loss: 0.7947, acc: 0.6389\n",
            "[epoch 2] loss: 0.7948, acc: 0.6390\n",
            "[epoch 2] loss: 0.7920, acc: 0.6397\n",
            "[epoch 2] loss: 0.7915, acc: 0.6403\n",
            "[epoch 2] loss: 0.7954, acc: 0.6387\n",
            "[epoch 2] loss: 0.7985, acc: 0.6373\n",
            "[epoch 2] loss: 0.8008, acc: 0.6360\n",
            "[epoch 2] loss: 0.8018, acc: 0.6354\n",
            "[epoch 2] loss: 0.8035, acc: 0.6341\n",
            "[epoch 2] loss: 0.8029, acc: 0.6350\n",
            "[epoch 2] loss: 0.8023, acc: 0.6357\n",
            "[epoch 2] loss: 0.8030, acc: 0.6348\n",
            "[epoch 2] loss: 0.8031, acc: 0.6347\n",
            "[epoch 2] loss: 0.8033, acc: 0.6347\n",
            "[epoch 2] loss: 0.8043, acc: 0.6344\n",
            "[epoch 2] loss: 0.8055, acc: 0.6345\n",
            "[epoch 2] loss: 0.8048, acc: 0.6345\n",
            "[epoch 2] loss: 0.8050, acc: 0.6347\n",
            "[epoch 2] loss: 0.8051, acc: 0.6344\n",
            "> val_acc: 0.6437, val_f1: 0.6437\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6437\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 3\n",
            "[epoch 3] loss: 0.7505, acc: 0.6800\n",
            "[epoch 3] loss: 0.7255, acc: 0.6675\n",
            "[epoch 3] loss: 0.7438, acc: 0.6631\n",
            "[epoch 3] loss: 0.7700, acc: 0.6556\n",
            "[epoch 3] loss: 0.7601, acc: 0.6591\n",
            "[epoch 3] loss: 0.7526, acc: 0.6586\n",
            "[epoch 3] loss: 0.7525, acc: 0.6594\n",
            "[epoch 3] loss: 0.7559, acc: 0.6574\n",
            "[epoch 3] loss: 0.7617, acc: 0.6572\n",
            "[epoch 3] loss: 0.7668, acc: 0.6538\n",
            "[epoch 3] loss: 0.7648, acc: 0.6558\n",
            "[epoch 3] loss: 0.7577, acc: 0.6597\n",
            "[epoch 3] loss: 0.7556, acc: 0.6622\n",
            "[epoch 3] loss: 0.7599, acc: 0.6615\n",
            "[epoch 3] loss: 0.7605, acc: 0.6616\n",
            "[epoch 3] loss: 0.7652, acc: 0.6608\n",
            "[epoch 3] loss: 0.7708, acc: 0.6569\n",
            "[epoch 3] loss: 0.7708, acc: 0.6570\n",
            "[epoch 3] loss: 0.7710, acc: 0.6563\n",
            "[epoch 3] loss: 0.7716, acc: 0.6563\n",
            "[epoch 3] loss: 0.7701, acc: 0.6581\n",
            "[epoch 3] loss: 0.7691, acc: 0.6600\n",
            "[epoch 3] loss: 0.7703, acc: 0.6584\n",
            "[epoch 3] loss: 0.7692, acc: 0.6590\n",
            "[epoch 3] loss: 0.7753, acc: 0.6556\n",
            "[epoch 3] loss: 0.7774, acc: 0.6553\n",
            "[epoch 3] loss: 0.7761, acc: 0.6570\n",
            "[epoch 3] loss: 0.7765, acc: 0.6570\n",
            "[epoch 3] loss: 0.7799, acc: 0.6543\n",
            "[epoch 3] loss: 0.7800, acc: 0.6546\n",
            "[epoch 3] loss: 0.7816, acc: 0.6528\n",
            "[epoch 3] loss: 0.7811, acc: 0.6523\n",
            "[epoch 3] loss: 0.7829, acc: 0.6501\n",
            "[epoch 3] loss: 0.7831, acc: 0.6495\n",
            "[epoch 3] loss: 0.7828, acc: 0.6492\n",
            "[epoch 3] loss: 0.7836, acc: 0.6493\n",
            "[epoch 3] loss: 0.7849, acc: 0.6486\n",
            "[epoch 3] loss: 0.7846, acc: 0.6487\n",
            "[epoch 3] loss: 0.7845, acc: 0.6482\n",
            "[epoch 3] loss: 0.7855, acc: 0.6475\n",
            "[epoch 3] loss: 0.7865, acc: 0.6468\n",
            "[epoch 3] loss: 0.7857, acc: 0.6470\n",
            "[epoch 3] loss: 0.7856, acc: 0.6470\n",
            "[epoch 3] loss: 0.7859, acc: 0.6468\n",
            "[epoch 3] loss: 0.7862, acc: 0.6467\n",
            "> val_acc: 0.6460, val_f1: 0.6460\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.646\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 4\n",
            "[epoch 4] loss: 0.7669, acc: 0.6750\n",
            "[epoch 4] loss: 0.7659, acc: 0.6756\n",
            "[epoch 4] loss: 0.7599, acc: 0.6743\n",
            "[epoch 4] loss: 0.7776, acc: 0.6611\n",
            "[epoch 4] loss: 0.7855, acc: 0.6483\n",
            "[epoch 4] loss: 0.7776, acc: 0.6490\n",
            "[epoch 4] loss: 0.7696, acc: 0.6553\n",
            "[epoch 4] loss: 0.7648, acc: 0.6564\n",
            "[epoch 4] loss: 0.7560, acc: 0.6636\n",
            "[epoch 4] loss: 0.7522, acc: 0.6669\n",
            "[epoch 4] loss: 0.7596, acc: 0.6630\n",
            "[epoch 4] loss: 0.7658, acc: 0.6603\n",
            "[epoch 4] loss: 0.7624, acc: 0.6613\n",
            "[epoch 4] loss: 0.7694, acc: 0.6580\n",
            "[epoch 4] loss: 0.7657, acc: 0.6592\n",
            "[epoch 4] loss: 0.7684, acc: 0.6580\n",
            "[epoch 4] loss: 0.7686, acc: 0.6581\n",
            "[epoch 4] loss: 0.7696, acc: 0.6575\n",
            "[epoch 4] loss: 0.7676, acc: 0.6572\n",
            "[epoch 4] loss: 0.7721, acc: 0.6541\n",
            "[epoch 4] loss: 0.7705, acc: 0.6550\n",
            "[epoch 4] loss: 0.7691, acc: 0.6561\n",
            "[epoch 4] loss: 0.7703, acc: 0.6574\n",
            "[epoch 4] loss: 0.7685, acc: 0.6583\n",
            "[epoch 4] loss: 0.7673, acc: 0.6576\n",
            "[epoch 4] loss: 0.7684, acc: 0.6550\n",
            "[epoch 4] loss: 0.7711, acc: 0.6543\n",
            "[epoch 4] loss: 0.7736, acc: 0.6541\n",
            "[epoch 4] loss: 0.7710, acc: 0.6556\n",
            "[epoch 4] loss: 0.7695, acc: 0.6560\n",
            "[epoch 4] loss: 0.7692, acc: 0.6566\n",
            "[epoch 4] loss: 0.7672, acc: 0.6572\n",
            "[epoch 4] loss: 0.7667, acc: 0.6573\n",
            "[epoch 4] loss: 0.7669, acc: 0.6578\n",
            "[epoch 4] loss: 0.7676, acc: 0.6578\n",
            "[epoch 4] loss: 0.7708, acc: 0.6560\n",
            "[epoch 4] loss: 0.7695, acc: 0.6567\n",
            "[epoch 4] loss: 0.7704, acc: 0.6557\n",
            "[epoch 4] loss: 0.7712, acc: 0.6549\n",
            "[epoch 4] loss: 0.7709, acc: 0.6546\n",
            "[epoch 4] loss: 0.7716, acc: 0.6545\n",
            "[epoch 4] loss: 0.7712, acc: 0.6547\n",
            "[epoch 4] loss: 0.7700, acc: 0.6555\n",
            "[epoch 4] loss: 0.7703, acc: 0.6551\n",
            "[epoch 4] loss: 0.7712, acc: 0.6545\n",
            "> val_acc: 0.6482, val_f1: 0.6482\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6482\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 5\n",
            "[epoch 5] loss: 0.8391, acc: 0.6360\n",
            "[epoch 5] loss: 0.8259, acc: 0.6300\n",
            "[epoch 5] loss: 0.8096, acc: 0.6373\n",
            "[epoch 5] loss: 0.7820, acc: 0.6510\n",
            "[epoch 5] loss: 0.7852, acc: 0.6480\n",
            "[epoch 5] loss: 0.7747, acc: 0.6513\n",
            "[epoch 5] loss: 0.7721, acc: 0.6549\n",
            "[epoch 5] loss: 0.7709, acc: 0.6565\n",
            "[epoch 5] loss: 0.7632, acc: 0.6578\n",
            "[epoch 5] loss: 0.7625, acc: 0.6572\n",
            "[epoch 5] loss: 0.7663, acc: 0.6564\n",
            "[epoch 5] loss: 0.7618, acc: 0.6573\n",
            "[epoch 5] loss: 0.7616, acc: 0.6588\n",
            "[epoch 5] loss: 0.7618, acc: 0.6589\n",
            "[epoch 5] loss: 0.7601, acc: 0.6595\n",
            "[epoch 5] loss: 0.7604, acc: 0.6610\n",
            "[epoch 5] loss: 0.7594, acc: 0.6612\n",
            "[epoch 5] loss: 0.7556, acc: 0.6616\n",
            "[epoch 5] loss: 0.7540, acc: 0.6627\n",
            "[epoch 5] loss: 0.7524, acc: 0.6628\n",
            "[epoch 5] loss: 0.7535, acc: 0.6636\n",
            "[epoch 5] loss: 0.7534, acc: 0.6633\n",
            "[epoch 5] loss: 0.7520, acc: 0.6633\n",
            "[epoch 5] loss: 0.7496, acc: 0.6655\n",
            "[epoch 5] loss: 0.7500, acc: 0.6656\n",
            "[epoch 5] loss: 0.7485, acc: 0.6671\n",
            "[epoch 5] loss: 0.7479, acc: 0.6673\n",
            "[epoch 5] loss: 0.7508, acc: 0.6654\n",
            "[epoch 5] loss: 0.7497, acc: 0.6659\n",
            "[epoch 5] loss: 0.7500, acc: 0.6657\n",
            "[epoch 5] loss: 0.7470, acc: 0.6668\n",
            "[epoch 5] loss: 0.7470, acc: 0.6663\n",
            "[epoch 5] loss: 0.7499, acc: 0.6648\n",
            "[epoch 5] loss: 0.7511, acc: 0.6639\n",
            "[epoch 5] loss: 0.7535, acc: 0.6625\n",
            "[epoch 5] loss: 0.7532, acc: 0.6630\n",
            "[epoch 5] loss: 0.7520, acc: 0.6637\n",
            "[epoch 5] loss: 0.7529, acc: 0.6632\n",
            "[epoch 5] loss: 0.7529, acc: 0.6635\n",
            "[epoch 5] loss: 0.7537, acc: 0.6632\n",
            "[epoch 5] loss: 0.7540, acc: 0.6629\n",
            "[epoch 5] loss: 0.7539, acc: 0.6630\n",
            "[epoch 5] loss: 0.7542, acc: 0.6625\n",
            "[epoch 5] loss: 0.7564, acc: 0.6609\n",
            "> val_acc: 0.6340, val_f1: 0.6340\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 6\n",
            "[epoch 6] loss: 0.7773, acc: 0.6400\n",
            "[epoch 6] loss: 0.7071, acc: 0.6900\n",
            "[epoch 6] loss: 0.7225, acc: 0.6764\n",
            "[epoch 6] loss: 0.7334, acc: 0.6775\n",
            "[epoch 6] loss: 0.7312, acc: 0.6819\n",
            "[epoch 6] loss: 0.7327, acc: 0.6823\n",
            "[epoch 6] loss: 0.7359, acc: 0.6787\n",
            "[epoch 6] loss: 0.7322, acc: 0.6778\n",
            "[epoch 6] loss: 0.7326, acc: 0.6766\n",
            "[epoch 6] loss: 0.7421, acc: 0.6722\n",
            "[epoch 6] loss: 0.7415, acc: 0.6710\n",
            "[epoch 6] loss: 0.7391, acc: 0.6736\n",
            "[epoch 6] loss: 0.7358, acc: 0.6751\n",
            "[epoch 6] loss: 0.7384, acc: 0.6755\n",
            "[epoch 6] loss: 0.7382, acc: 0.6755\n",
            "[epoch 6] loss: 0.7392, acc: 0.6755\n",
            "[epoch 6] loss: 0.7390, acc: 0.6768\n",
            "[epoch 6] loss: 0.7431, acc: 0.6749\n",
            "[epoch 6] loss: 0.7472, acc: 0.6723\n",
            "[epoch 6] loss: 0.7460, acc: 0.6706\n",
            "[epoch 6] loss: 0.7472, acc: 0.6711\n",
            "[epoch 6] loss: 0.7433, acc: 0.6742\n",
            "[epoch 6] loss: 0.7439, acc: 0.6723\n",
            "[epoch 6] loss: 0.7400, acc: 0.6740\n",
            "[epoch 6] loss: 0.7382, acc: 0.6750\n",
            "[epoch 6] loss: 0.7374, acc: 0.6754\n",
            "[epoch 6] loss: 0.7387, acc: 0.6756\n",
            "[epoch 6] loss: 0.7378, acc: 0.6757\n",
            "[epoch 6] loss: 0.7389, acc: 0.6745\n",
            "[epoch 6] loss: 0.7397, acc: 0.6737\n",
            "[epoch 6] loss: 0.7387, acc: 0.6748\n",
            "[epoch 6] loss: 0.7431, acc: 0.6723\n",
            "[epoch 6] loss: 0.7411, acc: 0.6735\n",
            "[epoch 6] loss: 0.7426, acc: 0.6733\n",
            "[epoch 6] loss: 0.7422, acc: 0.6743\n",
            "[epoch 6] loss: 0.7416, acc: 0.6737\n",
            "[epoch 6] loss: 0.7443, acc: 0.6724\n",
            "[epoch 6] loss: 0.7462, acc: 0.6712\n",
            "[epoch 6] loss: 0.7467, acc: 0.6706\n",
            "[epoch 6] loss: 0.7488, acc: 0.6683\n",
            "[epoch 6] loss: 0.7476, acc: 0.6694\n",
            "[epoch 6] loss: 0.7473, acc: 0.6688\n",
            "[epoch 6] loss: 0.7477, acc: 0.6682\n",
            "[epoch 6] loss: 0.7468, acc: 0.6685\n",
            "[epoch 6] loss: 0.7468, acc: 0.6683\n",
            "> val_acc: 0.6482, val_f1: 0.6482\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 7\n",
            "[epoch 7] loss: 0.6903, acc: 0.6900\n",
            "[epoch 7] loss: 0.7310, acc: 0.6714\n",
            "[epoch 7] loss: 0.7485, acc: 0.6550\n",
            "[epoch 7] loss: 0.7361, acc: 0.6718\n",
            "[epoch 7] loss: 0.7271, acc: 0.6764\n",
            "[epoch 7] loss: 0.7269, acc: 0.6837\n",
            "[epoch 7] loss: 0.7281, acc: 0.6806\n",
            "[epoch 7] loss: 0.7320, acc: 0.6784\n",
            "[epoch 7] loss: 0.7229, acc: 0.6833\n",
            "[epoch 7] loss: 0.7241, acc: 0.6855\n",
            "[epoch 7] loss: 0.7258, acc: 0.6842\n",
            "[epoch 7] loss: 0.7289, acc: 0.6807\n",
            "[epoch 7] loss: 0.7274, acc: 0.6803\n",
            "[epoch 7] loss: 0.7238, acc: 0.6833\n",
            "[epoch 7] loss: 0.7279, acc: 0.6817\n",
            "[epoch 7] loss: 0.7302, acc: 0.6810\n",
            "[epoch 7] loss: 0.7301, acc: 0.6805\n",
            "[epoch 7] loss: 0.7356, acc: 0.6777\n",
            "[epoch 7] loss: 0.7378, acc: 0.6767\n",
            "[epoch 7] loss: 0.7375, acc: 0.6773\n",
            "[epoch 7] loss: 0.7368, acc: 0.6757\n",
            "[epoch 7] loss: 0.7375, acc: 0.6742\n",
            "[epoch 7] loss: 0.7376, acc: 0.6752\n",
            "[epoch 7] loss: 0.7347, acc: 0.6761\n",
            "[epoch 7] loss: 0.7353, acc: 0.6769\n",
            "[epoch 7] loss: 0.7332, acc: 0.6798\n",
            "[epoch 7] loss: 0.7322, acc: 0.6803\n",
            "[epoch 7] loss: 0.7315, acc: 0.6807\n",
            "[epoch 7] loss: 0.7321, acc: 0.6811\n",
            "[epoch 7] loss: 0.7321, acc: 0.6808\n",
            "[epoch 7] loss: 0.7322, acc: 0.6803\n",
            "[epoch 7] loss: 0.7330, acc: 0.6790\n",
            "[epoch 7] loss: 0.7319, acc: 0.6796\n",
            "[epoch 7] loss: 0.7307, acc: 0.6804\n",
            "[epoch 7] loss: 0.7316, acc: 0.6807\n",
            "[epoch 7] loss: 0.7306, acc: 0.6812\n",
            "[epoch 7] loss: 0.7311, acc: 0.6811\n",
            "[epoch 7] loss: 0.7305, acc: 0.6820\n",
            "[epoch 7] loss: 0.7301, acc: 0.6822\n",
            "[epoch 7] loss: 0.7315, acc: 0.6817\n",
            "[epoch 7] loss: 0.7318, acc: 0.6812\n",
            "[epoch 7] loss: 0.7337, acc: 0.6799\n",
            "[epoch 7] loss: 0.7329, acc: 0.6792\n",
            "[epoch 7] loss: 0.7347, acc: 0.6783\n",
            "[epoch 7] loss: 0.7352, acc: 0.6781\n",
            "> val_acc: 0.6460, val_f1: 0.6460\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 8\n",
            "[epoch 8] loss: 0.6978, acc: 0.6733\n",
            "[epoch 8] loss: 0.7088, acc: 0.6800\n",
            "[epoch 8] loss: 0.7026, acc: 0.6846\n",
            "[epoch 8] loss: 0.7135, acc: 0.6733\n",
            "[epoch 8] loss: 0.7157, acc: 0.6713\n",
            "[epoch 8] loss: 0.7080, acc: 0.6836\n",
            "[epoch 8] loss: 0.7158, acc: 0.6788\n",
            "[epoch 8] loss: 0.7085, acc: 0.6868\n",
            "[epoch 8] loss: 0.7177, acc: 0.6777\n",
            "[epoch 8] loss: 0.7122, acc: 0.6817\n",
            "[epoch 8] loss: 0.7119, acc: 0.6823\n",
            "[epoch 8] loss: 0.7131, acc: 0.6834\n",
            "[epoch 8] loss: 0.7092, acc: 0.6889\n",
            "[epoch 8] loss: 0.7137, acc: 0.6862\n",
            "[epoch 8] loss: 0.7134, acc: 0.6855\n",
            "[epoch 8] loss: 0.7159, acc: 0.6851\n",
            "[epoch 8] loss: 0.7137, acc: 0.6877\n",
            "[epoch 8] loss: 0.7125, acc: 0.6857\n",
            "[epoch 8] loss: 0.7151, acc: 0.6839\n",
            "[epoch 8] loss: 0.7177, acc: 0.6820\n",
            "[epoch 8] loss: 0.7214, acc: 0.6798\n",
            "[epoch 8] loss: 0.7224, acc: 0.6776\n",
            "[epoch 8] loss: 0.7209, acc: 0.6773\n",
            "[epoch 8] loss: 0.7266, acc: 0.6766\n",
            "[epoch 8] loss: 0.7279, acc: 0.6763\n",
            "[epoch 8] loss: 0.7250, acc: 0.6787\n",
            "[epoch 8] loss: 0.7285, acc: 0.6774\n",
            "[epoch 8] loss: 0.7266, acc: 0.6784\n",
            "[epoch 8] loss: 0.7273, acc: 0.6778\n",
            "[epoch 8] loss: 0.7272, acc: 0.6778\n",
            "[epoch 8] loss: 0.7287, acc: 0.6769\n",
            "[epoch 8] loss: 0.7284, acc: 0.6761\n",
            "[epoch 8] loss: 0.7298, acc: 0.6747\n",
            "[epoch 8] loss: 0.7276, acc: 0.6769\n",
            "[epoch 8] loss: 0.7271, acc: 0.6776\n",
            "[epoch 8] loss: 0.7275, acc: 0.6784\n",
            "[epoch 8] loss: 0.7280, acc: 0.6783\n",
            "[epoch 8] loss: 0.7287, acc: 0.6786\n",
            "[epoch 8] loss: 0.7290, acc: 0.6782\n",
            "[epoch 8] loss: 0.7294, acc: 0.6780\n",
            "[epoch 8] loss: 0.7285, acc: 0.6778\n",
            "[epoch 8] loss: 0.7298, acc: 0.6776\n",
            "[epoch 8] loss: 0.7295, acc: 0.6777\n",
            "[epoch 8] loss: 0.7295, acc: 0.6783\n",
            "[epoch 8] loss: 0.7297, acc: 0.6777\n",
            "> val_acc: 0.6497, val_f1: 0.6497\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6497\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 9\n",
            "[epoch 9] loss: 0.7463, acc: 0.6800\n",
            "[epoch 9] loss: 0.7473, acc: 0.6600\n",
            "[epoch 9] loss: 0.7633, acc: 0.6529\n",
            "[epoch 9] loss: 0.7499, acc: 0.6674\n",
            "[epoch 9] loss: 0.7408, acc: 0.6675\n",
            "[epoch 9] loss: 0.7311, acc: 0.6738\n",
            "[epoch 9] loss: 0.7287, acc: 0.6771\n",
            "[epoch 9] loss: 0.7368, acc: 0.6749\n",
            "[epoch 9] loss: 0.7341, acc: 0.6773\n",
            "[epoch 9] loss: 0.7297, acc: 0.6824\n",
            "[epoch 9] loss: 0.7310, acc: 0.6822\n",
            "[epoch 9] loss: 0.7256, acc: 0.6824\n",
            "[epoch 9] loss: 0.7269, acc: 0.6853\n",
            "[epoch 9] loss: 0.7285, acc: 0.6826\n",
            "[epoch 9] loss: 0.7248, acc: 0.6827\n",
            "[epoch 9] loss: 0.7243, acc: 0.6815\n",
            "[epoch 9] loss: 0.7217, acc: 0.6821\n",
            "[epoch 9] loss: 0.7180, acc: 0.6838\n",
            "[epoch 9] loss: 0.7138, acc: 0.6866\n",
            "[epoch 9] loss: 0.7169, acc: 0.6859\n",
            "[epoch 9] loss: 0.7107, acc: 0.6892\n",
            "[epoch 9] loss: 0.7143, acc: 0.6877\n",
            "[epoch 9] loss: 0.7137, acc: 0.6874\n",
            "[epoch 9] loss: 0.7153, acc: 0.6866\n",
            "[epoch 9] loss: 0.7151, acc: 0.6866\n",
            "[epoch 9] loss: 0.7171, acc: 0.6851\n",
            "[epoch 9] loss: 0.7169, acc: 0.6860\n",
            "[epoch 9] loss: 0.7184, acc: 0.6852\n",
            "[epoch 9] loss: 0.7182, acc: 0.6847\n",
            "[epoch 9] loss: 0.7168, acc: 0.6855\n",
            "[epoch 9] loss: 0.7159, acc: 0.6864\n",
            "[epoch 9] loss: 0.7151, acc: 0.6857\n",
            "[epoch 9] loss: 0.7145, acc: 0.6857\n",
            "[epoch 9] loss: 0.7136, acc: 0.6871\n",
            "[epoch 9] loss: 0.7125, acc: 0.6877\n",
            "[epoch 9] loss: 0.7108, acc: 0.6883\n",
            "[epoch 9] loss: 0.7112, acc: 0.6878\n",
            "[epoch 9] loss: 0.7130, acc: 0.6870\n",
            "[epoch 9] loss: 0.7157, acc: 0.6860\n",
            "[epoch 9] loss: 0.7149, acc: 0.6856\n",
            "[epoch 9] loss: 0.7146, acc: 0.6863\n",
            "[epoch 9] loss: 0.7150, acc: 0.6861\n",
            "[epoch 9] loss: 0.7148, acc: 0.6864\n",
            "[epoch 9] loss: 0.7151, acc: 0.6859\n",
            "[epoch 9] loss: 0.7151, acc: 0.6859\n",
            "> val_acc: 0.6512, val_f1: 0.6512\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6512\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 10\n",
            "[epoch 10] loss: 0.6863, acc: 0.7080\n",
            "[epoch 10] loss: 0.6906, acc: 0.6960\n",
            "[epoch 10] loss: 0.7123, acc: 0.6800\n",
            "[epoch 10] loss: 0.7078, acc: 0.6860\n",
            "[epoch 10] loss: 0.7001, acc: 0.6936\n",
            "[epoch 10] loss: 0.7011, acc: 0.6947\n",
            "[epoch 10] loss: 0.6983, acc: 0.7006\n",
            "[epoch 10] loss: 0.6999, acc: 0.6965\n",
            "[epoch 10] loss: 0.6972, acc: 0.6938\n",
            "[epoch 10] loss: 0.6956, acc: 0.6908\n",
            "[epoch 10] loss: 0.6993, acc: 0.6909\n",
            "[epoch 10] loss: 0.7076, acc: 0.6850\n",
            "[epoch 10] loss: 0.7063, acc: 0.6865\n",
            "[epoch 10] loss: 0.7115, acc: 0.6869\n",
            "[epoch 10] loss: 0.7137, acc: 0.6848\n",
            "[epoch 10] loss: 0.7137, acc: 0.6855\n",
            "[epoch 10] loss: 0.7202, acc: 0.6852\n",
            "[epoch 10] loss: 0.7203, acc: 0.6856\n",
            "[epoch 10] loss: 0.7176, acc: 0.6863\n",
            "[epoch 10] loss: 0.7172, acc: 0.6858\n",
            "[epoch 10] loss: 0.7151, acc: 0.6865\n",
            "[epoch 10] loss: 0.7109, acc: 0.6882\n",
            "[epoch 10] loss: 0.7136, acc: 0.6871\n",
            "[epoch 10] loss: 0.7109, acc: 0.6882\n",
            "[epoch 10] loss: 0.7113, acc: 0.6869\n",
            "[epoch 10] loss: 0.7102, acc: 0.6874\n",
            "[epoch 10] loss: 0.7117, acc: 0.6880\n",
            "[epoch 10] loss: 0.7111, acc: 0.6881\n",
            "[epoch 10] loss: 0.7102, acc: 0.6884\n",
            "[epoch 10] loss: 0.7097, acc: 0.6883\n",
            "[epoch 10] loss: 0.7095, acc: 0.6885\n",
            "[epoch 10] loss: 0.7114, acc: 0.6880\n",
            "[epoch 10] loss: 0.7116, acc: 0.6881\n",
            "[epoch 10] loss: 0.7093, acc: 0.6889\n",
            "[epoch 10] loss: 0.7101, acc: 0.6891\n",
            "[epoch 10] loss: 0.7107, acc: 0.6897\n",
            "[epoch 10] loss: 0.7120, acc: 0.6885\n",
            "[epoch 10] loss: 0.7130, acc: 0.6876\n",
            "[epoch 10] loss: 0.7144, acc: 0.6865\n",
            "[epoch 10] loss: 0.7133, acc: 0.6866\n",
            "[epoch 10] loss: 0.7131, acc: 0.6873\n",
            "[epoch 10] loss: 0.7117, acc: 0.6886\n",
            "[epoch 10] loss: 0.7104, acc: 0.6897\n",
            "[epoch 10] loss: 0.7105, acc: 0.6902\n",
            "> val_acc: 0.6579, val_f1: 0.6579\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6579\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 11\n",
            "[epoch 11] loss: 0.6133, acc: 0.7600\n",
            "[epoch 11] loss: 0.6464, acc: 0.7100\n",
            "[epoch 11] loss: 0.6672, acc: 0.7127\n",
            "[epoch 11] loss: 0.6660, acc: 0.7188\n",
            "[epoch 11] loss: 0.6571, acc: 0.7190\n",
            "[epoch 11] loss: 0.6682, acc: 0.7131\n",
            "[epoch 11] loss: 0.6659, acc: 0.7155\n",
            "[epoch 11] loss: 0.6570, acc: 0.7189\n",
            "[epoch 11] loss: 0.6666, acc: 0.7141\n",
            "[epoch 11] loss: 0.6716, acc: 0.7083\n",
            "[epoch 11] loss: 0.6814, acc: 0.7043\n",
            "[epoch 11] loss: 0.6861, acc: 0.7054\n",
            "[epoch 11] loss: 0.6882, acc: 0.7023\n",
            "[epoch 11] loss: 0.6875, acc: 0.7033\n",
            "[epoch 11] loss: 0.6874, acc: 0.7054\n",
            "[epoch 11] loss: 0.6863, acc: 0.7061\n",
            "[epoch 11] loss: 0.6839, acc: 0.7086\n",
            "[epoch 11] loss: 0.6858, acc: 0.7079\n",
            "[epoch 11] loss: 0.6825, acc: 0.7097\n",
            "[epoch 11] loss: 0.6866, acc: 0.7065\n",
            "[epoch 11] loss: 0.6917, acc: 0.7032\n",
            "[epoch 11] loss: 0.6921, acc: 0.7017\n",
            "[epoch 11] loss: 0.6953, acc: 0.7005\n",
            "[epoch 11] loss: 0.6965, acc: 0.7014\n",
            "[epoch 11] loss: 0.6954, acc: 0.7025\n",
            "[epoch 11] loss: 0.6967, acc: 0.7017\n",
            "[epoch 11] loss: 0.6980, acc: 0.7006\n",
            "[epoch 11] loss: 0.6964, acc: 0.7018\n",
            "[epoch 11] loss: 0.6990, acc: 0.7014\n",
            "[epoch 11] loss: 0.6986, acc: 0.7019\n",
            "[epoch 11] loss: 0.6955, acc: 0.7025\n",
            "[epoch 11] loss: 0.6968, acc: 0.7014\n",
            "[epoch 11] loss: 0.6961, acc: 0.7016\n",
            "[epoch 11] loss: 0.6955, acc: 0.7013\n",
            "[epoch 11] loss: 0.6962, acc: 0.7000\n",
            "[epoch 11] loss: 0.6965, acc: 0.6992\n",
            "[epoch 11] loss: 0.6972, acc: 0.6988\n",
            "[epoch 11] loss: 0.6977, acc: 0.6986\n",
            "[epoch 11] loss: 0.6983, acc: 0.6981\n",
            "[epoch 11] loss: 0.6985, acc: 0.6980\n",
            "[epoch 11] loss: 0.6984, acc: 0.6977\n",
            "[epoch 11] loss: 0.7004, acc: 0.6960\n",
            "[epoch 11] loss: 0.7008, acc: 0.6962\n",
            "[epoch 11] loss: 0.7005, acc: 0.6966\n",
            "[epoch 11] loss: 0.6995, acc: 0.6966\n",
            "> val_acc: 0.6564, val_f1: 0.6564\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 12\n",
            "[epoch 12] loss: 0.6992, acc: 0.7100\n",
            "[epoch 12] loss: 0.7181, acc: 0.6886\n",
            "[epoch 12] loss: 0.6914, acc: 0.7017\n",
            "[epoch 12] loss: 0.6888, acc: 0.7000\n",
            "[epoch 12] loss: 0.7054, acc: 0.6864\n",
            "[epoch 12] loss: 0.7017, acc: 0.6904\n",
            "[epoch 12] loss: 0.6957, acc: 0.6906\n",
            "[epoch 12] loss: 0.6940, acc: 0.6892\n",
            "[epoch 12] loss: 0.6828, acc: 0.6967\n",
            "[epoch 12] loss: 0.6799, acc: 0.7000\n",
            "[epoch 12] loss: 0.6828, acc: 0.6981\n",
            "[epoch 12] loss: 0.6834, acc: 0.6968\n",
            "[epoch 12] loss: 0.6871, acc: 0.6990\n",
            "[epoch 12] loss: 0.6866, acc: 0.6976\n",
            "[epoch 12] loss: 0.6812, acc: 0.7011\n",
            "[epoch 12] loss: 0.6825, acc: 0.7008\n",
            "[epoch 12] loss: 0.6822, acc: 0.6993\n",
            "[epoch 12] loss: 0.6847, acc: 0.6998\n",
            "[epoch 12] loss: 0.6841, acc: 0.7002\n",
            "[epoch 12] loss: 0.6813, acc: 0.7025\n",
            "[epoch 12] loss: 0.6820, acc: 0.7004\n",
            "[epoch 12] loss: 0.6868, acc: 0.6993\n",
            "[epoch 12] loss: 0.6869, acc: 0.6993\n",
            "[epoch 12] loss: 0.6878, acc: 0.6983\n",
            "[epoch 12] loss: 0.6871, acc: 0.6987\n",
            "[epoch 12] loss: 0.6856, acc: 0.6986\n",
            "[epoch 12] loss: 0.6843, acc: 0.7000\n",
            "[epoch 12] loss: 0.6868, acc: 0.6980\n",
            "[epoch 12] loss: 0.6898, acc: 0.6958\n",
            "[epoch 12] loss: 0.6879, acc: 0.6970\n",
            "[epoch 12] loss: 0.6857, acc: 0.6988\n",
            "[epoch 12] loss: 0.6863, acc: 0.6996\n",
            "[epoch 12] loss: 0.6872, acc: 0.6983\n",
            "[epoch 12] loss: 0.6882, acc: 0.6976\n",
            "[epoch 12] loss: 0.6891, acc: 0.6969\n",
            "[epoch 12] loss: 0.6912, acc: 0.6957\n",
            "[epoch 12] loss: 0.6925, acc: 0.6953\n",
            "[epoch 12] loss: 0.6923, acc: 0.6963\n",
            "[epoch 12] loss: 0.6925, acc: 0.6967\n",
            "[epoch 12] loss: 0.6926, acc: 0.6971\n",
            "[epoch 12] loss: 0.6915, acc: 0.6977\n",
            "[epoch 12] loss: 0.6909, acc: 0.6984\n",
            "[epoch 12] loss: 0.6892, acc: 0.6992\n",
            "[epoch 12] loss: 0.6895, acc: 0.6986\n",
            "[epoch 12] loss: 0.6891, acc: 0.6987\n",
            "> val_acc: 0.6647, val_f1: 0.6647\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6647\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 13\n",
            "[epoch 13] loss: 0.6946, acc: 0.7267\n",
            "[epoch 13] loss: 0.7027, acc: 0.7200\n",
            "[epoch 13] loss: 0.7073, acc: 0.7062\n",
            "[epoch 13] loss: 0.6757, acc: 0.7178\n",
            "[epoch 13] loss: 0.6988, acc: 0.7043\n",
            "[epoch 13] loss: 0.6984, acc: 0.6964\n",
            "[epoch 13] loss: 0.6972, acc: 0.6970\n",
            "[epoch 13] loss: 0.6989, acc: 0.6953\n",
            "[epoch 13] loss: 0.6919, acc: 0.7019\n",
            "[epoch 13] loss: 0.6879, acc: 0.7033\n",
            "[epoch 13] loss: 0.6878, acc: 0.7019\n",
            "[epoch 13] loss: 0.6927, acc: 0.6990\n",
            "[epoch 13] loss: 0.6922, acc: 0.6990\n",
            "[epoch 13] loss: 0.6920, acc: 0.6976\n",
            "[epoch 13] loss: 0.6907, acc: 0.7003\n",
            "[epoch 13] loss: 0.6933, acc: 0.6967\n",
            "[epoch 13] loss: 0.6927, acc: 0.6973\n",
            "[epoch 13] loss: 0.6898, acc: 0.6984\n",
            "[epoch 13] loss: 0.6903, acc: 0.6998\n",
            "[epoch 13] loss: 0.6878, acc: 0.7000\n",
            "[epoch 13] loss: 0.6848, acc: 0.7017\n",
            "[epoch 13] loss: 0.6856, acc: 0.7004\n",
            "[epoch 13] loss: 0.6862, acc: 0.7000\n",
            "[epoch 13] loss: 0.6861, acc: 0.7005\n",
            "[epoch 13] loss: 0.6869, acc: 0.7003\n",
            "[epoch 13] loss: 0.6884, acc: 0.6998\n",
            "[epoch 13] loss: 0.6891, acc: 0.7000\n",
            "[epoch 13] loss: 0.6876, acc: 0.7016\n",
            "[epoch 13] loss: 0.6869, acc: 0.7017\n",
            "[epoch 13] loss: 0.6870, acc: 0.7009\n",
            "[epoch 13] loss: 0.6863, acc: 0.7014\n",
            "[epoch 13] loss: 0.6871, acc: 0.7009\n",
            "[epoch 13] loss: 0.6867, acc: 0.7010\n",
            "[epoch 13] loss: 0.6867, acc: 0.7008\n",
            "[epoch 13] loss: 0.6877, acc: 0.7006\n",
            "[epoch 13] loss: 0.6878, acc: 0.7000\n",
            "[epoch 13] loss: 0.6870, acc: 0.7002\n",
            "[epoch 13] loss: 0.6858, acc: 0.7006\n",
            "[epoch 13] loss: 0.6849, acc: 0.7009\n",
            "[epoch 13] loss: 0.6828, acc: 0.7021\n",
            "[epoch 13] loss: 0.6826, acc: 0.7022\n",
            "[epoch 13] loss: 0.6811, acc: 0.7022\n",
            "[epoch 13] loss: 0.6798, acc: 0.7023\n",
            "[epoch 13] loss: 0.6799, acc: 0.7022\n",
            "[epoch 13] loss: 0.6815, acc: 0.7016\n",
            "> val_acc: 0.6549, val_f1: 0.6549\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 14\n",
            "[epoch 14] loss: 0.5841, acc: 0.7600\n",
            "[epoch 14] loss: 0.6114, acc: 0.7400\n",
            "[epoch 14] loss: 0.6749, acc: 0.7086\n",
            "[epoch 14] loss: 0.6769, acc: 0.7063\n",
            "[epoch 14] loss: 0.6751, acc: 0.7050\n",
            "[epoch 14] loss: 0.6699, acc: 0.7131\n",
            "[epoch 14] loss: 0.6751, acc: 0.7082\n",
            "[epoch 14] loss: 0.6687, acc: 0.7154\n",
            "[epoch 14] loss: 0.6705, acc: 0.7118\n",
            "[epoch 14] loss: 0.6759, acc: 0.7114\n",
            "[epoch 14] loss: 0.6741, acc: 0.7100\n",
            "[epoch 14] loss: 0.6733, acc: 0.7098\n",
            "[epoch 14] loss: 0.6792, acc: 0.7053\n",
            "[epoch 14] loss: 0.6770, acc: 0.7064\n",
            "[epoch 14] loss: 0.6768, acc: 0.7059\n",
            "[epoch 14] loss: 0.6768, acc: 0.7053\n",
            "[epoch 14] loss: 0.6691, acc: 0.7081\n",
            "[epoch 14] loss: 0.6699, acc: 0.7061\n",
            "[epoch 14] loss: 0.6713, acc: 0.7040\n",
            "[epoch 14] loss: 0.6750, acc: 0.7024\n",
            "[epoch 14] loss: 0.6775, acc: 0.7006\n",
            "[epoch 14] loss: 0.6747, acc: 0.7002\n",
            "[epoch 14] loss: 0.6743, acc: 0.7009\n",
            "[epoch 14] loss: 0.6755, acc: 0.7003\n",
            "[epoch 14] loss: 0.6724, acc: 0.7027\n",
            "[epoch 14] loss: 0.6721, acc: 0.7020\n",
            "[epoch 14] loss: 0.6724, acc: 0.7027\n",
            "[epoch 14] loss: 0.6723, acc: 0.7019\n",
            "[epoch 14] loss: 0.6726, acc: 0.7014\n",
            "[epoch 14] loss: 0.6730, acc: 0.7005\n",
            "[epoch 14] loss: 0.6726, acc: 0.7010\n",
            "[epoch 14] loss: 0.6760, acc: 0.7006\n",
            "[epoch 14] loss: 0.6750, acc: 0.7015\n",
            "[epoch 14] loss: 0.6750, acc: 0.7015\n",
            "[epoch 14] loss: 0.6774, acc: 0.7009\n",
            "[epoch 14] loss: 0.6754, acc: 0.7017\n",
            "[epoch 14] loss: 0.6754, acc: 0.7020\n",
            "[epoch 14] loss: 0.6753, acc: 0.7015\n",
            "[epoch 14] loss: 0.6754, acc: 0.7020\n",
            "[epoch 14] loss: 0.6746, acc: 0.7022\n",
            "[epoch 14] loss: 0.6742, acc: 0.7032\n",
            "[epoch 14] loss: 0.6752, acc: 0.7023\n",
            "[epoch 14] loss: 0.6741, acc: 0.7033\n",
            "[epoch 14] loss: 0.6757, acc: 0.7023\n",
            "[epoch 14] loss: 0.6762, acc: 0.7021\n",
            "> val_acc: 0.6437, val_f1: 0.6437\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 15\n",
            "[epoch 15] loss: 0.6479, acc: 0.7400\n",
            "[epoch 15] loss: 0.6588, acc: 0.7280\n",
            "[epoch 15] loss: 0.6653, acc: 0.7213\n",
            "[epoch 15] loss: 0.6756, acc: 0.7100\n",
            "[epoch 15] loss: 0.6713, acc: 0.7112\n",
            "[epoch 15] loss: 0.6672, acc: 0.7113\n",
            "[epoch 15] loss: 0.6653, acc: 0.7091\n",
            "[epoch 15] loss: 0.6685, acc: 0.7090\n",
            "[epoch 15] loss: 0.6751, acc: 0.7080\n",
            "[epoch 15] loss: 0.6763, acc: 0.7076\n",
            "[epoch 15] loss: 0.6738, acc: 0.7065\n",
            "[epoch 15] loss: 0.6698, acc: 0.7073\n",
            "[epoch 15] loss: 0.6679, acc: 0.7077\n",
            "[epoch 15] loss: 0.6721, acc: 0.7083\n",
            "[epoch 15] loss: 0.6729, acc: 0.7059\n",
            "[epoch 15] loss: 0.6742, acc: 0.7045\n",
            "[epoch 15] loss: 0.6698, acc: 0.7075\n",
            "[epoch 15] loss: 0.6692, acc: 0.7078\n",
            "[epoch 15] loss: 0.6710, acc: 0.7076\n",
            "[epoch 15] loss: 0.6742, acc: 0.7050\n",
            "[epoch 15] loss: 0.6727, acc: 0.7059\n",
            "[epoch 15] loss: 0.6710, acc: 0.7076\n",
            "[epoch 15] loss: 0.6733, acc: 0.7070\n",
            "[epoch 15] loss: 0.6701, acc: 0.7085\n",
            "[epoch 15] loss: 0.6696, acc: 0.7085\n",
            "[epoch 15] loss: 0.6712, acc: 0.7091\n",
            "[epoch 15] loss: 0.6729, acc: 0.7077\n",
            "[epoch 15] loss: 0.6725, acc: 0.7086\n",
            "[epoch 15] loss: 0.6721, acc: 0.7083\n",
            "[epoch 15] loss: 0.6724, acc: 0.7077\n",
            "[epoch 15] loss: 0.6720, acc: 0.7079\n",
            "[epoch 15] loss: 0.6737, acc: 0.7077\n",
            "[epoch 15] loss: 0.6722, acc: 0.7092\n",
            "[epoch 15] loss: 0.6736, acc: 0.7080\n",
            "[epoch 15] loss: 0.6738, acc: 0.7080\n",
            "[epoch 15] loss: 0.6749, acc: 0.7064\n",
            "[epoch 15] loss: 0.6752, acc: 0.7065\n",
            "[epoch 15] loss: 0.6750, acc: 0.7058\n",
            "[epoch 15] loss: 0.6761, acc: 0.7055\n",
            "[epoch 15] loss: 0.6757, acc: 0.7055\n",
            "[epoch 15] loss: 0.6747, acc: 0.7061\n",
            "[epoch 15] loss: 0.6727, acc: 0.7071\n",
            "[epoch 15] loss: 0.6709, acc: 0.7082\n",
            "[epoch 15] loss: 0.6713, acc: 0.7081\n",
            "> val_acc: 0.6737, val_f1: 0.6737\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6737\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 16\n",
            "[epoch 16] loss: 0.6542, acc: 0.6600\n",
            "[epoch 16] loss: 0.6198, acc: 0.6933\n",
            "[epoch 16] loss: 0.6162, acc: 0.7109\n",
            "[epoch 16] loss: 0.6043, acc: 0.7262\n",
            "[epoch 16] loss: 0.5929, acc: 0.7381\n",
            "[epoch 16] loss: 0.6014, acc: 0.7346\n",
            "[epoch 16] loss: 0.6016, acc: 0.7381\n",
            "[epoch 16] loss: 0.5999, acc: 0.7433\n",
            "[epoch 16] loss: 0.6081, acc: 0.7405\n",
            "[epoch 16] loss: 0.6094, acc: 0.7383\n",
            "[epoch 16] loss: 0.6155, acc: 0.7369\n",
            "[epoch 16] loss: 0.6241, acc: 0.7307\n",
            "[epoch 16] loss: 0.6370, acc: 0.7246\n",
            "[epoch 16] loss: 0.6411, acc: 0.7227\n",
            "[epoch 16] loss: 0.6404, acc: 0.7237\n",
            "[epoch 16] loss: 0.6437, acc: 0.7224\n",
            "[epoch 16] loss: 0.6439, acc: 0.7200\n",
            "[epoch 16] loss: 0.6499, acc: 0.7184\n",
            "[epoch 16] loss: 0.6481, acc: 0.7174\n",
            "[epoch 16] loss: 0.6515, acc: 0.7163\n",
            "[epoch 16] loss: 0.6494, acc: 0.7184\n",
            "[epoch 16] loss: 0.6514, acc: 0.7172\n",
            "[epoch 16] loss: 0.6514, acc: 0.7173\n",
            "[epoch 16] loss: 0.6525, acc: 0.7171\n",
            "[epoch 16] loss: 0.6530, acc: 0.7169\n",
            "[epoch 16] loss: 0.6567, acc: 0.7156\n",
            "[epoch 16] loss: 0.6565, acc: 0.7159\n",
            "[epoch 16] loss: 0.6568, acc: 0.7151\n",
            "[epoch 16] loss: 0.6576, acc: 0.7152\n",
            "[epoch 16] loss: 0.6594, acc: 0.7137\n",
            "[epoch 16] loss: 0.6594, acc: 0.7132\n",
            "[epoch 16] loss: 0.6581, acc: 0.7145\n",
            "[epoch 16] loss: 0.6593, acc: 0.7142\n",
            "[epoch 16] loss: 0.6580, acc: 0.7149\n",
            "[epoch 16] loss: 0.6589, acc: 0.7147\n",
            "[epoch 16] loss: 0.6572, acc: 0.7153\n",
            "[epoch 16] loss: 0.6563, acc: 0.7156\n",
            "[epoch 16] loss: 0.6553, acc: 0.7159\n",
            "[epoch 16] loss: 0.6564, acc: 0.7152\n",
            "[epoch 16] loss: 0.6579, acc: 0.7145\n",
            "[epoch 16] loss: 0.6581, acc: 0.7147\n",
            "[epoch 16] loss: 0.6589, acc: 0.7144\n",
            "[epoch 16] loss: 0.6588, acc: 0.7140\n",
            "[epoch 16] loss: 0.6582, acc: 0.7147\n",
            "[epoch 16] loss: 0.6599, acc: 0.7135\n",
            "> val_acc: 0.6647, val_f1: 0.6647\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 17\n",
            "[epoch 17] loss: 0.6409, acc: 0.7100\n",
            "[epoch 17] loss: 0.6880, acc: 0.7086\n",
            "[epoch 17] loss: 0.6400, acc: 0.7300\n",
            "[epoch 17] loss: 0.6395, acc: 0.7224\n",
            "[epoch 17] loss: 0.6492, acc: 0.7209\n",
            "[epoch 17] loss: 0.6572, acc: 0.7104\n",
            "[epoch 17] loss: 0.6496, acc: 0.7106\n",
            "[epoch 17] loss: 0.6476, acc: 0.7103\n",
            "[epoch 17] loss: 0.6433, acc: 0.7167\n",
            "[epoch 17] loss: 0.6429, acc: 0.7162\n",
            "[epoch 17] loss: 0.6473, acc: 0.7119\n",
            "[epoch 17] loss: 0.6483, acc: 0.7179\n",
            "[epoch 17] loss: 0.6438, acc: 0.7187\n",
            "[epoch 17] loss: 0.6420, acc: 0.7197\n",
            "[epoch 17] loss: 0.6477, acc: 0.7211\n",
            "[epoch 17] loss: 0.6496, acc: 0.7210\n",
            "[epoch 17] loss: 0.6544, acc: 0.7200\n",
            "[epoch 17] loss: 0.6534, acc: 0.7214\n",
            "[epoch 17] loss: 0.6563, acc: 0.7202\n",
            "[epoch 17] loss: 0.6559, acc: 0.7200\n",
            "[epoch 17] loss: 0.6544, acc: 0.7202\n",
            "[epoch 17] loss: 0.6504, acc: 0.7222\n",
            "[epoch 17] loss: 0.6447, acc: 0.7243\n",
            "[epoch 17] loss: 0.6457, acc: 0.7236\n",
            "[epoch 17] loss: 0.6447, acc: 0.7234\n",
            "[epoch 17] loss: 0.6462, acc: 0.7222\n",
            "[epoch 17] loss: 0.6445, acc: 0.7220\n",
            "[epoch 17] loss: 0.6428, acc: 0.7228\n",
            "[epoch 17] loss: 0.6418, acc: 0.7221\n",
            "[epoch 17] loss: 0.6442, acc: 0.7208\n",
            "[epoch 17] loss: 0.6478, acc: 0.7183\n",
            "[epoch 17] loss: 0.6488, acc: 0.7177\n",
            "[epoch 17] loss: 0.6502, acc: 0.7158\n",
            "[epoch 17] loss: 0.6494, acc: 0.7158\n",
            "[epoch 17] loss: 0.6489, acc: 0.7166\n",
            "[epoch 17] loss: 0.6491, acc: 0.7167\n",
            "[epoch 17] loss: 0.6514, acc: 0.7163\n",
            "[epoch 17] loss: 0.6498, acc: 0.7172\n",
            "[epoch 17] loss: 0.6507, acc: 0.7168\n",
            "[epoch 17] loss: 0.6505, acc: 0.7168\n",
            "[epoch 17] loss: 0.6520, acc: 0.7160\n",
            "[epoch 17] loss: 0.6527, acc: 0.7153\n",
            "[epoch 17] loss: 0.6522, acc: 0.7154\n",
            "[epoch 17] loss: 0.6537, acc: 0.7145\n",
            "[epoch 17] loss: 0.6552, acc: 0.7136\n",
            "> val_acc: 0.6564, val_f1: 0.6564\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 18\n",
            "[epoch 18] loss: 0.7102, acc: 0.7200\n",
            "[epoch 18] loss: 0.6456, acc: 0.7175\n",
            "[epoch 18] loss: 0.6466, acc: 0.7154\n",
            "[epoch 18] loss: 0.6398, acc: 0.7156\n",
            "[epoch 18] loss: 0.6301, acc: 0.7270\n",
            "[epoch 18] loss: 0.6405, acc: 0.7293\n",
            "[epoch 18] loss: 0.6426, acc: 0.7291\n",
            "[epoch 18] loss: 0.6537, acc: 0.7242\n",
            "[epoch 18] loss: 0.6540, acc: 0.7214\n",
            "[epoch 18] loss: 0.6632, acc: 0.7163\n",
            "[epoch 18] loss: 0.6576, acc: 0.7181\n",
            "[epoch 18] loss: 0.6548, acc: 0.7210\n",
            "[epoch 18] loss: 0.6568, acc: 0.7178\n",
            "[epoch 18] loss: 0.6504, acc: 0.7206\n",
            "[epoch 18] loss: 0.6503, acc: 0.7192\n",
            "[epoch 18] loss: 0.6481, acc: 0.7192\n",
            "[epoch 18] loss: 0.6523, acc: 0.7171\n",
            "[epoch 18] loss: 0.6501, acc: 0.7180\n",
            "[epoch 18] loss: 0.6516, acc: 0.7161\n",
            "[epoch 18] loss: 0.6507, acc: 0.7165\n",
            "[epoch 18] loss: 0.6470, acc: 0.7188\n",
            "[epoch 18] loss: 0.6490, acc: 0.7181\n",
            "[epoch 18] loss: 0.6494, acc: 0.7177\n",
            "[epoch 18] loss: 0.6471, acc: 0.7190\n",
            "[epoch 18] loss: 0.6446, acc: 0.7197\n",
            "[epoch 18] loss: 0.6449, acc: 0.7194\n",
            "[epoch 18] loss: 0.6466, acc: 0.7183\n",
            "[epoch 18] loss: 0.6456, acc: 0.7187\n",
            "[epoch 18] loss: 0.6457, acc: 0.7183\n",
            "[epoch 18] loss: 0.6452, acc: 0.7195\n",
            "[epoch 18] loss: 0.6431, acc: 0.7203\n",
            "[epoch 18] loss: 0.6425, acc: 0.7211\n",
            "[epoch 18] loss: 0.6421, acc: 0.7221\n",
            "[epoch 18] loss: 0.6428, acc: 0.7214\n",
            "[epoch 18] loss: 0.6440, acc: 0.7202\n",
            "[epoch 18] loss: 0.6442, acc: 0.7210\n",
            "[epoch 18] loss: 0.6430, acc: 0.7219\n",
            "[epoch 18] loss: 0.6441, acc: 0.7203\n",
            "[epoch 18] loss: 0.6436, acc: 0.7204\n",
            "[epoch 18] loss: 0.6451, acc: 0.7196\n",
            "[epoch 18] loss: 0.6459, acc: 0.7189\n",
            "[epoch 18] loss: 0.6469, acc: 0.7188\n",
            "[epoch 18] loss: 0.6470, acc: 0.7189\n",
            "[epoch 18] loss: 0.6470, acc: 0.7194\n",
            "[epoch 18] loss: 0.6460, acc: 0.7194\n",
            "> val_acc: 0.6602, val_f1: 0.6602\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 19\n",
            "[epoch 19] loss: 0.6447, acc: 0.6850\n",
            "[epoch 19] loss: 0.6324, acc: 0.6933\n",
            "[epoch 19] loss: 0.6199, acc: 0.7057\n",
            "[epoch 19] loss: 0.6211, acc: 0.7221\n",
            "[epoch 19] loss: 0.6372, acc: 0.7167\n",
            "[epoch 19] loss: 0.6323, acc: 0.7200\n",
            "[epoch 19] loss: 0.6401, acc: 0.7171\n",
            "[epoch 19] loss: 0.6322, acc: 0.7174\n",
            "[epoch 19] loss: 0.6285, acc: 0.7186\n",
            "[epoch 19] loss: 0.6358, acc: 0.7143\n",
            "[epoch 19] loss: 0.6370, acc: 0.7133\n",
            "[epoch 19] loss: 0.6404, acc: 0.7142\n",
            "[epoch 19] loss: 0.6363, acc: 0.7147\n",
            "[epoch 19] loss: 0.6371, acc: 0.7151\n",
            "[epoch 19] loss: 0.6377, acc: 0.7157\n",
            "[epoch 19] loss: 0.6356, acc: 0.7165\n",
            "[epoch 19] loss: 0.6376, acc: 0.7155\n",
            "[epoch 19] loss: 0.6388, acc: 0.7160\n",
            "[epoch 19] loss: 0.6346, acc: 0.7183\n",
            "[epoch 19] loss: 0.6328, acc: 0.7188\n",
            "[epoch 19] loss: 0.6338, acc: 0.7190\n",
            "[epoch 19] loss: 0.6317, acc: 0.7213\n",
            "[epoch 19] loss: 0.6342, acc: 0.7200\n",
            "[epoch 19] loss: 0.6352, acc: 0.7207\n",
            "[epoch 19] loss: 0.6366, acc: 0.7198\n",
            "[epoch 19] loss: 0.6365, acc: 0.7206\n",
            "[epoch 19] loss: 0.6379, acc: 0.7196\n",
            "[epoch 19] loss: 0.6353, acc: 0.7224\n",
            "[epoch 19] loss: 0.6354, acc: 0.7231\n",
            "[epoch 19] loss: 0.6373, acc: 0.7221\n",
            "[epoch 19] loss: 0.6370, acc: 0.7218\n",
            "[epoch 19] loss: 0.6363, acc: 0.7224\n",
            "[epoch 19] loss: 0.6374, acc: 0.7216\n",
            "[epoch 19] loss: 0.6391, acc: 0.7211\n",
            "[epoch 19] loss: 0.6396, acc: 0.7207\n",
            "[epoch 19] loss: 0.6387, acc: 0.7217\n",
            "[epoch 19] loss: 0.6388, acc: 0.7218\n",
            "[epoch 19] loss: 0.6377, acc: 0.7229\n",
            "[epoch 19] loss: 0.6383, acc: 0.7233\n",
            "[epoch 19] loss: 0.6374, acc: 0.7239\n",
            "[epoch 19] loss: 0.6379, acc: 0.7239\n",
            "[epoch 19] loss: 0.6379, acc: 0.7242\n",
            "[epoch 19] loss: 0.6383, acc: 0.7248\n",
            "[epoch 19] loss: 0.6371, acc: 0.7247\n",
            "[epoch 19] loss: 0.6387, acc: 0.7240\n",
            "> val_acc: 0.6617, val_f1: 0.6617\n",
            ">> test_acc: 0.6737\n",
            ">> test_f1_macro: 0.6571\n",
            ">> test_f1_micro: 0.6737\n",
            ">> test_f1_weighted: 0.6734\n",
            ">> test_precision_macro: 0.6596\n",
            ">> test_precision_micro: 0.6737\n",
            ">> test_precision_weighted: 0.6737\n",
            ">> test_recall_macro: 0.6552\n",
            ">> test_recall_micro: 0.6737\n",
            ">> test_recall_weighted: 0.6737\n",
            "confusion matrix:\n",
            "[[198  61  70]\n",
            " [ 46 459 102]\n",
            " [ 62  95 243]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}