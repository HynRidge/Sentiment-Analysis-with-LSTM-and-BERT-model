{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SETUP**"
      ],
      "metadata": {
        "id": "5zXJrSMo495u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "8_7Yx7y-5BOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tYF42jlo5Q4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7feea83-6d94-4d48-a9c5-10eccd26f46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "eTERBhPf5Ykk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import strftime, localtime\n",
        "from sklearn import metrics\n",
        "from xml.etree.ElementTree import parse\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "ZEPQFq-e6gVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "6674vF4n6le1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASSES**"
      ],
      "metadata": {
        "id": "IDenOuxLpJFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**"
      ],
      "metadata": {
        "id": "USX4iChw67d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, max_len, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(max_len) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-max_len:]\n",
        "    else:\n",
        "        trunc = sequence[:max_len]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "L5p9tWvxkY2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self, max_len, lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_len = max_len\n",
        "        self.word_to_index = {}\n",
        "        self.index_to_word = {}\n",
        "        self.idx = 1\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word_to_index:\n",
        "                self.word_to_index[word] = self.idx\n",
        "                self.index_to_word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unk = len(self.word_to_index)+1\n",
        "        sequence = [self.word_to_index[w] if w in self.word_to_index else unk for w in words]\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_len, padding=padding, truncating=truncating)"
      ],
      "metadata": {
        "id": "4y8GnOALjcjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, file_name, tokenizer):\n",
        "        with open(file_name, 'r', newline='\\n', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "        with open(file_name + '.graph', 'rb') as f:\n",
        "            index_to_graph = pickle.load(f)\n",
        "        \n",
        "        self.data = []\n",
        "\n",
        "        # Create tokens with tokenizer\n",
        "        for i in range(0, len(lines), 3):\n",
        "            left, right = [s.lower().strip() for s in lines[i].split(\"$T$\")]\n",
        "            aspect = lines[i+1].lower().strip()\n",
        "            polarity = lines[i+2].strip()\n",
        "\n",
        "            text_token = tokenizer.text_to_sequence(\" \".join([left, aspect, right]))\n",
        "            context_token = tokenizer.text_to_sequence(\" \".join([left, right]))\n",
        "\n",
        "            left_token = tokenizer.text_to_sequence(left)\n",
        "            left_aspect_token = tokenizer.text_to_sequence(\" \".join([left, aspect]))\n",
        "\n",
        "            right_token = tokenizer.text_to_sequence(right, reverse=True)\n",
        "            right_aspect_token = tokenizer.text_to_sequence(\" \".join([aspect, right]), reverse=True)\n",
        "\n",
        "            aspect_token = tokenizer.text_to_sequence(aspect)\n",
        "\n",
        "            left_len = np.sum(left_token != 0)\n",
        "            aspect_len = np.sum(aspect_token != 0)\n",
        "            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype = np.int64)\n",
        "            polarity = int(polarity) + 1\n",
        "            \n",
        "            text_len = np.sum(text_token != 0)\n",
        "            concat_segments_tokens = pad_and_truncate([0] * (text_len + 1) + [1] * (aspect_len + 1), tokenizer.max_len)\n",
        "            \n",
        "            dependency_graph = np.pad(index_to_graph[i],\n",
        "                                      ((0, tokenizer.max_len-index_to_graph[i].shape[0]),\n",
        "                                       (0, tokenizer.max_len-index_to_graph[i].shape[0])),\n",
        "                                      'constant')\n",
        "            \n",
        "            self.data.append({\n",
        "                'text_token' : text_token,\n",
        "                'context_token' : context_token, \n",
        "                'left_token' : left_token, \n",
        "                'left_aspect_token' : left_aspect_token,\n",
        "                'right_token' : right_token,\n",
        "                'right_aspect_token' : right_aspect_token,\n",
        "                'aspect_token' : aspect_token, \n",
        "                'aspect_boundary' : aspect_boundary, \n",
        "                'polarity' : polarity, \n",
        "                'dependency_graph' : dependency_graph\n",
        "            })\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "Q5FagHQ9cZGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split()\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "gjHh_czJ0Rbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "fuiAR0ng6vID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.squeeze_embedding = SqueezeEmbedding()\n",
        "        self.lstm = nn.LSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=1, batch_first=True)\n",
        "        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        text_indices, aspect_indices = inputs[0], inputs[1]\n",
        "        x_len = torch.sum(text_indices != 0, dim=-1)\n",
        "        x_len_max = torch.max(x_len)\n",
        "        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n",
        "\n",
        "        x = self.embed(text_indices)\n",
        "        x = self.squeeze_embedding(x, x_len)\n",
        "        aspect = self.embed(aspect_indices)\n",
        "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n",
        "        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n",
        "        x = torch.cat((aspect, x), dim=-1)\n",
        "\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # global ht\n",
        "        out_pack, (ht, ct) = self.lstm(x_emb_p, None)\n",
        "\n",
        "        return self.dense(ht[0])"
      ],
      "metadata": {
        "id": "xPQIKGdi0qXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze sequence embedding length to the longest one in the batch\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_first=True):\n",
        "        super(SqueezeEmbedding, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n",
        "        \"\"\"unpack: out\"\"\"\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "        out = out[0]  #\n",
        "        \"\"\"unsort\"\"\"\n",
        "        out = out[x_unsort_idx]\n",
        "        return out"
      ],
      "metadata": {
        "id": "tFr-dJ6B1E55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main**"
      ],
      "metadata": {
        "id": "6BmgF1FSp32r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Instructor:\n",
        "    def __init__(self):\n",
        "        tokenizer = build_tokenizer(\n",
        "            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n",
        "            max_seq_len=config2[\"max_seq_len\"],\n",
        "            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n",
        "        embedding_matrix = build_embedding_matrix(\n",
        "            word2idx=tokenizer.word_to_index,\n",
        "            embed_dim=config2[\"embed_dim\"],\n",
        "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n",
        "        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n",
        "\n",
        "        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n",
        "        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n",
        "        assert 0 <= config2[\"valset_ratio\"] < 1\n",
        "        if config2[\"valset_ratio\"] > 0:\n",
        "            valset_len = int(len(self.trainset) * config2[\"valset_ratio\"])\n",
        "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
        "        else:\n",
        "            self.valset = self.testset\n",
        "\n",
        "        if config2[\"device\"].type == 'cuda':\n",
        "            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n",
        "        self._print_args()\n",
        "\n",
        "    def _print_args(self):\n",
        "        n_trainable_params, n_nontrainable_params = 0, 0\n",
        "        for p in self.model.parameters():\n",
        "            n_params = torch.prod(torch.tensor(p.shape))\n",
        "            if p.requires_grad:\n",
        "                n_trainable_params += n_params\n",
        "            else:\n",
        "                n_nontrainable_params += n_params\n",
        "        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
        "\n",
        "\n",
        "    def _reset_params(self):\n",
        "        for child in self.model.children():\n",
        "            for p in child.parameters():\n",
        "                if p.requires_grad:\n",
        "                    if len(p.shape) > 1:\n",
        "                        config2[\"initializer\"](p)\n",
        "                    else:\n",
        "                        stdv = 1. / math.sqrt(p.shape[0])\n",
        "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "\n",
        "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
        "        max_val_acc = 0\n",
        "        max_val_f1 = 0\n",
        "        max_val_epoch = 0\n",
        "        global_step = 0\n",
        "        path = None\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        for i_epoch in range(config2[\"num_epoch\"]):\n",
        "            print('>' * 100)\n",
        "            print('epoch: {}'.format(i_epoch))\n",
        "            n_correct, n_total, loss_total = 0, 0, 0\n",
        "            # switch model to training mode\n",
        "            self.model.train()\n",
        "            for i_batch, batch in enumerate(train_data_loader):\n",
        "                global_step += 1\n",
        "                # clear gradient accumulators\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                global outputs, targets\n",
        "                outputs = self.model(inputs)\n",
        "                targets = batch['polarity'].to(config2[\"device\"])\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
        "                n_total += len(outputs)\n",
        "                loss_total += loss.item() * len(outputs)\n",
        "                if global_step % config2[\"log_step\"] == 0:\n",
        "                    train_acc = n_correct / n_total\n",
        "                    train_loss = loss_total / n_total\n",
        "                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n",
        "\n",
        "            val_acc, val_loss, _, val_f1, _, _, _, _, _, _, _, _ = self._evaluate(criterion, val_data_loader)\n",
        "            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                max_val_epoch = i_epoch\n",
        "                if not os.path.exists('state_dict'):\n",
        "                    os.mkdir('state_dict')\n",
        "                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n",
        "                torch.save(self.model.state_dict(), path)\n",
        "                print('>> saved: {}'.format(path))\n",
        "            if val_f1 > max_val_f1:\n",
        "                max_val_f1 = val_f1\n",
        "            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n",
        "                print('>> early stop.')\n",
        "                break\n",
        "            train_losses.append(loss_total / n_total)\n",
        "            train_accuracies.append(n_correct / n_total)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        return path, train_losses, train_accuracies, val_losses, val_accuracies\n",
        "\n",
        "    def _evaluate(self, criterion, data_loader):\n",
        "        n_correct, n_total, loss_total = 0, 0, 0\n",
        "        t_targets_all, t_outputs_all = None, None\n",
        "        # switch model to evaluation mode\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i_batch, t_batch in enumerate(data_loader):\n",
        "                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                t_targets = t_batch['polarity'].to(config2[\"device\"])\n",
        "                t_outputs = self.model(t_inputs)\n",
        "\n",
        "                loss = criterion(t_outputs, t_targets)\n",
        "\n",
        "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
        "                n_total += len(t_outputs)\n",
        "                loss_total += loss.item() * len(t_outputs)\n",
        "\n",
        "                if t_targets_all is None:\n",
        "                    t_targets_all = t_targets\n",
        "                    t_outputs_all = t_outputs\n",
        "                else:\n",
        "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
        "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
        "\n",
        "        acc = n_correct / n_total\n",
        "        loss = loss_total / n_total\n",
        "        f1_macro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
        "        f1_micro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='micro')\n",
        "        f1_weighted = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='weighted')\n",
        "        precision_macro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        precision_micro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        precision_weighted = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        recall_macro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        recall_micro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        recall_weighted = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        confusion_matrix = metrics.confusion_matrix(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2])\n",
        "        return acc, loss, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix\n",
        "\n",
        "    def run(self):\n",
        "        # Loss and Optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n",
        "\n",
        "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n",
        "        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "\n",
        "        self._reset_params()\n",
        "        best_model_path, train_losses, train_accuracies, val_losses, val_accuracies = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "        acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = self._evaluate(nn.CrossEntropyLoss(), test_data_loader)\n",
        "        print('>> test_acc: {:.4f}'.format(acc))\n",
        "        print('>> test_f1_macro: {:.4f}'.format(f1_macro))\n",
        "        print('>> test_f1_micro: {:.4f}'.format(f1_micro))\n",
        "        print('>> test_f1_weighted: {:.4f}'.format(f1_weighted))\n",
        "        print('>> test_precision_macro: {:.4f}'.format(precision_macro))\n",
        "        print('>> test_precision_micro: {:.4f}'.format(precision_micro))\n",
        "        print('>> test_precision_weighted: {:.4f}'.format(precision_weighted))\n",
        "        print('>> test_recall_macro: {:.4f}'.format(recall_macro))\n",
        "        print('>> test_recall_micro: {:.4f}'.format(recall_micro))\n",
        "        print('>> test_recall_weighted: {:.4f}'.format(recall_weighted))\n",
        "        print('confusion matrix:')\n",
        "        print(confusion_matrix)\n",
        "\n",
        "        epochs = len(train_losses)\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        for i, metrics in enumerate(zip([train_losses, train_accuracies], [val_losses, val_accuracies], ['Loss', 'Accuracy'])):\n",
        "            plt.subplot(1, 2, i + 1)\n",
        "            plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "            plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "            plt.legend()\n",
        "        plt.show()\n",
        "        plt.savefig('lstm_accuracy.png')\n",
        "        return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "mPRxQS-Jp5vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix([1, 2, 1], [1, 2, 0], labels=[0, 1, 2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9gGC2RFg6yV",
        "outputId": "e61adc18-576b-4404-f0c9-51c78b4657b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0]\n",
            " [1 1 0]\n",
            " [0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "bv09Mo677CkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_format_sentences(source_path, target_path):\n",
        "    f = open(target_path, \"w\")\n",
        "\n",
        "    sentences = parse(source_path).getroot()\n",
        "    preprocessed = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            if polarity == \"positive\":\n",
        "                polarity = '1'\n",
        "            elif polarity == \"neutral\":\n",
        "                polarity = '0'\n",
        "            elif polarity == \"negative\":\n",
        "                polarity = '-1'\n",
        "            else:\n",
        "                raise Exception(\"invalid polarity!\")\n",
        "            start = int(aspectTerm.get('from'))\n",
        "            end = int(aspectTerm.get('to'))\n",
        "            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n",
        "            preprocessed.append(text[start:end])\n",
        "            preprocessed.append(polarity)\n",
        "    f.write(\"\\n\".join(preprocessed))"
      ],
      "metadata": {
        "id": "Mlhjev_wrkDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading tokenizer:', dat_fname)\n",
        "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        text = ''\n",
        "        for fname in fnames:\n",
        "            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "            lines = fin.readlines()\n",
        "            fin.close()\n",
        "            for i in range(0, len(lines), 3):\n",
        "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "                aspect = lines[i + 1].lower().strip()\n",
        "                text_raw = text_left + \" \" + aspect + \" \" + text_right\n",
        "                text += text_raw + \" \"\n",
        "\n",
        "        tokenizer = Tokenizer(max_seq_len)\n",
        "        tokenizer.fit_on_text(text)\n",
        "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "pV174mP37UZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_word_vec(path, word2idx=None, embed_dim=300):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n",
        "        if word in word2idx.keys():\n",
        "            word_vec[word] = np.asarray(vec, dtype='float32')\n",
        "    return word_vec"
      ],
      "metadata": {
        "id": "2h-2UjGs7fu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix:', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = config[\"glove_path\"]\n",
        "        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "nB3jz5Fk7dZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "7JH84-Kx7Y-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dependency_adj_matrix(text):\n",
        "    # https://spacy.io/docs/usage/processing-text\n",
        "    tokens = nlp(text)\n",
        "    words = text.split()\n",
        "    matrix = np.zeros((len(words), len(words))).astype('float32')\n",
        "    assert len(words) == len(list(tokens))\n",
        "\n",
        "    for token in tokens:\n",
        "        matrix[token.i][token.i] = 1\n",
        "        for child in token.children:\n",
        "            matrix[token.i][child.i] = 1\n",
        "            matrix[child.i][token.i] = 1\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "VemwD4FB7sY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n",
        "    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n",
        "    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n",
        "    process(config[\"processed_train_path\"])\n",
        "    process(config[\"processed_val_path\"])\n",
        "    process(config[\"processed_test_path\"])"
      ],
      "metadata": {
        "id": "vC45U4k21oPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(filename):\n",
        "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    lines = fin.readlines()\n",
        "    fin.close()\n",
        "    idx2graph = {}\n",
        "    fout = open(filename+'.graph', 'wb')\n",
        "    for i in range(0, len(lines), 3):\n",
        "        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n",
        "        aspect = lines[i + 1].strip()\n",
        "        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n",
        "        idx2graph[i] = adj_matrix\n",
        "    pickle.dump(idx2graph, fout)        \n",
        "    fout.close() "
      ],
      "metadata": {
        "id": "HlLsFiUJ7mPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    ins = Instructor()\n",
        "    best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = ins.run()\n",
        "    return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "ANtAieVU8lwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **MAIN**"
      ],
      "metadata": {
        "id": "cxb8D05u8UN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "5QyUuFxj8XuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "AF3DYep7AW8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"base_path\": \"/content/drive/MyDrive/CS4248/MAMS-ATSA\",\n",
        "    \"glove_path\": \"/content/drive/MyDrive/CS4248/glove.42B.300d.txt\"\n",
        "}\n",
        "\n",
        "config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n",
        "config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n",
        "config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n",
        "config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n",
        "config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n",
        "config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n",
        "config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"
      ],
      "metadata": {
        "id": "9y9MO-RW8bcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_files = {\n",
        "    'train': config[\"processed_train_path\"],\n",
        "    'test': config[\"processed_test_path\"]\n",
        "}\n",
        "\n",
        "config2 = {\n",
        "    \"model_name\" : \"LSTM\",\n",
        "    \"lr\" : 3e-5,\n",
        "    \"dropout\" : 0.1,\n",
        "    \"l2reg\" : 0.001,\n",
        "    \"num_epoch\" : 20,\n",
        "    \"batch_size\" : 25,\n",
        "    \"log_step\" : 10,\n",
        "    \"embed_dim\" : 300,\n",
        "    \"hidden_dim\" : 300,\n",
        "    \"model_class\" : LSTM,\n",
        "    \"dataset\": \"MAMS\",\n",
        "    \"dataset_file\" : dataset_files,\n",
        "    \"inputs_cols\" : ['text_token', 'aspect_token'],\n",
        "    \"initializer\" : torch.nn.init.xavier_uniform_,\n",
        "    \"optimizer\" : torch.optim.Adam,\n",
        "    \"max_seq_len\" : 85,\n",
        "    \"polarities_dim\" : 3,\n",
        "    \"patience\" : 5, # for early stopping\n",
        "    \"device\" : None,\n",
        "    \"seed\" : 1234,\n",
        "    \"valset_ratio\" : 0\n",
        "}\n",
        "\n",
        "config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
        "        if config2[\"device\"] is None else torch.device(config2[\"device\"])\n",
        "\n",
        "if config2[\"seed\"] is not None:\n",
        "    random.seed(config2[\"seed\"])\n",
        "    np.random.seed(config2[\"seed\"])\n",
        "    torch.manual_seed(config2[\"seed\"])\n",
        "    torch.cuda.manual_seed(config2[\"seed\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"
      ],
      "metadata": {
        "id": "clOIqT523tWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**"
      ],
      "metadata": {
        "id": "3fT6RqeiAGmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this line only for the first time you run this file\n",
        "# process_data()"
      ],
      "metadata": {
        "id": "vT4xfKSC-Xpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = run()"
      ],
      "metadata": {
        "id": "8gEIkSn38pHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb5be21c-ff3d-478a-8a3e-2a25ef5c73e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading tokenizer: MAMS_tokenizer.dat\n",
            "loading embedding_matrix: 300_MAMS_embedding_matrix.dat\n",
            "> n_trainable_params: 1083303, n_nontrainable_params: 3994500\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 0\n",
            "[epoch 0] loss: 1.1110, acc: 0.3920\n",
            "[epoch 0] loss: 1.0863, acc: 0.4160\n",
            "[epoch 0] loss: 1.0679, acc: 0.4360\n",
            "[epoch 0] loss: 1.0548, acc: 0.4380\n",
            "[epoch 0] loss: 1.0487, acc: 0.4400\n",
            "[epoch 0] loss: 1.0386, acc: 0.4533\n",
            "[epoch 0] loss: 1.0303, acc: 0.4651\n",
            "[epoch 0] loss: 1.0256, acc: 0.4740\n",
            "[epoch 0] loss: 1.0199, acc: 0.4844\n",
            "[epoch 0] loss: 1.0141, acc: 0.4896\n",
            "[epoch 0] loss: 1.0087, acc: 0.4956\n",
            "[epoch 0] loss: 1.0027, acc: 0.5047\n",
            "[epoch 0] loss: 1.0001, acc: 0.5071\n",
            "[epoch 0] loss: 0.9940, acc: 0.5131\n",
            "[epoch 0] loss: 0.9888, acc: 0.5171\n",
            "[epoch 0] loss: 0.9858, acc: 0.5202\n",
            "[epoch 0] loss: 0.9850, acc: 0.5219\n",
            "[epoch 0] loss: 0.9822, acc: 0.5231\n",
            "[epoch 0] loss: 0.9778, acc: 0.5274\n",
            "[epoch 0] loss: 0.9735, acc: 0.5316\n",
            "[epoch 0] loss: 0.9708, acc: 0.5326\n",
            "[epoch 0] loss: 0.9679, acc: 0.5345\n",
            "[epoch 0] loss: 0.9680, acc: 0.5343\n",
            "[epoch 0] loss: 0.9649, acc: 0.5357\n",
            "[epoch 0] loss: 0.9624, acc: 0.5371\n",
            "[epoch 0] loss: 0.9593, acc: 0.5395\n",
            "[epoch 0] loss: 0.9568, acc: 0.5413\n",
            "[epoch 0] loss: 0.9532, acc: 0.5434\n",
            "[epoch 0] loss: 0.9499, acc: 0.5461\n",
            "[epoch 0] loss: 0.9491, acc: 0.5464\n",
            "[epoch 0] loss: 0.9464, acc: 0.5488\n",
            "[epoch 0] loss: 0.9455, acc: 0.5481\n",
            "[epoch 0] loss: 0.9456, acc: 0.5479\n",
            "[epoch 0] loss: 0.9430, acc: 0.5492\n",
            "[epoch 0] loss: 0.9420, acc: 0.5505\n",
            "[epoch 0] loss: 0.9407, acc: 0.5513\n",
            "[epoch 0] loss: 0.9395, acc: 0.5514\n",
            "[epoch 0] loss: 0.9376, acc: 0.5534\n",
            "[epoch 0] loss: 0.9349, acc: 0.5552\n",
            "[epoch 0] loss: 0.9343, acc: 0.5555\n",
            "[epoch 0] loss: 0.9318, acc: 0.5562\n",
            "[epoch 0] loss: 0.9311, acc: 0.5573\n",
            "[epoch 0] loss: 0.9296, acc: 0.5588\n",
            "[epoch 0] loss: 0.9290, acc: 0.5586\n",
            "> val_acc: 0.5943, val_f1: 0.5943\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.5943\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 1\n",
            "[epoch 1] loss: 0.7822, acc: 0.6200\n",
            "[epoch 1] loss: 0.8469, acc: 0.6200\n",
            "[epoch 1] loss: 0.8330, acc: 0.6218\n",
            "[epoch 1] loss: 0.8168, acc: 0.6338\n",
            "[epoch 1] loss: 0.8490, acc: 0.6181\n",
            "[epoch 1] loss: 0.8629, acc: 0.6038\n",
            "[epoch 1] loss: 0.8790, acc: 0.5903\n",
            "[epoch 1] loss: 0.8704, acc: 0.5994\n",
            "[epoch 1] loss: 0.8627, acc: 0.6049\n",
            "[epoch 1] loss: 0.8579, acc: 0.6096\n",
            "[epoch 1] loss: 0.8577, acc: 0.6078\n",
            "[epoch 1] loss: 0.8490, acc: 0.6118\n",
            "[epoch 1] loss: 0.8549, acc: 0.6072\n",
            "[epoch 1] loss: 0.8558, acc: 0.6091\n",
            "[epoch 1] loss: 0.8577, acc: 0.6076\n",
            "[epoch 1] loss: 0.8558, acc: 0.6092\n",
            "[epoch 1] loss: 0.8551, acc: 0.6101\n",
            "[epoch 1] loss: 0.8541, acc: 0.6116\n",
            "[epoch 1] loss: 0.8537, acc: 0.6125\n",
            "[epoch 1] loss: 0.8510, acc: 0.6146\n",
            "[epoch 1] loss: 0.8514, acc: 0.6147\n",
            "[epoch 1] loss: 0.8513, acc: 0.6140\n",
            "[epoch 1] loss: 0.8503, acc: 0.6133\n",
            "[epoch 1] loss: 0.8490, acc: 0.6129\n",
            "[epoch 1] loss: 0.8486, acc: 0.6134\n",
            "[epoch 1] loss: 0.8478, acc: 0.6132\n",
            "[epoch 1] loss: 0.8488, acc: 0.6127\n",
            "[epoch 1] loss: 0.8479, acc: 0.6128\n",
            "[epoch 1] loss: 0.8466, acc: 0.6140\n",
            "[epoch 1] loss: 0.8473, acc: 0.6134\n",
            "[epoch 1] loss: 0.8475, acc: 0.6121\n",
            "[epoch 1] loss: 0.8466, acc: 0.6113\n",
            "[epoch 1] loss: 0.8472, acc: 0.6112\n",
            "[epoch 1] loss: 0.8490, acc: 0.6107\n",
            "[epoch 1] loss: 0.8489, acc: 0.6105\n",
            "[epoch 1] loss: 0.8501, acc: 0.6088\n",
            "[epoch 1] loss: 0.8491, acc: 0.6091\n",
            "[epoch 1] loss: 0.8505, acc: 0.6077\n",
            "[epoch 1] loss: 0.8519, acc: 0.6061\n",
            "[epoch 1] loss: 0.8513, acc: 0.6069\n",
            "[epoch 1] loss: 0.8504, acc: 0.6080\n",
            "[epoch 1] loss: 0.8499, acc: 0.6082\n",
            "[epoch 1] loss: 0.8492, acc: 0.6085\n",
            "[epoch 1] loss: 0.8495, acc: 0.6088\n",
            "[epoch 1] loss: 0.8490, acc: 0.6090\n",
            "> val_acc: 0.6183, val_f1: 0.6183\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6183\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 2\n",
            "[epoch 2] loss: 0.8067, acc: 0.6300\n",
            "[epoch 2] loss: 0.8474, acc: 0.6029\n",
            "[epoch 2] loss: 0.8405, acc: 0.5983\n",
            "[epoch 2] loss: 0.8404, acc: 0.6094\n",
            "[epoch 2] loss: 0.8443, acc: 0.6036\n",
            "[epoch 2] loss: 0.8305, acc: 0.6059\n",
            "[epoch 2] loss: 0.8284, acc: 0.6119\n",
            "[epoch 2] loss: 0.8397, acc: 0.6092\n",
            "[epoch 2] loss: 0.8404, acc: 0.6133\n",
            "[epoch 2] loss: 0.8371, acc: 0.6191\n",
            "[epoch 2] loss: 0.8346, acc: 0.6200\n",
            "[epoch 2] loss: 0.8345, acc: 0.6200\n",
            "[epoch 2] loss: 0.8360, acc: 0.6219\n",
            "[epoch 2] loss: 0.8336, acc: 0.6212\n",
            "[epoch 2] loss: 0.8329, acc: 0.6217\n",
            "[epoch 2] loss: 0.8306, acc: 0.6226\n",
            "[epoch 2] loss: 0.8346, acc: 0.6193\n",
            "[epoch 2] loss: 0.8354, acc: 0.6193\n",
            "[epoch 2] loss: 0.8327, acc: 0.6198\n",
            "[epoch 2] loss: 0.8308, acc: 0.6208\n",
            "[epoch 2] loss: 0.8322, acc: 0.6194\n",
            "[epoch 2] loss: 0.8320, acc: 0.6176\n",
            "[epoch 2] loss: 0.8320, acc: 0.6166\n",
            "[epoch 2] loss: 0.8306, acc: 0.6190\n",
            "[epoch 2] loss: 0.8307, acc: 0.6193\n",
            "[epoch 2] loss: 0.8270, acc: 0.6225\n",
            "[epoch 2] loss: 0.8277, acc: 0.6223\n",
            "[epoch 2] loss: 0.8296, acc: 0.6212\n",
            "[epoch 2] loss: 0.8301, acc: 0.6203\n",
            "[epoch 2] loss: 0.8275, acc: 0.6220\n",
            "[epoch 2] loss: 0.8272, acc: 0.6224\n",
            "[epoch 2] loss: 0.8263, acc: 0.6219\n",
            "[epoch 2] loss: 0.8246, acc: 0.6230\n",
            "[epoch 2] loss: 0.8241, acc: 0.6225\n",
            "[epoch 2] loss: 0.8237, acc: 0.6220\n",
            "[epoch 2] loss: 0.8225, acc: 0.6225\n",
            "[epoch 2] loss: 0.8210, acc: 0.6237\n",
            "[epoch 2] loss: 0.8197, acc: 0.6245\n",
            "[epoch 2] loss: 0.8216, acc: 0.6242\n",
            "[epoch 2] loss: 0.8212, acc: 0.6250\n",
            "[epoch 2] loss: 0.8210, acc: 0.6249\n",
            "[epoch 2] loss: 0.8194, acc: 0.6263\n",
            "[epoch 2] loss: 0.8201, acc: 0.6258\n",
            "[epoch 2] loss: 0.8204, acc: 0.6256\n",
            "[epoch 2] loss: 0.8201, acc: 0.6261\n",
            "> val_acc: 0.6280, val_f1: 0.6280\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.628\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 3\n",
            "[epoch 3] loss: 0.7816, acc: 0.6400\n",
            "[epoch 3] loss: 0.8119, acc: 0.6300\n",
            "[epoch 3] loss: 0.7903, acc: 0.6400\n",
            "[epoch 3] loss: 0.8018, acc: 0.6400\n",
            "[epoch 3] loss: 0.8036, acc: 0.6348\n",
            "[epoch 3] loss: 0.7994, acc: 0.6321\n",
            "[epoch 3] loss: 0.8013, acc: 0.6339\n",
            "[epoch 3] loss: 0.8045, acc: 0.6332\n",
            "[epoch 3] loss: 0.8058, acc: 0.6284\n",
            "[epoch 3] loss: 0.8003, acc: 0.6304\n",
            "[epoch 3] loss: 0.8011, acc: 0.6306\n",
            "[epoch 3] loss: 0.7994, acc: 0.6341\n",
            "[epoch 3] loss: 0.7979, acc: 0.6365\n",
            "[epoch 3] loss: 0.8012, acc: 0.6350\n",
            "[epoch 3] loss: 0.7985, acc: 0.6386\n",
            "[epoch 3] loss: 0.8031, acc: 0.6359\n",
            "[epoch 3] loss: 0.7953, acc: 0.6419\n",
            "[epoch 3] loss: 0.7971, acc: 0.6411\n",
            "[epoch 3] loss: 0.7979, acc: 0.6411\n",
            "[epoch 3] loss: 0.7975, acc: 0.6408\n",
            "[epoch 3] loss: 0.7955, acc: 0.6421\n",
            "[epoch 3] loss: 0.7949, acc: 0.6424\n",
            "[epoch 3] loss: 0.7945, acc: 0.6421\n",
            "[epoch 3] loss: 0.7944, acc: 0.6420\n",
            "[epoch 3] loss: 0.7954, acc: 0.6397\n",
            "[epoch 3] loss: 0.7964, acc: 0.6386\n",
            "[epoch 3] loss: 0.7986, acc: 0.6374\n",
            "[epoch 3] loss: 0.7993, acc: 0.6365\n",
            "[epoch 3] loss: 0.7984, acc: 0.6376\n",
            "[epoch 3] loss: 0.7998, acc: 0.6364\n",
            "[epoch 3] loss: 0.8002, acc: 0.6362\n",
            "[epoch 3] loss: 0.7989, acc: 0.6368\n",
            "[epoch 3] loss: 0.8002, acc: 0.6356\n",
            "[epoch 3] loss: 0.7991, acc: 0.6367\n",
            "[epoch 3] loss: 0.7980, acc: 0.6368\n",
            "[epoch 3] loss: 0.7994, acc: 0.6360\n",
            "[epoch 3] loss: 0.7991, acc: 0.6364\n",
            "[epoch 3] loss: 0.7984, acc: 0.6370\n",
            "[epoch 3] loss: 0.7994, acc: 0.6365\n",
            "[epoch 3] loss: 0.8001, acc: 0.6362\n",
            "[epoch 3] loss: 0.7996, acc: 0.6364\n",
            "[epoch 3] loss: 0.7985, acc: 0.6370\n",
            "[epoch 3] loss: 0.7998, acc: 0.6362\n",
            "[epoch 3] loss: 0.7993, acc: 0.6364\n",
            "[epoch 3] loss: 0.7982, acc: 0.6378\n",
            "> val_acc: 0.6325, val_f1: 0.6325\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6325\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 4\n",
            "[epoch 4] loss: 0.6855, acc: 0.6900\n",
            "[epoch 4] loss: 0.7638, acc: 0.6467\n",
            "[epoch 4] loss: 0.7969, acc: 0.6343\n",
            "[epoch 4] loss: 0.7835, acc: 0.6453\n",
            "[epoch 4] loss: 0.7922, acc: 0.6358\n",
            "[epoch 4] loss: 0.7816, acc: 0.6414\n",
            "[epoch 4] loss: 0.7812, acc: 0.6406\n",
            "[epoch 4] loss: 0.7770, acc: 0.6426\n",
            "[epoch 4] loss: 0.7793, acc: 0.6432\n",
            "[epoch 4] loss: 0.7739, acc: 0.6445\n",
            "[epoch 4] loss: 0.7709, acc: 0.6456\n",
            "[epoch 4] loss: 0.7738, acc: 0.6447\n",
            "[epoch 4] loss: 0.7799, acc: 0.6406\n",
            "[epoch 4] loss: 0.7781, acc: 0.6449\n",
            "[epoch 4] loss: 0.7787, acc: 0.6435\n",
            "[epoch 4] loss: 0.7825, acc: 0.6428\n",
            "[epoch 4] loss: 0.7797, acc: 0.6460\n",
            "[epoch 4] loss: 0.7779, acc: 0.6458\n",
            "[epoch 4] loss: 0.7824, acc: 0.6464\n",
            "[epoch 4] loss: 0.7821, acc: 0.6479\n",
            "[epoch 4] loss: 0.7813, acc: 0.6488\n",
            "[epoch 4] loss: 0.7832, acc: 0.6486\n",
            "[epoch 4] loss: 0.7848, acc: 0.6481\n",
            "[epoch 4] loss: 0.7837, acc: 0.6482\n",
            "[epoch 4] loss: 0.7836, acc: 0.6476\n",
            "[epoch 4] loss: 0.7817, acc: 0.6501\n",
            "[epoch 4] loss: 0.7811, acc: 0.6516\n",
            "[epoch 4] loss: 0.7787, acc: 0.6532\n",
            "[epoch 4] loss: 0.7782, acc: 0.6529\n",
            "[epoch 4] loss: 0.7770, acc: 0.6532\n",
            "[epoch 4] loss: 0.7789, acc: 0.6516\n",
            "[epoch 4] loss: 0.7771, acc: 0.6527\n",
            "[epoch 4] loss: 0.7772, acc: 0.6515\n",
            "[epoch 4] loss: 0.7756, acc: 0.6516\n",
            "[epoch 4] loss: 0.7769, acc: 0.6500\n",
            "[epoch 4] loss: 0.7784, acc: 0.6492\n",
            "[epoch 4] loss: 0.7774, acc: 0.6499\n",
            "[epoch 4] loss: 0.7781, acc: 0.6496\n",
            "[epoch 4] loss: 0.7800, acc: 0.6484\n",
            "[epoch 4] loss: 0.7790, acc: 0.6481\n",
            "[epoch 4] loss: 0.7799, acc: 0.6474\n",
            "[epoch 4] loss: 0.7804, acc: 0.6478\n",
            "[epoch 4] loss: 0.7796, acc: 0.6487\n",
            "[epoch 4] loss: 0.7789, acc: 0.6480\n",
            "[epoch 4] loss: 0.7801, acc: 0.6479\n",
            "> val_acc: 0.6347, val_f1: 0.6347\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6347\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 5\n",
            "[epoch 5] loss: 0.6797, acc: 0.6800\n",
            "[epoch 5] loss: 0.7436, acc: 0.6560\n",
            "[epoch 5] loss: 0.7555, acc: 0.6560\n",
            "[epoch 5] loss: 0.7569, acc: 0.6560\n",
            "[epoch 5] loss: 0.7615, acc: 0.6512\n",
            "[epoch 5] loss: 0.7554, acc: 0.6600\n",
            "[epoch 5] loss: 0.7528, acc: 0.6606\n",
            "[epoch 5] loss: 0.7551, acc: 0.6610\n",
            "[epoch 5] loss: 0.7634, acc: 0.6551\n",
            "[epoch 5] loss: 0.7624, acc: 0.6580\n",
            "[epoch 5] loss: 0.7630, acc: 0.6575\n",
            "[epoch 5] loss: 0.7658, acc: 0.6570\n",
            "[epoch 5] loss: 0.7654, acc: 0.6591\n",
            "[epoch 5] loss: 0.7712, acc: 0.6563\n",
            "[epoch 5] loss: 0.7708, acc: 0.6597\n",
            "[epoch 5] loss: 0.7672, acc: 0.6613\n",
            "[epoch 5] loss: 0.7641, acc: 0.6621\n",
            "[epoch 5] loss: 0.7656, acc: 0.6604\n",
            "[epoch 5] loss: 0.7638, acc: 0.6598\n",
            "[epoch 5] loss: 0.7676, acc: 0.6588\n",
            "[epoch 5] loss: 0.7659, acc: 0.6592\n",
            "[epoch 5] loss: 0.7647, acc: 0.6595\n",
            "[epoch 5] loss: 0.7635, acc: 0.6595\n",
            "[epoch 5] loss: 0.7666, acc: 0.6575\n",
            "[epoch 5] loss: 0.7661, acc: 0.6573\n",
            "[epoch 5] loss: 0.7696, acc: 0.6534\n",
            "[epoch 5] loss: 0.7707, acc: 0.6532\n",
            "[epoch 5] loss: 0.7690, acc: 0.6553\n",
            "[epoch 5] loss: 0.7678, acc: 0.6568\n",
            "[epoch 5] loss: 0.7668, acc: 0.6576\n",
            "[epoch 5] loss: 0.7662, acc: 0.6588\n",
            "[epoch 5] loss: 0.7644, acc: 0.6599\n",
            "[epoch 5] loss: 0.7647, acc: 0.6593\n",
            "[epoch 5] loss: 0.7630, acc: 0.6601\n",
            "[epoch 5] loss: 0.7634, acc: 0.6601\n",
            "[epoch 5] loss: 0.7632, acc: 0.6596\n",
            "[epoch 5] loss: 0.7652, acc: 0.6589\n",
            "[epoch 5] loss: 0.7623, acc: 0.6598\n",
            "[epoch 5] loss: 0.7619, acc: 0.6604\n",
            "[epoch 5] loss: 0.7626, acc: 0.6597\n",
            "[epoch 5] loss: 0.7623, acc: 0.6600\n",
            "[epoch 5] loss: 0.7624, acc: 0.6594\n",
            "[epoch 5] loss: 0.7619, acc: 0.6597\n",
            "[epoch 5] loss: 0.7613, acc: 0.6597\n",
            "> val_acc: 0.6392, val_f1: 0.6392\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6392\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 6\n",
            "[epoch 6] loss: 0.5936, acc: 0.8200\n",
            "[epoch 6] loss: 0.7564, acc: 0.6833\n",
            "[epoch 6] loss: 0.7585, acc: 0.6818\n",
            "[epoch 6] loss: 0.7539, acc: 0.6675\n",
            "[epoch 6] loss: 0.7442, acc: 0.6724\n",
            "[epoch 6] loss: 0.7501, acc: 0.6715\n",
            "[epoch 6] loss: 0.7592, acc: 0.6626\n",
            "[epoch 6] loss: 0.7487, acc: 0.6656\n",
            "[epoch 6] loss: 0.7502, acc: 0.6659\n",
            "[epoch 6] loss: 0.7509, acc: 0.6687\n",
            "[epoch 6] loss: 0.7499, acc: 0.6694\n",
            "[epoch 6] loss: 0.7584, acc: 0.6650\n",
            "[epoch 6] loss: 0.7557, acc: 0.6682\n",
            "[epoch 6] loss: 0.7536, acc: 0.6691\n",
            "[epoch 6] loss: 0.7548, acc: 0.6670\n",
            "[epoch 6] loss: 0.7547, acc: 0.6668\n",
            "[epoch 6] loss: 0.7586, acc: 0.6654\n",
            "[epoch 6] loss: 0.7596, acc: 0.6623\n",
            "[epoch 6] loss: 0.7563, acc: 0.6653\n",
            "[epoch 6] loss: 0.7501, acc: 0.6671\n",
            "[epoch 6] loss: 0.7482, acc: 0.6667\n",
            "[epoch 6] loss: 0.7469, acc: 0.6675\n",
            "[epoch 6] loss: 0.7464, acc: 0.6690\n",
            "[epoch 6] loss: 0.7492, acc: 0.6667\n",
            "[epoch 6] loss: 0.7504, acc: 0.6655\n",
            "[epoch 6] loss: 0.7470, acc: 0.6665\n",
            "[epoch 6] loss: 0.7463, acc: 0.6673\n",
            "[epoch 6] loss: 0.7453, acc: 0.6684\n",
            "[epoch 6] loss: 0.7435, acc: 0.6702\n",
            "[epoch 6] loss: 0.7426, acc: 0.6703\n",
            "[epoch 6] loss: 0.7402, acc: 0.6715\n",
            "[epoch 6] loss: 0.7374, acc: 0.6733\n",
            "[epoch 6] loss: 0.7411, acc: 0.6718\n",
            "[epoch 6] loss: 0.7409, acc: 0.6713\n",
            "[epoch 6] loss: 0.7384, acc: 0.6724\n",
            "[epoch 6] loss: 0.7406, acc: 0.6711\n",
            "[epoch 6] loss: 0.7406, acc: 0.6710\n",
            "[epoch 6] loss: 0.7419, acc: 0.6700\n",
            "[epoch 6] loss: 0.7429, acc: 0.6692\n",
            "[epoch 6] loss: 0.7445, acc: 0.6683\n",
            "[epoch 6] loss: 0.7452, acc: 0.6674\n",
            "[epoch 6] loss: 0.7445, acc: 0.6675\n",
            "[epoch 6] loss: 0.7455, acc: 0.6665\n",
            "[epoch 6] loss: 0.7462, acc: 0.6666\n",
            "[epoch 6] loss: 0.7469, acc: 0.6663\n",
            "> val_acc: 0.6452, val_f1: 0.6452\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6452\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 7\n",
            "[epoch 7] loss: 0.6694, acc: 0.6500\n",
            "[epoch 7] loss: 0.6997, acc: 0.6829\n",
            "[epoch 7] loss: 0.7263, acc: 0.6833\n",
            "[epoch 7] loss: 0.7300, acc: 0.6776\n",
            "[epoch 7] loss: 0.7153, acc: 0.6836\n",
            "[epoch 7] loss: 0.7133, acc: 0.6815\n",
            "[epoch 7] loss: 0.7187, acc: 0.6719\n",
            "[epoch 7] loss: 0.7253, acc: 0.6697\n",
            "[epoch 7] loss: 0.7220, acc: 0.6738\n",
            "[epoch 7] loss: 0.7234, acc: 0.6736\n",
            "[epoch 7] loss: 0.7249, acc: 0.6769\n",
            "[epoch 7] loss: 0.7264, acc: 0.6751\n",
            "[epoch 7] loss: 0.7265, acc: 0.6735\n",
            "[epoch 7] loss: 0.7289, acc: 0.6731\n",
            "[epoch 7] loss: 0.7300, acc: 0.6708\n",
            "[epoch 7] loss: 0.7281, acc: 0.6719\n",
            "[epoch 7] loss: 0.7294, acc: 0.6712\n",
            "[epoch 7] loss: 0.7290, acc: 0.6729\n",
            "[epoch 7] loss: 0.7284, acc: 0.6737\n",
            "[epoch 7] loss: 0.7313, acc: 0.6730\n",
            "[epoch 7] loss: 0.7292, acc: 0.6733\n",
            "[epoch 7] loss: 0.7306, acc: 0.6738\n",
            "[epoch 7] loss: 0.7334, acc: 0.6736\n",
            "[epoch 7] loss: 0.7323, acc: 0.6740\n",
            "[epoch 7] loss: 0.7348, acc: 0.6730\n",
            "[epoch 7] loss: 0.7333, acc: 0.6732\n",
            "[epoch 7] loss: 0.7348, acc: 0.6720\n",
            "[epoch 7] loss: 0.7332, acc: 0.6731\n",
            "[epoch 7] loss: 0.7325, acc: 0.6737\n",
            "[epoch 7] loss: 0.7320, acc: 0.6736\n",
            "[epoch 7] loss: 0.7345, acc: 0.6725\n",
            "[epoch 7] loss: 0.7354, acc: 0.6724\n",
            "[epoch 7] loss: 0.7357, acc: 0.6722\n",
            "[epoch 7] loss: 0.7356, acc: 0.6722\n",
            "[epoch 7] loss: 0.7349, acc: 0.6715\n",
            "[epoch 7] loss: 0.7340, acc: 0.6719\n",
            "[epoch 7] loss: 0.7354, acc: 0.6708\n",
            "[epoch 7] loss: 0.7376, acc: 0.6694\n",
            "[epoch 7] loss: 0.7369, acc: 0.6696\n",
            "[epoch 7] loss: 0.7370, acc: 0.6693\n",
            "[epoch 7] loss: 0.7360, acc: 0.6705\n",
            "[epoch 7] loss: 0.7366, acc: 0.6705\n",
            "[epoch 7] loss: 0.7363, acc: 0.6708\n",
            "[epoch 7] loss: 0.7348, acc: 0.6721\n",
            "[epoch 7] loss: 0.7337, acc: 0.6728\n",
            "> val_acc: 0.6542, val_f1: 0.6542\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 8\n",
            "[epoch 8] loss: 0.6871, acc: 0.7200\n",
            "[epoch 8] loss: 0.7300, acc: 0.7025\n",
            "[epoch 8] loss: 0.7174, acc: 0.7015\n",
            "[epoch 8] loss: 0.7123, acc: 0.6989\n",
            "[epoch 8] loss: 0.7255, acc: 0.6861\n",
            "[epoch 8] loss: 0.7241, acc: 0.6886\n",
            "[epoch 8] loss: 0.7209, acc: 0.6861\n",
            "[epoch 8] loss: 0.7257, acc: 0.6826\n",
            "[epoch 8] loss: 0.7332, acc: 0.6758\n",
            "[epoch 8] loss: 0.7403, acc: 0.6704\n",
            "[epoch 8] loss: 0.7345, acc: 0.6709\n",
            "[epoch 8] loss: 0.7427, acc: 0.6676\n",
            "[epoch 8] loss: 0.7499, acc: 0.6606\n",
            "[epoch 8] loss: 0.7449, acc: 0.6647\n",
            "[epoch 8] loss: 0.7428, acc: 0.6677\n",
            "[epoch 8] loss: 0.7399, acc: 0.6692\n",
            "[epoch 8] loss: 0.7398, acc: 0.6704\n",
            "[epoch 8] loss: 0.7441, acc: 0.6675\n",
            "[epoch 8] loss: 0.7455, acc: 0.6665\n",
            "[epoch 8] loss: 0.7459, acc: 0.6663\n",
            "[epoch 8] loss: 0.7429, acc: 0.6668\n",
            "[epoch 8] loss: 0.7428, acc: 0.6665\n",
            "[epoch 8] loss: 0.7405, acc: 0.6676\n",
            "[epoch 8] loss: 0.7394, acc: 0.6697\n",
            "[epoch 8] loss: 0.7378, acc: 0.6717\n",
            "[epoch 8] loss: 0.7371, acc: 0.6723\n",
            "[epoch 8] loss: 0.7371, acc: 0.6716\n",
            "[epoch 8] loss: 0.7353, acc: 0.6722\n",
            "[epoch 8] loss: 0.7350, acc: 0.6706\n",
            "[epoch 8] loss: 0.7321, acc: 0.6722\n",
            "[epoch 8] loss: 0.7308, acc: 0.6725\n",
            "[epoch 8] loss: 0.7275, acc: 0.6747\n",
            "[epoch 8] loss: 0.7276, acc: 0.6753\n",
            "[epoch 8] loss: 0.7281, acc: 0.6754\n",
            "[epoch 8] loss: 0.7256, acc: 0.6776\n",
            "[epoch 8] loss: 0.7267, acc: 0.6780\n",
            "[epoch 8] loss: 0.7258, acc: 0.6787\n",
            "[epoch 8] loss: 0.7251, acc: 0.6798\n",
            "[epoch 8] loss: 0.7250, acc: 0.6800\n",
            "[epoch 8] loss: 0.7241, acc: 0.6808\n",
            "[epoch 8] loss: 0.7241, acc: 0.6807\n",
            "[epoch 8] loss: 0.7233, acc: 0.6810\n",
            "[epoch 8] loss: 0.7238, acc: 0.6800\n",
            "[epoch 8] loss: 0.7241, acc: 0.6792\n",
            "[epoch 8] loss: 0.7240, acc: 0.6791\n",
            "> val_acc: 0.6579, val_f1: 0.6579\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6579\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 9\n",
            "[epoch 9] loss: 0.6488, acc: 0.7150\n",
            "[epoch 9] loss: 0.6753, acc: 0.6978\n",
            "[epoch 9] loss: 0.6938, acc: 0.6929\n",
            "[epoch 9] loss: 0.6921, acc: 0.6863\n",
            "[epoch 9] loss: 0.6936, acc: 0.6808\n",
            "[epoch 9] loss: 0.6945, acc: 0.6828\n",
            "[epoch 9] loss: 0.6925, acc: 0.6888\n",
            "[epoch 9] loss: 0.6935, acc: 0.6923\n",
            "[epoch 9] loss: 0.6946, acc: 0.6918\n",
            "[epoch 9] loss: 0.6932, acc: 0.6951\n",
            "[epoch 9] loss: 0.6982, acc: 0.6941\n",
            "[epoch 9] loss: 0.7028, acc: 0.6905\n",
            "[epoch 9] loss: 0.7013, acc: 0.6913\n",
            "[epoch 9] loss: 0.7029, acc: 0.6907\n",
            "[epoch 9] loss: 0.7043, acc: 0.6905\n",
            "[epoch 9] loss: 0.6990, acc: 0.6929\n",
            "[epoch 9] loss: 0.6963, acc: 0.6950\n",
            "[epoch 9] loss: 0.6980, acc: 0.6933\n",
            "[epoch 9] loss: 0.7036, acc: 0.6889\n",
            "[epoch 9] loss: 0.7034, acc: 0.6883\n",
            "[epoch 9] loss: 0.7056, acc: 0.6873\n",
            "[epoch 9] loss: 0.7057, acc: 0.6859\n",
            "[epoch 9] loss: 0.7091, acc: 0.6846\n",
            "[epoch 9] loss: 0.7079, acc: 0.6844\n",
            "[epoch 9] loss: 0.7099, acc: 0.6848\n",
            "[epoch 9] loss: 0.7114, acc: 0.6840\n",
            "[epoch 9] loss: 0.7092, acc: 0.6854\n",
            "[epoch 9] loss: 0.7062, acc: 0.6872\n",
            "[epoch 9] loss: 0.7068, acc: 0.6875\n",
            "[epoch 9] loss: 0.7089, acc: 0.6867\n",
            "[epoch 9] loss: 0.7080, acc: 0.6870\n",
            "[epoch 9] loss: 0.7090, acc: 0.6864\n",
            "[epoch 9] loss: 0.7077, acc: 0.6876\n",
            "[epoch 9] loss: 0.7091, acc: 0.6872\n",
            "[epoch 9] loss: 0.7119, acc: 0.6856\n",
            "[epoch 9] loss: 0.7119, acc: 0.6847\n",
            "[epoch 9] loss: 0.7116, acc: 0.6847\n",
            "[epoch 9] loss: 0.7124, acc: 0.6851\n",
            "[epoch 9] loss: 0.7142, acc: 0.6842\n",
            "[epoch 9] loss: 0.7151, acc: 0.6831\n",
            "[epoch 9] loss: 0.7157, acc: 0.6826\n",
            "[epoch 9] loss: 0.7155, acc: 0.6833\n",
            "[epoch 9] loss: 0.7156, acc: 0.6830\n",
            "[epoch 9] loss: 0.7154, acc: 0.6826\n",
            "[epoch 9] loss: 0.7144, acc: 0.6835\n",
            "> val_acc: 0.6519, val_f1: 0.6519\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 10\n",
            "[epoch 10] loss: 0.7319, acc: 0.6640\n",
            "[epoch 10] loss: 0.7012, acc: 0.6820\n",
            "[epoch 10] loss: 0.6894, acc: 0.6960\n",
            "[epoch 10] loss: 0.6944, acc: 0.6810\n",
            "[epoch 10] loss: 0.6903, acc: 0.6880\n",
            "[epoch 10] loss: 0.7019, acc: 0.6853\n",
            "[epoch 10] loss: 0.6970, acc: 0.6909\n",
            "[epoch 10] loss: 0.6997, acc: 0.6910\n",
            "[epoch 10] loss: 0.7091, acc: 0.6858\n",
            "[epoch 10] loss: 0.7114, acc: 0.6852\n",
            "[epoch 10] loss: 0.7139, acc: 0.6844\n",
            "[epoch 10] loss: 0.7098, acc: 0.6863\n",
            "[epoch 10] loss: 0.7054, acc: 0.6898\n",
            "[epoch 10] loss: 0.7032, acc: 0.6923\n",
            "[epoch 10] loss: 0.7054, acc: 0.6923\n",
            "[epoch 10] loss: 0.7022, acc: 0.6955\n",
            "[epoch 10] loss: 0.7024, acc: 0.6951\n",
            "[epoch 10] loss: 0.7020, acc: 0.6951\n",
            "[epoch 10] loss: 0.7031, acc: 0.6947\n",
            "[epoch 10] loss: 0.7052, acc: 0.6914\n",
            "[epoch 10] loss: 0.7069, acc: 0.6897\n",
            "[epoch 10] loss: 0.7062, acc: 0.6898\n",
            "[epoch 10] loss: 0.7092, acc: 0.6877\n",
            "[epoch 10] loss: 0.7091, acc: 0.6873\n",
            "[epoch 10] loss: 0.7100, acc: 0.6859\n",
            "[epoch 10] loss: 0.7066, acc: 0.6880\n",
            "[epoch 10] loss: 0.7059, acc: 0.6880\n",
            "[epoch 10] loss: 0.7096, acc: 0.6860\n",
            "[epoch 10] loss: 0.7076, acc: 0.6862\n",
            "[epoch 10] loss: 0.7080, acc: 0.6853\n",
            "[epoch 10] loss: 0.7087, acc: 0.6844\n",
            "[epoch 10] loss: 0.7070, acc: 0.6857\n",
            "[epoch 10] loss: 0.7064, acc: 0.6859\n",
            "[epoch 10] loss: 0.7082, acc: 0.6852\n",
            "[epoch 10] loss: 0.7078, acc: 0.6862\n",
            "[epoch 10] loss: 0.7081, acc: 0.6863\n",
            "[epoch 10] loss: 0.7084, acc: 0.6864\n",
            "[epoch 10] loss: 0.7072, acc: 0.6871\n",
            "[epoch 10] loss: 0.7077, acc: 0.6870\n",
            "[epoch 10] loss: 0.7073, acc: 0.6869\n",
            "[epoch 10] loss: 0.7069, acc: 0.6880\n",
            "[epoch 10] loss: 0.7077, acc: 0.6872\n",
            "[epoch 10] loss: 0.7063, acc: 0.6881\n",
            "[epoch 10] loss: 0.7048, acc: 0.6890\n",
            "> val_acc: 0.6519, val_f1: 0.6519\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 11\n",
            "[epoch 11] loss: 0.7253, acc: 0.6800\n",
            "[epoch 11] loss: 0.6701, acc: 0.7100\n",
            "[epoch 11] loss: 0.6777, acc: 0.6982\n",
            "[epoch 11] loss: 0.6796, acc: 0.6913\n",
            "[epoch 11] loss: 0.6909, acc: 0.6933\n",
            "[epoch 11] loss: 0.6822, acc: 0.6962\n",
            "[epoch 11] loss: 0.6743, acc: 0.7000\n",
            "[epoch 11] loss: 0.6765, acc: 0.6989\n",
            "[epoch 11] loss: 0.6912, acc: 0.6941\n",
            "[epoch 11] loss: 0.6868, acc: 0.6974\n",
            "[epoch 11] loss: 0.6820, acc: 0.7020\n",
            "[epoch 11] loss: 0.6847, acc: 0.7011\n",
            "[epoch 11] loss: 0.6817, acc: 0.7020\n",
            "[epoch 11] loss: 0.6857, acc: 0.6994\n",
            "[epoch 11] loss: 0.6899, acc: 0.6946\n",
            "[epoch 11] loss: 0.6888, acc: 0.6942\n",
            "[epoch 11] loss: 0.6922, acc: 0.6919\n",
            "[epoch 11] loss: 0.6941, acc: 0.6891\n",
            "[epoch 11] loss: 0.6920, acc: 0.6899\n",
            "[epoch 11] loss: 0.6890, acc: 0.6902\n",
            "[epoch 11] loss: 0.6899, acc: 0.6897\n",
            "[epoch 11] loss: 0.6921, acc: 0.6887\n",
            "[epoch 11] loss: 0.6928, acc: 0.6888\n",
            "[epoch 11] loss: 0.6972, acc: 0.6876\n",
            "[epoch 11] loss: 0.6974, acc: 0.6878\n",
            "[epoch 11] loss: 0.6975, acc: 0.6879\n",
            "[epoch 11] loss: 0.6937, acc: 0.6910\n",
            "[epoch 11] loss: 0.6930, acc: 0.6924\n",
            "[epoch 11] loss: 0.6936, acc: 0.6929\n",
            "[epoch 11] loss: 0.6961, acc: 0.6919\n",
            "[epoch 11] loss: 0.6949, acc: 0.6927\n",
            "[epoch 11] loss: 0.6943, acc: 0.6942\n",
            "[epoch 11] loss: 0.6925, acc: 0.6945\n",
            "[epoch 11] loss: 0.6934, acc: 0.6942\n",
            "[epoch 11] loss: 0.6933, acc: 0.6940\n",
            "[epoch 11] loss: 0.6934, acc: 0.6943\n",
            "[epoch 11] loss: 0.6950, acc: 0.6929\n",
            "[epoch 11] loss: 0.6937, acc: 0.6937\n",
            "[epoch 11] loss: 0.6944, acc: 0.6935\n",
            "[epoch 11] loss: 0.6963, acc: 0.6933\n",
            "[epoch 11] loss: 0.6972, acc: 0.6932\n",
            "[epoch 11] loss: 0.6969, acc: 0.6937\n",
            "[epoch 11] loss: 0.6960, acc: 0.6947\n",
            "[epoch 11] loss: 0.6983, acc: 0.6932\n",
            "[epoch 11] loss: 0.6977, acc: 0.6933\n",
            "> val_acc: 0.6549, val_f1: 0.6549\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 12\n",
            "[epoch 12] loss: 0.7019, acc: 0.7100\n",
            "[epoch 12] loss: 0.7212, acc: 0.6657\n",
            "[epoch 12] loss: 0.6937, acc: 0.6900\n",
            "[epoch 12] loss: 0.6796, acc: 0.6918\n",
            "[epoch 12] loss: 0.6788, acc: 0.6936\n",
            "[epoch 12] loss: 0.6729, acc: 0.6978\n",
            "[epoch 12] loss: 0.6835, acc: 0.6956\n",
            "[epoch 12] loss: 0.6864, acc: 0.6962\n",
            "[epoch 12] loss: 0.6775, acc: 0.7000\n",
            "[epoch 12] loss: 0.6798, acc: 0.6987\n",
            "[epoch 12] loss: 0.6837, acc: 0.6973\n",
            "[epoch 12] loss: 0.6802, acc: 0.7021\n",
            "[epoch 12] loss: 0.6864, acc: 0.6984\n",
            "[epoch 12] loss: 0.6865, acc: 0.6991\n",
            "[epoch 12] loss: 0.6850, acc: 0.7008\n",
            "[epoch 12] loss: 0.6854, acc: 0.6997\n",
            "[epoch 12] loss: 0.6893, acc: 0.6983\n",
            "[epoch 12] loss: 0.6854, acc: 0.7002\n",
            "[epoch 12] loss: 0.6867, acc: 0.6998\n",
            "[epoch 12] loss: 0.6833, acc: 0.7010\n",
            "[epoch 12] loss: 0.6814, acc: 0.7010\n",
            "[epoch 12] loss: 0.6788, acc: 0.7030\n",
            "[epoch 12] loss: 0.6780, acc: 0.7032\n",
            "[epoch 12] loss: 0.6802, acc: 0.7024\n",
            "[epoch 12] loss: 0.6823, acc: 0.6997\n",
            "[epoch 12] loss: 0.6829, acc: 0.6994\n",
            "[epoch 12] loss: 0.6841, acc: 0.6997\n",
            "[epoch 12] loss: 0.6848, acc: 0.7003\n",
            "[epoch 12] loss: 0.6868, acc: 0.6989\n",
            "[epoch 12] loss: 0.6867, acc: 0.6989\n",
            "[epoch 12] loss: 0.6864, acc: 0.6980\n",
            "[epoch 12] loss: 0.6854, acc: 0.6983\n",
            "[epoch 12] loss: 0.6852, acc: 0.6994\n",
            "[epoch 12] loss: 0.6871, acc: 0.6994\n",
            "[epoch 12] loss: 0.6868, acc: 0.6986\n",
            "[epoch 12] loss: 0.6876, acc: 0.6985\n",
            "[epoch 12] loss: 0.6873, acc: 0.6988\n",
            "[epoch 12] loss: 0.6872, acc: 0.6984\n",
            "[epoch 12] loss: 0.6879, acc: 0.6986\n",
            "[epoch 12] loss: 0.6883, acc: 0.6976\n",
            "[epoch 12] loss: 0.6864, acc: 0.6983\n",
            "[epoch 12] loss: 0.6877, acc: 0.6980\n",
            "[epoch 12] loss: 0.6881, acc: 0.6979\n",
            "[epoch 12] loss: 0.6886, acc: 0.6973\n",
            "[epoch 12] loss: 0.6883, acc: 0.6985\n",
            "> val_acc: 0.6609, val_f1: 0.6609\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6609\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 13\n",
            "[epoch 13] loss: 0.7038, acc: 0.6800\n",
            "[epoch 13] loss: 0.7130, acc: 0.6900\n",
            "[epoch 13] loss: 0.6896, acc: 0.7092\n",
            "[epoch 13] loss: 0.6994, acc: 0.6967\n",
            "[epoch 13] loss: 0.6923, acc: 0.6974\n",
            "[epoch 13] loss: 0.6872, acc: 0.7021\n",
            "[epoch 13] loss: 0.6881, acc: 0.6939\n",
            "[epoch 13] loss: 0.6877, acc: 0.6989\n",
            "[epoch 13] loss: 0.6819, acc: 0.7033\n",
            "[epoch 13] loss: 0.6818, acc: 0.6996\n",
            "[epoch 13] loss: 0.6883, acc: 0.6992\n",
            "[epoch 13] loss: 0.6883, acc: 0.6993\n",
            "[epoch 13] loss: 0.6858, acc: 0.7010\n",
            "[epoch 13] loss: 0.6859, acc: 0.7035\n",
            "[epoch 13] loss: 0.6826, acc: 0.7025\n",
            "[epoch 13] loss: 0.6801, acc: 0.7044\n",
            "[epoch 13] loss: 0.6762, acc: 0.7053\n",
            "[epoch 13] loss: 0.6783, acc: 0.7036\n",
            "[epoch 13] loss: 0.6776, acc: 0.7039\n",
            "[epoch 13] loss: 0.6789, acc: 0.7020\n",
            "[epoch 13] loss: 0.6805, acc: 0.7002\n",
            "[epoch 13] loss: 0.6808, acc: 0.6989\n",
            "[epoch 13] loss: 0.6853, acc: 0.6968\n",
            "[epoch 13] loss: 0.6825, acc: 0.6975\n",
            "[epoch 13] loss: 0.6819, acc: 0.6984\n",
            "[epoch 13] loss: 0.6826, acc: 0.6991\n",
            "[epoch 13] loss: 0.6805, acc: 0.7006\n",
            "[epoch 13] loss: 0.6781, acc: 0.7017\n",
            "[epoch 13] loss: 0.6774, acc: 0.7022\n",
            "[epoch 13] loss: 0.6778, acc: 0.7018\n",
            "[epoch 13] loss: 0.6799, acc: 0.7012\n",
            "[epoch 13] loss: 0.6812, acc: 0.7014\n",
            "[epoch 13] loss: 0.6803, acc: 0.7020\n",
            "[epoch 13] loss: 0.6824, acc: 0.7011\n",
            "[epoch 13] loss: 0.6818, acc: 0.7017\n",
            "[epoch 13] loss: 0.6810, acc: 0.7030\n",
            "[epoch 13] loss: 0.6826, acc: 0.7023\n",
            "[epoch 13] loss: 0.6839, acc: 0.7014\n",
            "[epoch 13] loss: 0.6829, acc: 0.7012\n",
            "[epoch 13] loss: 0.6830, acc: 0.7010\n",
            "[epoch 13] loss: 0.6819, acc: 0.7012\n",
            "[epoch 13] loss: 0.6834, acc: 0.6997\n",
            "[epoch 13] loss: 0.6821, acc: 0.7008\n",
            "[epoch 13] loss: 0.6824, acc: 0.6998\n",
            "[epoch 13] loss: 0.6820, acc: 0.7004\n",
            "> val_acc: 0.6624, val_f1: 0.6624\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 14\n",
            "[epoch 14] loss: 0.7434, acc: 0.6600\n",
            "[epoch 14] loss: 0.7107, acc: 0.7044\n",
            "[epoch 14] loss: 0.6918, acc: 0.7114\n",
            "[epoch 14] loss: 0.6892, acc: 0.7063\n",
            "[epoch 14] loss: 0.6886, acc: 0.7050\n",
            "[epoch 14] loss: 0.6863, acc: 0.7007\n",
            "[epoch 14] loss: 0.6806, acc: 0.7024\n",
            "[epoch 14] loss: 0.6761, acc: 0.7026\n",
            "[epoch 14] loss: 0.6703, acc: 0.7105\n",
            "[epoch 14] loss: 0.6627, acc: 0.7159\n",
            "[epoch 14] loss: 0.6636, acc: 0.7174\n",
            "[epoch 14] loss: 0.6614, acc: 0.7186\n",
            "[epoch 14] loss: 0.6622, acc: 0.7172\n",
            "[epoch 14] loss: 0.6611, acc: 0.7177\n",
            "[epoch 14] loss: 0.6617, acc: 0.7176\n",
            "[epoch 14] loss: 0.6604, acc: 0.7182\n",
            "[epoch 14] loss: 0.6616, acc: 0.7162\n",
            "[epoch 14] loss: 0.6608, acc: 0.7166\n",
            "[epoch 14] loss: 0.6655, acc: 0.7130\n",
            "[epoch 14] loss: 0.6634, acc: 0.7137\n",
            "[epoch 14] loss: 0.6653, acc: 0.7117\n",
            "[epoch 14] loss: 0.6677, acc: 0.7106\n",
            "[epoch 14] loss: 0.6697, acc: 0.7104\n",
            "[epoch 14] loss: 0.6691, acc: 0.7118\n",
            "[epoch 14] loss: 0.6710, acc: 0.7095\n",
            "[epoch 14] loss: 0.6714, acc: 0.7104\n",
            "[epoch 14] loss: 0.6725, acc: 0.7099\n",
            "[epoch 14] loss: 0.6763, acc: 0.7085\n",
            "[epoch 14] loss: 0.6782, acc: 0.7076\n",
            "[epoch 14] loss: 0.6783, acc: 0.7075\n",
            "[epoch 14] loss: 0.6785, acc: 0.7069\n",
            "[epoch 14] loss: 0.6778, acc: 0.7074\n",
            "[epoch 14] loss: 0.6761, acc: 0.7076\n",
            "[epoch 14] loss: 0.6774, acc: 0.7066\n",
            "[epoch 14] loss: 0.6785, acc: 0.7056\n",
            "[epoch 14] loss: 0.6789, acc: 0.7044\n",
            "[epoch 14] loss: 0.6786, acc: 0.7041\n",
            "[epoch 14] loss: 0.6784, acc: 0.7046\n",
            "[epoch 14] loss: 0.6775, acc: 0.7049\n",
            "[epoch 14] loss: 0.6773, acc: 0.7046\n",
            "[epoch 14] loss: 0.6769, acc: 0.7054\n",
            "[epoch 14] loss: 0.6773, acc: 0.7051\n",
            "[epoch 14] loss: 0.6768, acc: 0.7052\n",
            "[epoch 14] loss: 0.6755, acc: 0.7058\n",
            "[epoch 14] loss: 0.6752, acc: 0.7062\n",
            "> val_acc: 0.6677, val_f1: 0.6677\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 15\n",
            "[epoch 15] loss: 0.5962, acc: 0.7400\n",
            "[epoch 15] loss: 0.6442, acc: 0.7100\n",
            "[epoch 15] loss: 0.6338, acc: 0.7267\n",
            "[epoch 15] loss: 0.6354, acc: 0.7260\n",
            "[epoch 15] loss: 0.6395, acc: 0.7232\n",
            "[epoch 15] loss: 0.6463, acc: 0.7147\n",
            "[epoch 15] loss: 0.6466, acc: 0.7143\n",
            "[epoch 15] loss: 0.6513, acc: 0.7115\n",
            "[epoch 15] loss: 0.6539, acc: 0.7102\n",
            "[epoch 15] loss: 0.6564, acc: 0.7088\n",
            "[epoch 15] loss: 0.6635, acc: 0.7044\n",
            "[epoch 15] loss: 0.6569, acc: 0.7093\n",
            "[epoch 15] loss: 0.6540, acc: 0.7117\n",
            "[epoch 15] loss: 0.6532, acc: 0.7126\n",
            "[epoch 15] loss: 0.6524, acc: 0.7125\n",
            "[epoch 15] loss: 0.6592, acc: 0.7083\n",
            "[epoch 15] loss: 0.6606, acc: 0.7096\n",
            "[epoch 15] loss: 0.6645, acc: 0.7076\n",
            "[epoch 15] loss: 0.6698, acc: 0.7074\n",
            "[epoch 15] loss: 0.6736, acc: 0.7066\n",
            "[epoch 15] loss: 0.6722, acc: 0.7074\n",
            "[epoch 15] loss: 0.6727, acc: 0.7060\n",
            "[epoch 15] loss: 0.6705, acc: 0.7078\n",
            "[epoch 15] loss: 0.6722, acc: 0.7067\n",
            "[epoch 15] loss: 0.6732, acc: 0.7061\n",
            "[epoch 15] loss: 0.6724, acc: 0.7068\n",
            "[epoch 15] loss: 0.6706, acc: 0.7080\n",
            "[epoch 15] loss: 0.6684, acc: 0.7084\n",
            "[epoch 15] loss: 0.6693, acc: 0.7081\n",
            "[epoch 15] loss: 0.6672, acc: 0.7097\n",
            "[epoch 15] loss: 0.6671, acc: 0.7102\n",
            "[epoch 15] loss: 0.6663, acc: 0.7104\n",
            "[epoch 15] loss: 0.6659, acc: 0.7115\n",
            "[epoch 15] loss: 0.6650, acc: 0.7125\n",
            "[epoch 15] loss: 0.6663, acc: 0.7117\n",
            "[epoch 15] loss: 0.6684, acc: 0.7109\n",
            "[epoch 15] loss: 0.6681, acc: 0.7109\n",
            "[epoch 15] loss: 0.6686, acc: 0.7104\n",
            "[epoch 15] loss: 0.6684, acc: 0.7102\n",
            "[epoch 15] loss: 0.6684, acc: 0.7096\n",
            "[epoch 15] loss: 0.6686, acc: 0.7097\n",
            "[epoch 15] loss: 0.6692, acc: 0.7089\n",
            "[epoch 15] loss: 0.6691, acc: 0.7083\n",
            "[epoch 15] loss: 0.6684, acc: 0.7087\n",
            "> val_acc: 0.6572, val_f1: 0.6572\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 16\n",
            "[epoch 16] loss: 0.6186, acc: 0.7600\n",
            "[epoch 16] loss: 0.5889, acc: 0.7600\n",
            "[epoch 16] loss: 0.6154, acc: 0.7436\n",
            "[epoch 16] loss: 0.6268, acc: 0.7338\n",
            "[epoch 16] loss: 0.6418, acc: 0.7229\n",
            "[epoch 16] loss: 0.6454, acc: 0.7238\n",
            "[epoch 16] loss: 0.6370, acc: 0.7277\n",
            "[epoch 16] loss: 0.6496, acc: 0.7228\n",
            "[epoch 16] loss: 0.6422, acc: 0.7259\n",
            "[epoch 16] loss: 0.6424, acc: 0.7239\n",
            "[epoch 16] loss: 0.6421, acc: 0.7251\n",
            "[epoch 16] loss: 0.6355, acc: 0.7293\n",
            "[epoch 16] loss: 0.6390, acc: 0.7282\n",
            "[epoch 16] loss: 0.6368, acc: 0.7267\n",
            "[epoch 16] loss: 0.6358, acc: 0.7285\n",
            "[epoch 16] loss: 0.6364, acc: 0.7287\n",
            "[epoch 16] loss: 0.6430, acc: 0.7249\n",
            "[epoch 16] loss: 0.6416, acc: 0.7263\n",
            "[epoch 16] loss: 0.6380, acc: 0.7275\n",
            "[epoch 16] loss: 0.6401, acc: 0.7258\n",
            "[epoch 16] loss: 0.6402, acc: 0.7250\n",
            "[epoch 16] loss: 0.6398, acc: 0.7243\n",
            "[epoch 16] loss: 0.6413, acc: 0.7229\n",
            "[epoch 16] loss: 0.6434, acc: 0.7217\n",
            "[epoch 16] loss: 0.6449, acc: 0.7203\n",
            "[epoch 16] loss: 0.6484, acc: 0.7195\n",
            "[epoch 16] loss: 0.6484, acc: 0.7182\n",
            "[epoch 16] loss: 0.6496, acc: 0.7174\n",
            "[epoch 16] loss: 0.6496, acc: 0.7174\n",
            "[epoch 16] loss: 0.6511, acc: 0.7162\n",
            "[epoch 16] loss: 0.6505, acc: 0.7168\n",
            "[epoch 16] loss: 0.6515, acc: 0.7173\n",
            "[epoch 16] loss: 0.6537, acc: 0.7160\n",
            "[epoch 16] loss: 0.6551, acc: 0.7142\n",
            "[epoch 16] loss: 0.6578, acc: 0.7131\n",
            "[epoch 16] loss: 0.6589, acc: 0.7120\n",
            "[epoch 16] loss: 0.6592, acc: 0.7119\n",
            "[epoch 16] loss: 0.6586, acc: 0.7124\n",
            "[epoch 16] loss: 0.6589, acc: 0.7125\n",
            "[epoch 16] loss: 0.6598, acc: 0.7124\n",
            "[epoch 16] loss: 0.6596, acc: 0.7131\n",
            "[epoch 16] loss: 0.6606, acc: 0.7143\n",
            "[epoch 16] loss: 0.6599, acc: 0.7146\n",
            "[epoch 16] loss: 0.6606, acc: 0.7138\n",
            "[epoch 16] loss: 0.6603, acc: 0.7145\n",
            "> val_acc: 0.6549, val_f1: 0.6549\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 17\n",
            "[epoch 17] loss: 0.5764, acc: 0.7600\n",
            "[epoch 17] loss: 0.6548, acc: 0.7057\n",
            "[epoch 17] loss: 0.6664, acc: 0.6950\n",
            "[epoch 17] loss: 0.6769, acc: 0.6918\n",
            "[epoch 17] loss: 0.6749, acc: 0.6891\n",
            "[epoch 17] loss: 0.6708, acc: 0.6978\n",
            "[epoch 17] loss: 0.6719, acc: 0.6963\n",
            "[epoch 17] loss: 0.6839, acc: 0.6903\n",
            "[epoch 17] loss: 0.6770, acc: 0.6967\n",
            "[epoch 17] loss: 0.6804, acc: 0.6957\n",
            "[epoch 17] loss: 0.6750, acc: 0.7012\n",
            "[epoch 17] loss: 0.6722, acc: 0.7032\n",
            "[epoch 17] loss: 0.6734, acc: 0.7035\n",
            "[epoch 17] loss: 0.6754, acc: 0.7036\n",
            "[epoch 17] loss: 0.6738, acc: 0.7047\n",
            "[epoch 17] loss: 0.6729, acc: 0.7034\n",
            "[epoch 17] loss: 0.6730, acc: 0.7020\n",
            "[epoch 17] loss: 0.6733, acc: 0.7032\n",
            "[epoch 17] loss: 0.6711, acc: 0.7041\n",
            "[epoch 17] loss: 0.6726, acc: 0.7033\n",
            "[epoch 17] loss: 0.6706, acc: 0.7055\n",
            "[epoch 17] loss: 0.6700, acc: 0.7062\n",
            "[epoch 17] loss: 0.6688, acc: 0.7077\n",
            "[epoch 17] loss: 0.6666, acc: 0.7091\n",
            "[epoch 17] loss: 0.6660, acc: 0.7093\n",
            "[epoch 17] loss: 0.6632, acc: 0.7110\n",
            "[epoch 17] loss: 0.6620, acc: 0.7108\n",
            "[epoch 17] loss: 0.6610, acc: 0.7128\n",
            "[epoch 17] loss: 0.6570, acc: 0.7154\n",
            "[epoch 17] loss: 0.6571, acc: 0.7156\n",
            "[epoch 17] loss: 0.6574, acc: 0.7162\n",
            "[epoch 17] loss: 0.6570, acc: 0.7157\n",
            "[epoch 17] loss: 0.6558, acc: 0.7169\n",
            "[epoch 17] loss: 0.6556, acc: 0.7170\n",
            "[epoch 17] loss: 0.6552, acc: 0.7165\n",
            "[epoch 17] loss: 0.6555, acc: 0.7169\n",
            "[epoch 17] loss: 0.6557, acc: 0.7169\n",
            "[epoch 17] loss: 0.6556, acc: 0.7174\n",
            "[epoch 17] loss: 0.6546, acc: 0.7180\n",
            "[epoch 17] loss: 0.6534, acc: 0.7190\n",
            "[epoch 17] loss: 0.6538, acc: 0.7185\n",
            "[epoch 17] loss: 0.6548, acc: 0.7171\n",
            "[epoch 17] loss: 0.6544, acc: 0.7175\n",
            "[epoch 17] loss: 0.6542, acc: 0.7174\n",
            "[epoch 17] loss: 0.6538, acc: 0.7177\n",
            "> val_acc: 0.6714, val_f1: 0.6714\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6714\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 18\n",
            "[epoch 18] loss: 0.7108, acc: 0.6933\n",
            "[epoch 18] loss: 0.6562, acc: 0.7050\n",
            "[epoch 18] loss: 0.6660, acc: 0.7062\n",
            "[epoch 18] loss: 0.6796, acc: 0.7056\n",
            "[epoch 18] loss: 0.6460, acc: 0.7252\n",
            "[epoch 18] loss: 0.6450, acc: 0.7229\n",
            "[epoch 18] loss: 0.6411, acc: 0.7236\n",
            "[epoch 18] loss: 0.6435, acc: 0.7216\n",
            "[epoch 18] loss: 0.6434, acc: 0.7181\n",
            "[epoch 18] loss: 0.6334, acc: 0.7233\n",
            "[epoch 18] loss: 0.6377, acc: 0.7242\n",
            "[epoch 18] loss: 0.6385, acc: 0.7234\n",
            "[epoch 18] loss: 0.6409, acc: 0.7241\n",
            "[epoch 18] loss: 0.6406, acc: 0.7238\n",
            "[epoch 18] loss: 0.6416, acc: 0.7230\n",
            "[epoch 18] loss: 0.6380, acc: 0.7246\n",
            "[epoch 18] loss: 0.6396, acc: 0.7236\n",
            "[epoch 18] loss: 0.6395, acc: 0.7236\n",
            "[epoch 18] loss: 0.6378, acc: 0.7247\n",
            "[epoch 18] loss: 0.6357, acc: 0.7261\n",
            "[epoch 18] loss: 0.6343, acc: 0.7264\n",
            "[epoch 18] loss: 0.6347, acc: 0.7261\n",
            "[epoch 18] loss: 0.6351, acc: 0.7258\n",
            "[epoch 18] loss: 0.6337, acc: 0.7271\n",
            "[epoch 18] loss: 0.6342, acc: 0.7267\n",
            "[epoch 18] loss: 0.6328, acc: 0.7281\n",
            "[epoch 18] loss: 0.6341, acc: 0.7287\n",
            "[epoch 18] loss: 0.6351, acc: 0.7288\n",
            "[epoch 18] loss: 0.6362, acc: 0.7284\n",
            "[epoch 18] loss: 0.6385, acc: 0.7268\n",
            "[epoch 18] loss: 0.6418, acc: 0.7244\n",
            "[epoch 18] loss: 0.6432, acc: 0.7228\n",
            "[epoch 18] loss: 0.6418, acc: 0.7231\n",
            "[epoch 18] loss: 0.6426, acc: 0.7220\n",
            "[epoch 18] loss: 0.6445, acc: 0.7207\n",
            "[epoch 18] loss: 0.6459, acc: 0.7197\n",
            "[epoch 18] loss: 0.6449, acc: 0.7205\n",
            "[epoch 18] loss: 0.6431, acc: 0.7217\n",
            "[epoch 18] loss: 0.6447, acc: 0.7208\n",
            "[epoch 18] loss: 0.6443, acc: 0.7212\n",
            "[epoch 18] loss: 0.6472, acc: 0.7193\n",
            "[epoch 18] loss: 0.6474, acc: 0.7188\n",
            "[epoch 18] loss: 0.6484, acc: 0.7185\n",
            "[epoch 18] loss: 0.6474, acc: 0.7195\n",
            "[epoch 18] loss: 0.6482, acc: 0.7193\n",
            "> val_acc: 0.6632, val_f1: 0.6632\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 19\n",
            "[epoch 19] loss: 0.6167, acc: 0.7550\n",
            "[epoch 19] loss: 0.5936, acc: 0.7578\n",
            "[epoch 19] loss: 0.6077, acc: 0.7471\n",
            "[epoch 19] loss: 0.6197, acc: 0.7432\n",
            "[epoch 19] loss: 0.6390, acc: 0.7342\n",
            "[epoch 19] loss: 0.6404, acc: 0.7324\n",
            "[epoch 19] loss: 0.6547, acc: 0.7241\n",
            "[epoch 19] loss: 0.6462, acc: 0.7282\n",
            "[epoch 19] loss: 0.6478, acc: 0.7286\n",
            "[epoch 19] loss: 0.6471, acc: 0.7261\n",
            "[epoch 19] loss: 0.6528, acc: 0.7204\n",
            "[epoch 19] loss: 0.6500, acc: 0.7210\n",
            "[epoch 19] loss: 0.6512, acc: 0.7181\n",
            "[epoch 19] loss: 0.6516, acc: 0.7171\n",
            "[epoch 19] loss: 0.6513, acc: 0.7197\n",
            "[epoch 19] loss: 0.6516, acc: 0.7185\n",
            "[epoch 19] loss: 0.6498, acc: 0.7183\n",
            "[epoch 19] loss: 0.6566, acc: 0.7148\n",
            "[epoch 19] loss: 0.6533, acc: 0.7170\n",
            "[epoch 19] loss: 0.6502, acc: 0.7184\n",
            "[epoch 19] loss: 0.6471, acc: 0.7187\n",
            "[epoch 19] loss: 0.6460, acc: 0.7180\n",
            "[epoch 19] loss: 0.6450, acc: 0.7195\n",
            "[epoch 19] loss: 0.6458, acc: 0.7193\n",
            "[epoch 19] loss: 0.6434, acc: 0.7215\n",
            "[epoch 19] loss: 0.6448, acc: 0.7209\n",
            "[epoch 19] loss: 0.6436, acc: 0.7222\n",
            "[epoch 19] loss: 0.6439, acc: 0.7235\n",
            "[epoch 19] loss: 0.6451, acc: 0.7219\n",
            "[epoch 19] loss: 0.6449, acc: 0.7224\n",
            "[epoch 19] loss: 0.6451, acc: 0.7218\n",
            "[epoch 19] loss: 0.6441, acc: 0.7218\n",
            "[epoch 19] loss: 0.6423, acc: 0.7224\n",
            "[epoch 19] loss: 0.6419, acc: 0.7220\n",
            "[epoch 19] loss: 0.6410, acc: 0.7231\n",
            "[epoch 19] loss: 0.6408, acc: 0.7232\n",
            "[epoch 19] loss: 0.6401, acc: 0.7237\n",
            "[epoch 19] loss: 0.6399, acc: 0.7242\n",
            "[epoch 19] loss: 0.6408, acc: 0.7237\n",
            "[epoch 19] loss: 0.6393, acc: 0.7251\n",
            "[epoch 19] loss: 0.6395, acc: 0.7255\n",
            "[epoch 19] loss: 0.6399, acc: 0.7247\n",
            "[epoch 19] loss: 0.6400, acc: 0.7250\n",
            "[epoch 19] loss: 0.6395, acc: 0.7252\n",
            "[epoch 19] loss: 0.6423, acc: 0.7239\n",
            "> val_acc: 0.6587, val_f1: 0.6587\n",
            ">> test_acc: 0.6714\n",
            ">> test_f1_macro: 0.6578\n",
            ">> test_f1_micro: 0.6714\n",
            ">> test_f1_weighted: 0.6705\n",
            ">> test_precision_macro: 0.6625\n",
            ">> test_precision_micro: 0.6714\n",
            ">> test_precision_weighted: 0.6705\n",
            ">> test_recall_macro: 0.6543\n",
            ">> test_recall_micro: 0.6714\n",
            ">> test_recall_weighted: 0.6714\n",
            "confusion matrix:\n",
            "[[204  68  57]\n",
            " [ 43 457 107]\n",
            " [ 54 110 236]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}