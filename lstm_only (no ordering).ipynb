{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **SETUP**"],"metadata":{"id":"5zXJrSMo495u"}},{"cell_type":"markdown","source":["### **Imports**"],"metadata":{"id":"8_7Yx7y-5BOG"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tYF42jlo5Q4i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666458011500,"user_tz":-480,"elapsed":2891,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}},"outputId":"356b60f9-c4f6-46d9-addf-25ee050241ab"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import math\n","import argparse\n","import sys\n","import random\n","import pickle\n","import spacy\n","import numpy as np"],"metadata":{"id":"eTERBhPf5Ykk","executionInfo":{"status":"ok","timestamp":1666458011501,"user_tz":-480,"elapsed":25,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["from time import strftime, localtime\n","from sklearn import metrics\n","from xml.etree.ElementTree import parse\n","from spacy.tokens import Doc"],"metadata":{"id":"ZEPQFq-e6gVp","executionInfo":{"status":"ok","timestamp":1666458011502,"user_tz":-480,"elapsed":22,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split"],"metadata":{"id":"6674vF4n6le1","executionInfo":{"status":"ok","timestamp":1666458011502,"user_tz":-480,"elapsed":21,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":["# **CLASSES**"],"metadata":{"id":"IDenOuxLpJFY"}},{"cell_type":"markdown","source":["### **Data**"],"metadata":{"id":"USX4iChw67d2"}},{"cell_type":"code","source":["def pad_and_truncate(sequence, max_len, dtype='int64', padding='post', truncating='post', value=0):\n","    x = (np.ones(max_len) * value).astype(dtype)\n","    if truncating == 'pre':\n","        trunc = sequence[-max_len:]\n","    else:\n","        trunc = sequence[:max_len]\n","    trunc = np.asarray(trunc, dtype=dtype)\n","    if padding == 'post':\n","        x[:len(trunc)] = trunc\n","    else:\n","        x[-len(trunc):] = trunc\n","    return x"],"metadata":{"id":"L5p9tWvxkY2N","executionInfo":{"status":"ok","timestamp":1666458011503,"user_tz":-480,"elapsed":20,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["class Tokenizer():\n","    def __init__(self, max_len, lower=True):\n","        self.lower = lower\n","        self.max_len = max_len\n","        self.word_to_index = {}\n","        self.index_to_word = {}\n","        self.idx = 1\n","\n","    def fit_on_text(self, text):\n","        if self.lower:\n","            text = text.lower()\n","        words = text.split()\n","        for word in words:\n","            if word not in self.word_to_index:\n","                self.word_to_index[word] = self.idx\n","                self.index_to_word[self.idx] = word\n","                self.idx += 1\n","\n","    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n","        if self.lower:\n","            text = text.lower()\n","        words = text.split()\n","        unk = len(self.word_to_index)+1\n","        sequence = [self.word_to_index[w] if w in self.word_to_index else unk for w in words]\n","        if len(sequence) == 0:\n","            sequence = [0]\n","        if reverse:\n","            sequence = sequence[::-1]\n","        return pad_and_truncate(sequence, self.max_len, padding=padding, truncating=truncating)"],"metadata":{"id":"4y8GnOALjcjf","executionInfo":{"status":"ok","timestamp":1666458011503,"user_tz":-480,"elapsed":19,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["class ABSADataset(Dataset):\n","    def __init__(self, file_name, tokenizer):\n","        with open(file_name, 'r', newline='\\n', encoding='utf-8', errors='ignore') as f:\n","            lines = f.readlines()\n","        with open(file_name + '.graph', 'rb') as f:\n","            index_to_graph = pickle.load(f)\n","        \n","        self.data = []\n","\n","        # Create tokens with tokenizer\n","        for i in range(0, len(lines), 3):\n","            left, right = [s.lower().strip() for s in lines[i].split(\"$T$\")]\n","            aspect = lines[i+1].lower().strip()\n","            polarity = lines[i+2].strip()\n","\n","            text_token = tokenizer.text_to_sequence(\" \".join([left, aspect, right]))\n","            context_token = tokenizer.text_to_sequence(\" \".join([left, right]))\n","\n","            left_token = tokenizer.text_to_sequence(left)\n","            left_aspect_token = tokenizer.text_to_sequence(\" \".join([left, aspect]))\n","\n","            right_token = tokenizer.text_to_sequence(right, reverse=True)\n","            right_aspect_token = tokenizer.text_to_sequence(\" \".join([aspect, right]), reverse=True)\n","\n","            aspect_token = tokenizer.text_to_sequence(aspect)\n","\n","            left_len = np.sum(left_token != 0)\n","            aspect_len = np.sum(aspect_token != 0)\n","            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype = np.int64)\n","            polarity = int(polarity) + 1\n","            \n","            text_len = np.sum(text_token != 0)\n","            concat_segments_tokens = pad_and_truncate([0] * (text_len + 1) + [1] * (aspect_len + 1), tokenizer.max_len)\n","            \n","            dependency_graph = np.pad(index_to_graph[i],\n","                                      ((0, tokenizer.max_len-index_to_graph[i].shape[0]),\n","                                       (0, tokenizer.max_len-index_to_graph[i].shape[0])),\n","                                      'constant')\n","            \n","            self.data.append({\n","                'text_token' : text_token,\n","                'context_token' : context_token, \n","                'left_token' : left_token, \n","                'left_aspect_token' : left_aspect_token,\n","                'right_token' : right_token,\n","                'right_aspect_token' : right_aspect_token,\n","                'aspect_token' : aspect_token, \n","                'aspect_boundary' : aspect_boundary, \n","                'polarity' : polarity, \n","                'dependency_graph' : dependency_graph\n","            })\n","    \n","    def __getitem__(self, index):\n","        return self.data[index]\n","    \n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"Q5FagHQ9cZGs","executionInfo":{"status":"ok","timestamp":1666458011504,"user_tz":-480,"elapsed":19,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["class WhitespaceTokenizer(object):\n","    def __init__(self, vocab):\n","        self.vocab = vocab\n","\n","    def __call__(self, text):\n","        words = text.split()\n","        # All tokens 'own' a subsequent space character in this tokenizer\n","        spaces = [True] * len(words)\n","        return Doc(self.vocab, words=words, spaces=spaces)"],"metadata":{"id":"gjHh_czJ0Rbn","executionInfo":{"status":"ok","timestamp":1666458011504,"user_tz":-480,"elapsed":18,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":["### **Model**"],"metadata":{"id":"fuiAR0ng6vID"}},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(LSTM, self).__init__()\n","        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n","        self.squeeze_embedding = SqueezeEmbedding()\n","        self.lstm = nn.LSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=1, batch_first=True)\n","        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n","    \n","    def forward(self, inputs):\n","        text_indices, aspect_indices = inputs[0], inputs[1]\n","        x_len = torch.sum(text_indices != 0, dim=-1)\n","        x_len_max = torch.max(x_len)\n","        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n","\n","        x = self.embed(text_indices)\n","        x = self.squeeze_embedding(x, x_len)\n","        aspect = self.embed(aspect_indices)\n","        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n","        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n","        x = torch.cat((aspect, x), dim=-1)\n","\n","        # x_sort_idx = torch.sort(-x_len)[1].long()\n","        # x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n","        # x_len = x_len[x_sort_idx]\n","        # x = x[x_sort_idx]\n","\n","        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n","\n","        # global ht\n","        out_pack, (ht, ct) = self.lstm(x_emb_p, None)\n","        # print(out_pack)\n","        # print(out_pack.shape)\n","\n","        # ht = torch.transpose(ht, 0, 1)[\n","        #     x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n","        # ht = torch.transpose(ht, 0, 1)\n","\n","        # \"\"\"unpack: out\"\"\"\n","        # out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=True)  # (sequence, lengths)\n","        # out = out[0]  #\n","        # # global out\n","        # out = out[x_unsort_idx]\n","\n","        return self.dense(ht[0])"],"metadata":{"id":"xPQIKGdi0qXg","executionInfo":{"status":"ok","timestamp":1666458011504,"user_tz":-480,"elapsed":17,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["class SqueezeEmbedding(nn.Module):\n","    \"\"\"\n","    Squeeze sequence embedding length to the longest one in the batch\n","    \"\"\"\n","    def __init__(self, batch_first=True):\n","        super(SqueezeEmbedding, self).__init__()\n","        self.batch_first = batch_first\n","\n","    def forward(self, x, x_len):\n","        \"\"\"\n","        sequence -> sort -> pad and pack -> unpack ->unsort\n","        :param x: sequence embedding vectors\n","        :param x_len: numpy/tensor list\n","        :return:\n","        \"\"\"\n","        \"\"\"sort\"\"\"\n","        x_sort_idx = torch.sort(-x_len)[1].long()\n","        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n","        x_len = x_len[x_sort_idx]\n","        x = x[x_sort_idx]\n","        \"\"\"pack\"\"\"\n","        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n","        \"\"\"unpack: out\"\"\"\n","        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n","        out = out[0]  #\n","        \"\"\"unsort\"\"\"\n","        out = out[x_unsort_idx]\n","        return out"],"metadata":{"id":"tFr-dJ6B1E55","executionInfo":{"status":"ok","timestamp":1666458011505,"user_tz":-480,"elapsed":17,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":["### **Main**"],"metadata":{"id":"6BmgF1FSp32r"}},{"cell_type":"code","source":["class Instructor:\n","    def __init__(self):\n","        tokenizer = build_tokenizer(\n","            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n","            max_seq_len=config2[\"max_seq_len\"],\n","            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n","        embedding_matrix = build_embedding_matrix(\n","            word2idx=tokenizer.word_to_index,\n","            embed_dim=config2[\"embed_dim\"],\n","            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n","        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n","\n","        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n","        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n","        assert 0 <= config2[\"valset_ratio\"] < 1\n","        if config2[\"valset_ratio\"] > 0:\n","            valset_len = int(len(self.trainset) * config2[\"valset_ratio\"])\n","            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n","        else:\n","            self.valset = self.testset\n","\n","        if config2[\"device\"].type == 'cuda':\n","            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n","        self._print_args()\n","\n","    def _print_args(self):\n","        n_trainable_params, n_nontrainable_params = 0, 0\n","        for p in self.model.parameters():\n","            n_params = torch.prod(torch.tensor(p.shape))\n","            if p.requires_grad:\n","                n_trainable_params += n_params\n","            else:\n","                n_nontrainable_params += n_params\n","        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n","\n","\n","    def _reset_params(self):\n","        for child in self.model.children():\n","            for p in child.parameters():\n","                if p.requires_grad:\n","                    if len(p.shape) > 1:\n","                        config2[\"initializer\"](p)\n","                    else:\n","                        stdv = 1. / math.sqrt(p.shape[0])\n","                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n","\n","    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n","        max_val_acc = 0\n","        max_val_f1 = 0\n","        max_val_epoch = 0\n","        global_step = 0\n","        path = None\n","        for i_epoch in range(config2[\"num_epoch\"]):\n","            print('>' * 100)\n","            print('epoch: {}'.format(i_epoch))\n","            n_correct, n_total, loss_total = 0, 0, 0\n","            # switch model to training mode\n","            self.model.train()\n","            for i_batch, batch in enumerate(train_data_loader):\n","                global_step += 1\n","                # clear gradient accumulators\n","                optimizer.zero_grad()\n","\n","                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n","                global outputs, targets\n","                outputs = self.model(inputs)\n","                targets = batch['polarity'].to(config2[\"device\"])\n","                # print(outputs)\n","                # print(targets)\n","                loss = criterion(outputs, targets)\n","                loss.backward()\n","                optimizer.step()\n","\n","                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n","                n_total += len(outputs)\n","                loss_total += loss.item() * len(outputs)\n","                if global_step % config2[\"log_step\"] == 0:\n","                    train_acc = n_correct / n_total\n","                    train_loss = loss_total / n_total\n","                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n","\n","            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n","            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n","            if val_acc > max_val_acc:\n","                max_val_acc = val_acc\n","                max_val_epoch = i_epoch\n","                if not os.path.exists('state_dict'):\n","                    os.mkdir('state_dict')\n","                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n","                torch.save(self.model.state_dict(), path)\n","                print('>> saved: {}'.format(path))\n","            if val_f1 > max_val_f1:\n","                max_val_f1 = val_f1\n","            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n","                print('>> early stop.')\n","                break\n","\n","        return path\n","\n","    def _evaluate_acc_f1(self, data_loader):\n","        n_correct, n_total = 0, 0\n","        t_targets_all, t_outputs_all = None, None\n","        # switch model to evaluation mode\n","        self.model.eval()\n","        with torch.no_grad():\n","            for i_batch, t_batch in enumerate(data_loader):\n","                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n","                t_targets = t_batch['polarity'].to(config2[\"device\"])\n","                t_outputs = self.model(t_inputs)\n","\n","                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n","                n_total += len(t_outputs)\n","\n","                if t_targets_all is None:\n","                    t_targets_all = t_targets\n","                    t_outputs_all = t_outputs\n","                else:\n","                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n","                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n","\n","        acc = n_correct / n_total\n","        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n","        return acc, f1\n","\n","    def run(self):\n","        # Loss and Optimizer\n","        criterion = nn.CrossEntropyLoss()\n","        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n","        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n","\n","        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n","        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n","        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n","\n","        self._reset_params()\n","        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n","        self.model.load_state_dict(torch.load(best_model_path))\n","        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n","        print('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"],"metadata":{"id":"mPRxQS-Jp5vP","executionInfo":{"status":"ok","timestamp":1666458011505,"user_tz":-480,"elapsed":15,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":["# **Functions**"],"metadata":{"id":"bv09Mo677CkE"}},{"cell_type":"code","source":["def change_format_sentences(source_path, target_path):\n","    f = open(target_path, \"w\")\n","\n","    sentences = parse(source_path).getroot()\n","    preprocessed = []\n","\n","    for sentence in sentences:\n","        text = sentence.find('text')\n","        if text is None:\n","            continue\n","        text = text.text\n","        aspectTerms = sentence.find('aspectTerms')\n","        if aspectTerms is None:\n","            continue\n","        for aspectTerm in aspectTerms:\n","            term = aspectTerm.get('term')\n","            polarity = aspectTerm.get('polarity')\n","            if polarity == \"positive\":\n","                polarity = '1'\n","            elif polarity == \"neutral\":\n","                polarity = '0'\n","            elif polarity == \"negative\":\n","                polarity = '-1'\n","            else:\n","                raise Exception(\"invalid polarity!\")\n","            start = int(aspectTerm.get('from'))\n","            end = int(aspectTerm.get('to'))\n","            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n","            preprocessed.append(text[start:end])\n","            preprocessed.append(polarity)\n","    f.write(\"\\n\".join(preprocessed))"],"metadata":{"id":"Mlhjev_wrkDF","executionInfo":{"status":"ok","timestamp":1666458011506,"user_tz":-480,"elapsed":15,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["def build_tokenizer(fnames, max_seq_len, dat_fname):\n","    if os.path.exists(dat_fname):\n","        print('loading tokenizer:', dat_fname)\n","        tokenizer = pickle.load(open(dat_fname, 'rb'))\n","    else:\n","        text = ''\n","        for fname in fnames:\n","            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","            lines = fin.readlines()\n","            fin.close()\n","            for i in range(0, len(lines), 3):\n","                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n","                aspect = lines[i + 1].lower().strip()\n","                text_raw = text_left + \" \" + aspect + \" \" + text_right\n","                text += text_raw + \" \"\n","\n","        tokenizer = Tokenizer(max_seq_len)\n","        tokenizer.fit_on_text(text)\n","        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n","    return tokenizer"],"metadata":{"id":"pV174mP37UZy","executionInfo":{"status":"ok","timestamp":1666458011506,"user_tz":-480,"elapsed":14,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["def _load_word_vec(path, word2idx=None, embed_dim=300):\n","    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    word_vec = {}\n","    for line in fin:\n","        tokens = line.rstrip().split()\n","        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n","        if word in word2idx.keys():\n","            word_vec[word] = np.asarray(vec, dtype='float32')\n","    return word_vec"],"metadata":{"id":"2h-2UjGs7fu5","executionInfo":{"status":"ok","timestamp":1666458012094,"user_tz":-480,"elapsed":601,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n","    if os.path.exists(dat_fname):\n","        print('loading embedding_matrix:', dat_fname)\n","        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n","    else:\n","        print('loading word vectors...')\n","        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n","        fname = config[\"glove_path\"]\n","        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n","        print('building embedding_matrix:', dat_fname)\n","        for word, i in word2idx.items():\n","            vec = word_vec.get(word)\n","            if vec is not None:\n","                # words not found in embedding index will be all-zeros.\n","                embedding_matrix[i] = vec\n","        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n","    return embedding_matrix"],"metadata":{"id":"nB3jz5Fk7dZd","executionInfo":{"status":"ok","timestamp":1666458012096,"user_tz":-480,"elapsed":12,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n","    x = (np.ones(maxlen) * value).astype(dtype)\n","    if truncating == 'pre':\n","        trunc = sequence[-maxlen:]\n","    else:\n","        trunc = sequence[:maxlen]\n","    trunc = np.asarray(trunc, dtype=dtype)\n","    if padding == 'post':\n","        x[:len(trunc)] = trunc\n","    else:\n","        x[-len(trunc):] = trunc\n","    return x"],"metadata":{"id":"7JH84-Kx7Y-_","executionInfo":{"status":"ok","timestamp":1666458012097,"user_tz":-480,"elapsed":12,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["def dependency_adj_matrix(text):\n","    # https://spacy.io/docs/usage/processing-text\n","    tokens = nlp(text)\n","    words = text.split()\n","    matrix = np.zeros((len(words), len(words))).astype('float32')\n","    assert len(words) == len(list(tokens))\n","\n","    for token in tokens:\n","        matrix[token.i][token.i] = 1\n","        for child in token.children:\n","            matrix[token.i][child.i] = 1\n","            matrix[child.i][token.i] = 1\n","\n","    return matrix"],"metadata":{"id":"VemwD4FB7sY1","executionInfo":{"status":"ok","timestamp":1666458012097,"user_tz":-480,"elapsed":11,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["def process_data():\n","    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n","    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n","    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n","    process(config[\"processed_train_path\"])\n","    process(config[\"processed_val_path\"])\n","    process(config[\"processed_test_path\"])"],"metadata":{"id":"vC45U4k21oPd","executionInfo":{"status":"ok","timestamp":1666458012098,"user_tz":-480,"elapsed":11,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["def process(filename):\n","    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    lines = fin.readlines()\n","    fin.close()\n","    idx2graph = {}\n","    fout = open(filename+'.graph', 'wb')\n","    for i in range(0, len(lines), 3):\n","        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n","        aspect = lines[i + 1].strip()\n","        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n","        idx2graph[i] = adj_matrix\n","    pickle.dump(idx2graph, fout)        \n","    fout.close() "],"metadata":{"id":"HlLsFiUJ7mPH","executionInfo":{"status":"ok","timestamp":1666458012098,"user_tz":-480,"elapsed":10,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["def run():\n","    ins = Instructor()\n","    ins.run()"],"metadata":{"id":"ANtAieVU8lwS","executionInfo":{"status":"ok","timestamp":1666458012099,"user_tz":-480,"elapsed":10,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":["# **MAIN**"],"metadata":{"id":"cxb8D05u8UN7"}},{"cell_type":"markdown","source":["### **Configuration**"],"metadata":{"id":"5QyUuFxj8XuA"}},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"],"metadata":{"id":"AF3DYep7AW8k","executionInfo":{"status":"ok","timestamp":1666458012704,"user_tz":-480,"elapsed":614,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["config = {\n","    \"base_path\": \"/content/drive/MyDrive/CS4248/MAMS-ATSA\",\n","    \"glove_path\": \"/content/drive/MyDrive/CS4248/glove.42B.300d.txt\"\n","}\n","\n","config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n","config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n","config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n","config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n","config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n","config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n","config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"],"metadata":{"id":"9y9MO-RW8bcS","executionInfo":{"status":"ok","timestamp":1666458012704,"user_tz":-480,"elapsed":10,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["dataset_files = {\n","    'train': config[\"processed_train_path\"],\n","    'test': config[\"processed_test_path\"]\n","}\n","\n","config2 = {\n","    \"model_name\" : \"LSTM\",\n","    \"lr\" : 2e-5,\n","    \"dropout\" : 0.1,\n","    \"l2reg\" : 0.01,\n","    \"num_epoch\" : 20,\n","    \"batch_size\" : 16,\n","    \"log_step\" : 10,\n","    \"embed_dim\" : 300,\n","    \"hidden_dim\" : 300,\n","    \"model_class\" : LSTM,\n","    \"dataset\": \"MAMS\",\n","    \"dataset_file\" : dataset_files,\n","    \"inputs_cols\" : ['text_token', 'aspect_token'],\n","    \"initializer\" : torch.nn.init.xavier_uniform_,\n","    \"optimizer\" : torch.optim.Adam,\n","    \"max_seq_len\" : 85,\n","    \"polarities_dim\" : 3,\n","    \"hops\" : 3,\n","    \"patience\" : 5,\n","    \"device\" : None,\n","    \"seed\" : 1234,\n","    \"valset_ratio\" : 0\n","}\n","\n","config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n","        if config2[\"device\"] is None else torch.device(config2[\"device\"])\n","\n","if config2[\"seed\"] is not None:\n","    random.seed(config2[\"seed\"])\n","    np.random.seed(config2[\"seed\"])\n","    torch.manual_seed(config2[\"seed\"])\n","    torch.cuda.manual_seed(config2[\"seed\"])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"],"metadata":{"id":"clOIqT523tWy","executionInfo":{"status":"ok","timestamp":1666458012705,"user_tz":-480,"elapsed":9,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":["### **Pipeline**"],"metadata":{"id":"3fT6RqeiAGmN"}},{"cell_type":"code","source":["#process_data()"],"metadata":{"id":"vT4xfKSC-Xpx","executionInfo":{"status":"ok","timestamp":1666458012705,"user_tz":-480,"elapsed":8,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":124,"outputs":[]},{"cell_type":"code","source":["run()"],"metadata":{"id":"8gEIkSn38pHt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"126eaf89-4e37-467d-d2df-55ee87c44e92","executionInfo":{"status":"ok","timestamp":1666460147343,"user_tz":-480,"elapsed":2134645,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["loading tokenizer: MAMS_tokenizer.dat\n","loading embedding_matrix: 300_MAMS_embedding_matrix.dat\n","> n_trainable_params: 1083303, n_nontrainable_params: 3994500\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 0\n","[epoch 0] loss: 1.1198, acc: 0.3937\n","[epoch 0] loss: 1.1109, acc: 0.3969\n","[epoch 0] loss: 1.0910, acc: 0.4229\n","[epoch 0] loss: 1.0810, acc: 0.4281\n","[epoch 0] loss: 1.0751, acc: 0.4325\n","[epoch 0] loss: 1.0657, acc: 0.4323\n","[epoch 0] loss: 1.0586, acc: 0.4384\n","[epoch 0] loss: 1.0559, acc: 0.4344\n","[epoch 0] loss: 1.0535, acc: 0.4333\n","[epoch 0] loss: 1.0495, acc: 0.4363\n","[epoch 0] loss: 1.0428, acc: 0.4466\n","[epoch 0] loss: 1.0398, acc: 0.4531\n","[epoch 0] loss: 1.0371, acc: 0.4562\n","[epoch 0] loss: 1.0340, acc: 0.4616\n","[epoch 0] loss: 1.0300, acc: 0.4646\n","[epoch 0] loss: 1.0280, acc: 0.4668\n","[epoch 0] loss: 1.0241, acc: 0.4735\n","[epoch 0] loss: 1.0208, acc: 0.4792\n","[epoch 0] loss: 1.0189, acc: 0.4839\n","[epoch 0] loss: 1.0169, acc: 0.4888\n","[epoch 0] loss: 1.0151, acc: 0.4911\n","[epoch 0] loss: 1.0110, acc: 0.4949\n","[epoch 0] loss: 1.0074, acc: 0.4981\n","[epoch 0] loss: 1.0047, acc: 0.5013\n","[epoch 0] loss: 1.0037, acc: 0.5028\n","[epoch 0] loss: 1.0031, acc: 0.5036\n","[epoch 0] loss: 1.0015, acc: 0.5069\n","[epoch 0] loss: 1.0001, acc: 0.5078\n","[epoch 0] loss: 0.9976, acc: 0.5110\n","[epoch 0] loss: 0.9949, acc: 0.5138\n","[epoch 0] loss: 0.9934, acc: 0.5151\n","[epoch 0] loss: 0.9903, acc: 0.5197\n","[epoch 0] loss: 0.9895, acc: 0.5195\n","[epoch 0] loss: 0.9880, acc: 0.5199\n","[epoch 0] loss: 0.9886, acc: 0.5196\n","[epoch 0] loss: 0.9877, acc: 0.5196\n","[epoch 0] loss: 0.9851, acc: 0.5220\n","[epoch 0] loss: 0.9842, acc: 0.5217\n","[epoch 0] loss: 0.9828, acc: 0.5231\n","[epoch 0] loss: 0.9805, acc: 0.5255\n","[epoch 0] loss: 0.9799, acc: 0.5259\n","[epoch 0] loss: 0.9776, acc: 0.5287\n","[epoch 0] loss: 0.9757, acc: 0.5302\n","[epoch 0] loss: 0.9735, acc: 0.5315\n","[epoch 0] loss: 0.9717, acc: 0.5331\n","[epoch 0] loss: 0.9706, acc: 0.5337\n","[epoch 0] loss: 0.9698, acc: 0.5344\n","[epoch 0] loss: 0.9684, acc: 0.5355\n","[epoch 0] loss: 0.9673, acc: 0.5356\n","[epoch 0] loss: 0.9667, acc: 0.5355\n","[epoch 0] loss: 0.9668, acc: 0.5343\n","[epoch 0] loss: 0.9661, acc: 0.5351\n","[epoch 0] loss: 0.9639, acc: 0.5360\n","[epoch 0] loss: 0.9633, acc: 0.5368\n","[epoch 0] loss: 0.9621, acc: 0.5375\n","[epoch 0] loss: 0.9620, acc: 0.5372\n","[epoch 0] loss: 0.9612, acc: 0.5379\n","[epoch 0] loss: 0.9605, acc: 0.5380\n","[epoch 0] loss: 0.9587, acc: 0.5396\n","[epoch 0] loss: 0.9571, acc: 0.5405\n","[epoch 0] loss: 0.9555, acc: 0.5416\n","[epoch 0] loss: 0.9553, acc: 0.5415\n","[epoch 0] loss: 0.9539, acc: 0.5424\n","[epoch 0] loss: 0.9523, acc: 0.5430\n","[epoch 0] loss: 0.9522, acc: 0.5432\n","[epoch 0] loss: 0.9512, acc: 0.5441\n","[epoch 0] loss: 0.9506, acc: 0.5451\n","[epoch 0] loss: 0.9498, acc: 0.5459\n","[epoch 0] loss: 0.9491, acc: 0.5459\n","[epoch 0] loss: 0.9485, acc: 0.5464\n","> val_acc: 0.5928, val_f1: 0.5430\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.5928\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 1\n","[epoch 1] loss: 0.8860, acc: 0.5938\n","[epoch 1] loss: 0.8738, acc: 0.6000\n","[epoch 1] loss: 0.8658, acc: 0.6083\n","[epoch 1] loss: 0.8505, acc: 0.6172\n","[epoch 1] loss: 0.8439, acc: 0.6088\n","[epoch 1] loss: 0.8677, acc: 0.5958\n","[epoch 1] loss: 0.8735, acc: 0.5893\n","[epoch 1] loss: 0.8826, acc: 0.5805\n","[epoch 1] loss: 0.8895, acc: 0.5764\n","[epoch 1] loss: 0.8960, acc: 0.5681\n","[epoch 1] loss: 0.8928, acc: 0.5744\n","[epoch 1] loss: 0.8893, acc: 0.5766\n","[epoch 1] loss: 0.8842, acc: 0.5841\n","[epoch 1] loss: 0.8822, acc: 0.5866\n","[epoch 1] loss: 0.8783, acc: 0.5904\n","[epoch 1] loss: 0.8798, acc: 0.5887\n","[epoch 1] loss: 0.8753, acc: 0.5904\n","[epoch 1] loss: 0.8752, acc: 0.5903\n","[epoch 1] loss: 0.8795, acc: 0.5859\n","[epoch 1] loss: 0.8794, acc: 0.5872\n","[epoch 1] loss: 0.8799, acc: 0.5884\n","[epoch 1] loss: 0.8800, acc: 0.5881\n","[epoch 1] loss: 0.8801, acc: 0.5899\n","[epoch 1] loss: 0.8780, acc: 0.5911\n","[epoch 1] loss: 0.8758, acc: 0.5938\n","[epoch 1] loss: 0.8753, acc: 0.5950\n","[epoch 1] loss: 0.8767, acc: 0.5944\n","[epoch 1] loss: 0.8767, acc: 0.5949\n","[epoch 1] loss: 0.8748, acc: 0.5972\n","[epoch 1] loss: 0.8733, acc: 0.5981\n","[epoch 1] loss: 0.8724, acc: 0.5994\n","[epoch 1] loss: 0.8739, acc: 0.5986\n","[epoch 1] loss: 0.8735, acc: 0.5991\n","[epoch 1] loss: 0.8716, acc: 0.5994\n","[epoch 1] loss: 0.8715, acc: 0.5986\n","[epoch 1] loss: 0.8708, acc: 0.5993\n","[epoch 1] loss: 0.8711, acc: 0.5992\n","[epoch 1] loss: 0.8696, acc: 0.6010\n","[epoch 1] loss: 0.8705, acc: 0.6000\n","[epoch 1] loss: 0.8707, acc: 0.5994\n","[epoch 1] loss: 0.8705, acc: 0.5994\n","[epoch 1] loss: 0.8695, acc: 0.6001\n","[epoch 1] loss: 0.8697, acc: 0.6006\n","[epoch 1] loss: 0.8690, acc: 0.6011\n","[epoch 1] loss: 0.8685, acc: 0.6015\n","[epoch 1] loss: 0.8691, acc: 0.6003\n","[epoch 1] loss: 0.8691, acc: 0.5991\n","[epoch 1] loss: 0.8683, acc: 0.5990\n","[epoch 1] loss: 0.8691, acc: 0.5974\n","[epoch 1] loss: 0.8687, acc: 0.5974\n","[epoch 1] loss: 0.8696, acc: 0.5967\n","[epoch 1] loss: 0.8703, acc: 0.5965\n","[epoch 1] loss: 0.8705, acc: 0.5959\n","[epoch 1] loss: 0.8705, acc: 0.5954\n","[epoch 1] loss: 0.8717, acc: 0.5940\n","[epoch 1] loss: 0.8714, acc: 0.5941\n","[epoch 1] loss: 0.8703, acc: 0.5950\n","[epoch 1] loss: 0.8713, acc: 0.5940\n","[epoch 1] loss: 0.8731, acc: 0.5929\n","[epoch 1] loss: 0.8728, acc: 0.5924\n","[epoch 1] loss: 0.8729, acc: 0.5934\n","[epoch 1] loss: 0.8716, acc: 0.5948\n","[epoch 1] loss: 0.8711, acc: 0.5957\n","[epoch 1] loss: 0.8714, acc: 0.5951\n","[epoch 1] loss: 0.8706, acc: 0.5954\n","[epoch 1] loss: 0.8706, acc: 0.5957\n","[epoch 1] loss: 0.8706, acc: 0.5964\n","[epoch 1] loss: 0.8705, acc: 0.5969\n","[epoch 1] loss: 0.8702, acc: 0.5969\n","[epoch 1] loss: 0.8694, acc: 0.5977\n","> val_acc: 0.6123, val_f1: 0.5770\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6123\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 2\n","[epoch 2] loss: 0.8529, acc: 0.6125\n","[epoch 2] loss: 0.8732, acc: 0.5969\n","[epoch 2] loss: 0.8659, acc: 0.5979\n","[epoch 2] loss: 0.8629, acc: 0.6062\n","[epoch 2] loss: 0.8670, acc: 0.6088\n","[epoch 2] loss: 0.8645, acc: 0.6062\n","[epoch 2] loss: 0.8702, acc: 0.6045\n","[epoch 2] loss: 0.8618, acc: 0.6055\n","[epoch 2] loss: 0.8559, acc: 0.6049\n","[epoch 2] loss: 0.8533, acc: 0.6106\n","[epoch 2] loss: 0.8651, acc: 0.6034\n","[epoch 2] loss: 0.8669, acc: 0.6057\n","[epoch 2] loss: 0.8648, acc: 0.6072\n","[epoch 2] loss: 0.8603, acc: 0.6107\n","[epoch 2] loss: 0.8608, acc: 0.6138\n","[epoch 2] loss: 0.8592, acc: 0.6125\n","[epoch 2] loss: 0.8595, acc: 0.6132\n","[epoch 2] loss: 0.8585, acc: 0.6125\n","[epoch 2] loss: 0.8582, acc: 0.6132\n","[epoch 2] loss: 0.8584, acc: 0.6112\n","[epoch 2] loss: 0.8568, acc: 0.6119\n","[epoch 2] loss: 0.8575, acc: 0.6122\n","[epoch 2] loss: 0.8570, acc: 0.6114\n","[epoch 2] loss: 0.8550, acc: 0.6102\n","[epoch 2] loss: 0.8601, acc: 0.6062\n","[epoch 2] loss: 0.8576, acc: 0.6087\n","[epoch 2] loss: 0.8590, acc: 0.6079\n","[epoch 2] loss: 0.8567, acc: 0.6096\n","[epoch 2] loss: 0.8564, acc: 0.6103\n","[epoch 2] loss: 0.8555, acc: 0.6108\n","[epoch 2] loss: 0.8549, acc: 0.6105\n","[epoch 2] loss: 0.8563, acc: 0.6102\n","[epoch 2] loss: 0.8572, acc: 0.6091\n","[epoch 2] loss: 0.8575, acc: 0.6072\n","[epoch 2] loss: 0.8562, acc: 0.6077\n","[epoch 2] loss: 0.8538, acc: 0.6106\n","[epoch 2] loss: 0.8547, acc: 0.6110\n","[epoch 2] loss: 0.8554, acc: 0.6110\n","[epoch 2] loss: 0.8528, acc: 0.6125\n","[epoch 2] loss: 0.8525, acc: 0.6130\n","[epoch 2] loss: 0.8518, acc: 0.6139\n","[epoch 2] loss: 0.8545, acc: 0.6121\n","[epoch 2] loss: 0.8544, acc: 0.6121\n","[epoch 2] loss: 0.8550, acc: 0.6121\n","[epoch 2] loss: 0.8542, acc: 0.6131\n","[epoch 2] loss: 0.8527, acc: 0.6136\n","[epoch 2] loss: 0.8522, acc: 0.6134\n","[epoch 2] loss: 0.8527, acc: 0.6126\n","[epoch 2] loss: 0.8511, acc: 0.6142\n","[epoch 2] loss: 0.8503, acc: 0.6146\n","[epoch 2] loss: 0.8503, acc: 0.6152\n","[epoch 2] loss: 0.8496, acc: 0.6150\n","[epoch 2] loss: 0.8499, acc: 0.6144\n","[epoch 2] loss: 0.8497, acc: 0.6134\n","[epoch 2] loss: 0.8490, acc: 0.6140\n","[epoch 2] loss: 0.8474, acc: 0.6154\n","[epoch 2] loss: 0.8472, acc: 0.6158\n","[epoch 2] loss: 0.8458, acc: 0.6161\n","[epoch 2] loss: 0.8467, acc: 0.6159\n","[epoch 2] loss: 0.8482, acc: 0.6151\n","[epoch 2] loss: 0.8477, acc: 0.6158\n","[epoch 2] loss: 0.8476, acc: 0.6153\n","[epoch 2] loss: 0.8473, acc: 0.6150\n","[epoch 2] loss: 0.8473, acc: 0.6151\n","[epoch 2] loss: 0.8458, acc: 0.6162\n","[epoch 2] loss: 0.8464, acc: 0.6156\n","[epoch 2] loss: 0.8473, acc: 0.6146\n","[epoch 2] loss: 0.8474, acc: 0.6143\n","[epoch 2] loss: 0.8472, acc: 0.6143\n","[epoch 2] loss: 0.8471, acc: 0.6140\n","> val_acc: 0.6123, val_f1: 0.5745\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 3\n","[epoch 3] loss: 0.8154, acc: 0.6188\n","[epoch 3] loss: 0.8282, acc: 0.6188\n","[epoch 3] loss: 0.8384, acc: 0.6208\n","[epoch 3] loss: 0.8249, acc: 0.6312\n","[epoch 3] loss: 0.8375, acc: 0.6238\n","[epoch 3] loss: 0.8452, acc: 0.6167\n","[epoch 3] loss: 0.8438, acc: 0.6161\n","[epoch 3] loss: 0.8368, acc: 0.6148\n","[epoch 3] loss: 0.8340, acc: 0.6146\n","[epoch 3] loss: 0.8382, acc: 0.6112\n","[epoch 3] loss: 0.8402, acc: 0.6080\n","[epoch 3] loss: 0.8396, acc: 0.6068\n","[epoch 3] loss: 0.8377, acc: 0.6048\n","[epoch 3] loss: 0.8383, acc: 0.6036\n","[epoch 3] loss: 0.8353, acc: 0.6075\n","[epoch 3] loss: 0.8366, acc: 0.6059\n","[epoch 3] loss: 0.8362, acc: 0.6070\n","[epoch 3] loss: 0.8339, acc: 0.6090\n","[epoch 3] loss: 0.8339, acc: 0.6105\n","[epoch 3] loss: 0.8329, acc: 0.6106\n","[epoch 3] loss: 0.8350, acc: 0.6101\n","[epoch 3] loss: 0.8360, acc: 0.6122\n","[epoch 3] loss: 0.8356, acc: 0.6130\n","[epoch 3] loss: 0.8381, acc: 0.6117\n","[epoch 3] loss: 0.8379, acc: 0.6118\n","[epoch 3] loss: 0.8321, acc: 0.6178\n","[epoch 3] loss: 0.8323, acc: 0.6176\n","[epoch 3] loss: 0.8346, acc: 0.6158\n","[epoch 3] loss: 0.8348, acc: 0.6159\n","[epoch 3] loss: 0.8337, acc: 0.6167\n","[epoch 3] loss: 0.8331, acc: 0.6179\n","[epoch 3] loss: 0.8314, acc: 0.6193\n","[epoch 3] loss: 0.8299, acc: 0.6210\n","[epoch 3] loss: 0.8299, acc: 0.6221\n","[epoch 3] loss: 0.8287, acc: 0.6211\n","[epoch 3] loss: 0.8307, acc: 0.6205\n","[epoch 3] loss: 0.8307, acc: 0.6198\n","[epoch 3] loss: 0.8317, acc: 0.6179\n","[epoch 3] loss: 0.8333, acc: 0.6168\n","[epoch 3] loss: 0.8321, acc: 0.6172\n","[epoch 3] loss: 0.8333, acc: 0.6169\n","[epoch 3] loss: 0.8335, acc: 0.6170\n","[epoch 3] loss: 0.8348, acc: 0.6167\n","[epoch 3] loss: 0.8332, acc: 0.6170\n","[epoch 3] loss: 0.8338, acc: 0.6176\n","[epoch 3] loss: 0.8353, acc: 0.6159\n","[epoch 3] loss: 0.8357, acc: 0.6156\n","[epoch 3] loss: 0.8352, acc: 0.6163\n","[epoch 3] loss: 0.8345, acc: 0.6167\n","[epoch 3] loss: 0.8348, acc: 0.6168\n","[epoch 3] loss: 0.8349, acc: 0.6164\n","[epoch 3] loss: 0.8355, acc: 0.6157\n","[epoch 3] loss: 0.8329, acc: 0.6180\n","[epoch 3] loss: 0.8324, acc: 0.6182\n","[epoch 3] loss: 0.8336, acc: 0.6175\n","[epoch 3] loss: 0.8337, acc: 0.6174\n","[epoch 3] loss: 0.8328, acc: 0.6182\n","[epoch 3] loss: 0.8326, acc: 0.6182\n","[epoch 3] loss: 0.8333, acc: 0.6178\n","[epoch 3] loss: 0.8336, acc: 0.6180\n","[epoch 3] loss: 0.8338, acc: 0.6173\n","[epoch 3] loss: 0.8338, acc: 0.6172\n","[epoch 3] loss: 0.8332, acc: 0.6178\n","[epoch 3] loss: 0.8329, acc: 0.6186\n","[epoch 3] loss: 0.8325, acc: 0.6185\n","[epoch 3] loss: 0.8337, acc: 0.6181\n","[epoch 3] loss: 0.8335, acc: 0.6176\n","[epoch 3] loss: 0.8337, acc: 0.6182\n","[epoch 3] loss: 0.8330, acc: 0.6191\n","[epoch 3] loss: 0.8319, acc: 0.6203\n","> val_acc: 0.6183, val_f1: 0.5877\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6183\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 4\n","[epoch 4] loss: 0.7026, acc: 0.7000\n","[epoch 4] loss: 0.7709, acc: 0.6531\n","[epoch 4] loss: 0.7927, acc: 0.6500\n","[epoch 4] loss: 0.8320, acc: 0.6297\n","[epoch 4] loss: 0.8190, acc: 0.6338\n","[epoch 4] loss: 0.8230, acc: 0.6312\n","[epoch 4] loss: 0.8193, acc: 0.6348\n","[epoch 4] loss: 0.8267, acc: 0.6289\n","[epoch 4] loss: 0.8189, acc: 0.6306\n","[epoch 4] loss: 0.8201, acc: 0.6275\n","[epoch 4] loss: 0.8238, acc: 0.6256\n","[epoch 4] loss: 0.8207, acc: 0.6245\n","[epoch 4] loss: 0.8186, acc: 0.6279\n","[epoch 4] loss: 0.8173, acc: 0.6295\n","[epoch 4] loss: 0.8135, acc: 0.6283\n","[epoch 4] loss: 0.8137, acc: 0.6281\n","[epoch 4] loss: 0.8094, acc: 0.6312\n","[epoch 4] loss: 0.8119, acc: 0.6285\n","[epoch 4] loss: 0.8162, acc: 0.6266\n","[epoch 4] loss: 0.8179, acc: 0.6272\n","[epoch 4] loss: 0.8175, acc: 0.6265\n","[epoch 4] loss: 0.8174, acc: 0.6273\n","[epoch 4] loss: 0.8162, acc: 0.6272\n","[epoch 4] loss: 0.8173, acc: 0.6258\n","[epoch 4] loss: 0.8190, acc: 0.6255\n","[epoch 4] loss: 0.8177, acc: 0.6269\n","[epoch 4] loss: 0.8180, acc: 0.6250\n","[epoch 4] loss: 0.8161, acc: 0.6263\n","[epoch 4] loss: 0.8178, acc: 0.6276\n","[epoch 4] loss: 0.8200, acc: 0.6277\n","[epoch 4] loss: 0.8196, acc: 0.6278\n","[epoch 4] loss: 0.8190, acc: 0.6281\n","[epoch 4] loss: 0.8160, acc: 0.6307\n","[epoch 4] loss: 0.8198, acc: 0.6292\n","[epoch 4] loss: 0.8216, acc: 0.6286\n","[epoch 4] loss: 0.8218, acc: 0.6271\n","[epoch 4] loss: 0.8205, acc: 0.6284\n","[epoch 4] loss: 0.8204, acc: 0.6281\n","[epoch 4] loss: 0.8204, acc: 0.6282\n","[epoch 4] loss: 0.8176, acc: 0.6308\n","[epoch 4] loss: 0.8175, acc: 0.6312\n","[epoch 4] loss: 0.8182, acc: 0.6326\n","[epoch 4] loss: 0.8153, acc: 0.6349\n","[epoch 4] loss: 0.8154, acc: 0.6342\n","[epoch 4] loss: 0.8156, acc: 0.6340\n","[epoch 4] loss: 0.8151, acc: 0.6342\n","[epoch 4] loss: 0.8151, acc: 0.6336\n","[epoch 4] loss: 0.8163, acc: 0.6332\n","[epoch 4] loss: 0.8152, acc: 0.6339\n","[epoch 4] loss: 0.8152, acc: 0.6339\n","[epoch 4] loss: 0.8150, acc: 0.6335\n","[epoch 4] loss: 0.8144, acc: 0.6335\n","[epoch 4] loss: 0.8151, acc: 0.6329\n","[epoch 4] loss: 0.8149, acc: 0.6325\n","[epoch 4] loss: 0.8154, acc: 0.6315\n","[epoch 4] loss: 0.8170, acc: 0.6301\n","[epoch 4] loss: 0.8161, acc: 0.6303\n","[epoch 4] loss: 0.8173, acc: 0.6291\n","[epoch 4] loss: 0.8172, acc: 0.6296\n","[epoch 4] loss: 0.8185, acc: 0.6292\n","[epoch 4] loss: 0.8193, acc: 0.6282\n","[epoch 4] loss: 0.8188, acc: 0.6279\n","[epoch 4] loss: 0.8195, acc: 0.6275\n","[epoch 4] loss: 0.8196, acc: 0.6271\n","[epoch 4] loss: 0.8199, acc: 0.6274\n","[epoch 4] loss: 0.8199, acc: 0.6277\n","[epoch 4] loss: 0.8191, acc: 0.6288\n","[epoch 4] loss: 0.8184, acc: 0.6289\n","[epoch 4] loss: 0.8186, acc: 0.6288\n","[epoch 4] loss: 0.8195, acc: 0.6283\n","> val_acc: 0.6302, val_f1: 0.5908\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6302\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 5\n","[epoch 5] loss: 0.7444, acc: 0.6875\n","[epoch 5] loss: 0.7703, acc: 0.6687\n","[epoch 5] loss: 0.7814, acc: 0.6562\n","[epoch 5] loss: 0.7865, acc: 0.6531\n","[epoch 5] loss: 0.7961, acc: 0.6475\n","[epoch 5] loss: 0.8094, acc: 0.6344\n","[epoch 5] loss: 0.8045, acc: 0.6339\n","[epoch 5] loss: 0.8111, acc: 0.6289\n","[epoch 5] loss: 0.8060, acc: 0.6299\n","[epoch 5] loss: 0.8042, acc: 0.6331\n","[epoch 5] loss: 0.8029, acc: 0.6375\n","[epoch 5] loss: 0.8066, acc: 0.6359\n","[epoch 5] loss: 0.8050, acc: 0.6380\n","[epoch 5] loss: 0.8079, acc: 0.6344\n","[epoch 5] loss: 0.8084, acc: 0.6354\n","[epoch 5] loss: 0.8057, acc: 0.6402\n","[epoch 5] loss: 0.8071, acc: 0.6386\n","[epoch 5] loss: 0.8117, acc: 0.6344\n","[epoch 5] loss: 0.8121, acc: 0.6362\n","[epoch 5] loss: 0.8112, acc: 0.6366\n","[epoch 5] loss: 0.8106, acc: 0.6363\n","[epoch 5] loss: 0.8131, acc: 0.6332\n","[epoch 5] loss: 0.8140, acc: 0.6337\n","[epoch 5] loss: 0.8156, acc: 0.6341\n","[epoch 5] loss: 0.8119, acc: 0.6362\n","[epoch 5] loss: 0.8083, acc: 0.6389\n","[epoch 5] loss: 0.8088, acc: 0.6391\n","[epoch 5] loss: 0.8108, acc: 0.6371\n","[epoch 5] loss: 0.8095, acc: 0.6369\n","[epoch 5] loss: 0.8106, acc: 0.6362\n","[epoch 5] loss: 0.8128, acc: 0.6361\n","[epoch 5] loss: 0.8128, acc: 0.6350\n","[epoch 5] loss: 0.8109, acc: 0.6369\n","[epoch 5] loss: 0.8101, acc: 0.6375\n","[epoch 5] loss: 0.8093, acc: 0.6361\n","[epoch 5] loss: 0.8083, acc: 0.6366\n","[epoch 5] loss: 0.8101, acc: 0.6351\n","[epoch 5] loss: 0.8097, acc: 0.6359\n","[epoch 5] loss: 0.8112, acc: 0.6351\n","[epoch 5] loss: 0.8136, acc: 0.6320\n","[epoch 5] loss: 0.8150, acc: 0.6308\n","[epoch 5] loss: 0.8155, acc: 0.6308\n","[epoch 5] loss: 0.8136, acc: 0.6323\n","[epoch 5] loss: 0.8139, acc: 0.6331\n","[epoch 5] loss: 0.8136, acc: 0.6343\n","[epoch 5] loss: 0.8131, acc: 0.6346\n","[epoch 5] loss: 0.8124, acc: 0.6347\n","[epoch 5] loss: 0.8126, acc: 0.6337\n","[epoch 5] loss: 0.8124, acc: 0.6342\n","[epoch 5] loss: 0.8104, acc: 0.6352\n","[epoch 5] loss: 0.8101, acc: 0.6354\n","[epoch 5] loss: 0.8103, acc: 0.6349\n","[epoch 5] loss: 0.8098, acc: 0.6344\n","[epoch 5] loss: 0.8096, acc: 0.6343\n","[epoch 5] loss: 0.8102, acc: 0.6342\n","[epoch 5] loss: 0.8092, acc: 0.6353\n","[epoch 5] loss: 0.8104, acc: 0.6342\n","[epoch 5] loss: 0.8114, acc: 0.6341\n","[epoch 5] loss: 0.8091, acc: 0.6350\n","[epoch 5] loss: 0.8084, acc: 0.6349\n","[epoch 5] loss: 0.8086, acc: 0.6345\n","[epoch 5] loss: 0.8097, acc: 0.6339\n","[epoch 5] loss: 0.8091, acc: 0.6346\n","[epoch 5] loss: 0.8088, acc: 0.6354\n","[epoch 5] loss: 0.8086, acc: 0.6356\n","[epoch 5] loss: 0.8092, acc: 0.6347\n","[epoch 5] loss: 0.8084, acc: 0.6354\n","[epoch 5] loss: 0.8086, acc: 0.6346\n","[epoch 5] loss: 0.8080, acc: 0.6350\n","[epoch 5] loss: 0.8084, acc: 0.6343\n","> val_acc: 0.6257, val_f1: 0.5917\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 6\n","[epoch 6] loss: 0.7549, acc: 0.6625\n","[epoch 6] loss: 0.8082, acc: 0.6344\n","[epoch 6] loss: 0.7989, acc: 0.6458\n","[epoch 6] loss: 0.8001, acc: 0.6453\n","[epoch 6] loss: 0.7976, acc: 0.6412\n","[epoch 6] loss: 0.7924, acc: 0.6479\n","[epoch 6] loss: 0.8067, acc: 0.6339\n","[epoch 6] loss: 0.8019, acc: 0.6391\n","[epoch 6] loss: 0.8099, acc: 0.6306\n","[epoch 6] loss: 0.8082, acc: 0.6312\n","[epoch 6] loss: 0.8041, acc: 0.6301\n","[epoch 6] loss: 0.7956, acc: 0.6359\n","[epoch 6] loss: 0.7987, acc: 0.6361\n","[epoch 6] loss: 0.8016, acc: 0.6348\n","[epoch 6] loss: 0.8054, acc: 0.6325\n","[epoch 6] loss: 0.8013, acc: 0.6363\n","[epoch 6] loss: 0.8070, acc: 0.6324\n","[epoch 6] loss: 0.8098, acc: 0.6323\n","[epoch 6] loss: 0.8082, acc: 0.6309\n","[epoch 6] loss: 0.8083, acc: 0.6312\n","[epoch 6] loss: 0.8103, acc: 0.6286\n","[epoch 6] loss: 0.8068, acc: 0.6327\n","[epoch 6] loss: 0.8079, acc: 0.6329\n","[epoch 6] loss: 0.8074, acc: 0.6331\n","[epoch 6] loss: 0.8085, acc: 0.6320\n","[epoch 6] loss: 0.8101, acc: 0.6317\n","[epoch 6] loss: 0.8102, acc: 0.6315\n","[epoch 6] loss: 0.8080, acc: 0.6342\n","[epoch 6] loss: 0.8079, acc: 0.6334\n","[epoch 6] loss: 0.8027, acc: 0.6369\n","[epoch 6] loss: 0.8010, acc: 0.6383\n","[epoch 6] loss: 0.8002, acc: 0.6375\n","[epoch 6] loss: 0.7989, acc: 0.6384\n","[epoch 6] loss: 0.7999, acc: 0.6384\n","[epoch 6] loss: 0.7995, acc: 0.6384\n","[epoch 6] loss: 0.8019, acc: 0.6375\n","[epoch 6] loss: 0.8016, acc: 0.6382\n","[epoch 6] loss: 0.8021, acc: 0.6385\n","[epoch 6] loss: 0.8004, acc: 0.6397\n","[epoch 6] loss: 0.7994, acc: 0.6406\n","[epoch 6] loss: 0.7975, acc: 0.6418\n","[epoch 6] loss: 0.7976, acc: 0.6418\n","[epoch 6] loss: 0.7969, acc: 0.6422\n","[epoch 6] loss: 0.7966, acc: 0.6432\n","[epoch 6] loss: 0.7960, acc: 0.6435\n","[epoch 6] loss: 0.7949, acc: 0.6442\n","[epoch 6] loss: 0.7927, acc: 0.6453\n","[epoch 6] loss: 0.7913, acc: 0.6465\n","[epoch 6] loss: 0.7900, acc: 0.6481\n","[epoch 6] loss: 0.7925, acc: 0.6466\n","[epoch 6] loss: 0.7938, acc: 0.6452\n","[epoch 6] loss: 0.7928, acc: 0.6457\n","[epoch 6] loss: 0.7915, acc: 0.6461\n","[epoch 6] loss: 0.7911, acc: 0.6463\n","[epoch 6] loss: 0.7927, acc: 0.6450\n","[epoch 6] loss: 0.7925, acc: 0.6442\n","[epoch 6] loss: 0.7944, acc: 0.6429\n","[epoch 6] loss: 0.7940, acc: 0.6429\n","[epoch 6] loss: 0.7942, acc: 0.6422\n","[epoch 6] loss: 0.7952, acc: 0.6411\n","[epoch 6] loss: 0.7961, acc: 0.6405\n","[epoch 6] loss: 0.7970, acc: 0.6398\n","[epoch 6] loss: 0.7975, acc: 0.6400\n","[epoch 6] loss: 0.7964, acc: 0.6402\n","[epoch 6] loss: 0.7961, acc: 0.6398\n","[epoch 6] loss: 0.7972, acc: 0.6391\n","[epoch 6] loss: 0.7968, acc: 0.6394\n","[epoch 6] loss: 0.7979, acc: 0.6391\n","[epoch 6] loss: 0.7983, acc: 0.6396\n","[epoch 6] loss: 0.7997, acc: 0.6392\n","> val_acc: 0.6407, val_f1: 0.6070\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6407\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 7\n","[epoch 7] loss: 0.7222, acc: 0.7250\n","[epoch 7] loss: 0.7365, acc: 0.6875\n","[epoch 7] loss: 0.7454, acc: 0.6708\n","[epoch 7] loss: 0.7644, acc: 0.6578\n","[epoch 7] loss: 0.7689, acc: 0.6562\n","[epoch 7] loss: 0.7647, acc: 0.6615\n","[epoch 7] loss: 0.7595, acc: 0.6616\n","[epoch 7] loss: 0.7598, acc: 0.6625\n","[epoch 7] loss: 0.7702, acc: 0.6514\n","[epoch 7] loss: 0.7712, acc: 0.6512\n","[epoch 7] loss: 0.7787, acc: 0.6449\n","[epoch 7] loss: 0.7769, acc: 0.6458\n","[epoch 7] loss: 0.7797, acc: 0.6428\n","[epoch 7] loss: 0.7799, acc: 0.6438\n","[epoch 7] loss: 0.7815, acc: 0.6429\n","[epoch 7] loss: 0.7831, acc: 0.6430\n","[epoch 7] loss: 0.7806, acc: 0.6467\n","[epoch 7] loss: 0.7839, acc: 0.6451\n","[epoch 7] loss: 0.7837, acc: 0.6454\n","[epoch 7] loss: 0.7835, acc: 0.6450\n","[epoch 7] loss: 0.7848, acc: 0.6443\n","[epoch 7] loss: 0.7848, acc: 0.6440\n","[epoch 7] loss: 0.7887, acc: 0.6408\n","[epoch 7] loss: 0.7860, acc: 0.6427\n","[epoch 7] loss: 0.7875, acc: 0.6410\n","[epoch 7] loss: 0.7871, acc: 0.6411\n","[epoch 7] loss: 0.7850, acc: 0.6444\n","[epoch 7] loss: 0.7852, acc: 0.6451\n","[epoch 7] loss: 0.7856, acc: 0.6453\n","[epoch 7] loss: 0.7871, acc: 0.6446\n","[epoch 7] loss: 0.7875, acc: 0.6438\n","[epoch 7] loss: 0.7862, acc: 0.6426\n","[epoch 7] loss: 0.7863, acc: 0.6419\n","[epoch 7] loss: 0.7890, acc: 0.6403\n","[epoch 7] loss: 0.7896, acc: 0.6402\n","[epoch 7] loss: 0.7881, acc: 0.6415\n","[epoch 7] loss: 0.7896, acc: 0.6407\n","[epoch 7] loss: 0.7904, acc: 0.6398\n","[epoch 7] loss: 0.7895, acc: 0.6410\n","[epoch 7] loss: 0.7894, acc: 0.6409\n","[epoch 7] loss: 0.7914, acc: 0.6389\n","[epoch 7] loss: 0.7906, acc: 0.6396\n","[epoch 7] loss: 0.7895, acc: 0.6404\n","[epoch 7] loss: 0.7881, acc: 0.6416\n","[epoch 7] loss: 0.7902, acc: 0.6401\n","[epoch 7] loss: 0.7888, acc: 0.6408\n","[epoch 7] loss: 0.7902, acc: 0.6403\n","[epoch 7] loss: 0.7907, acc: 0.6404\n","[epoch 7] loss: 0.7914, acc: 0.6399\n","[epoch 7] loss: 0.7920, acc: 0.6400\n","[epoch 7] loss: 0.7928, acc: 0.6395\n","[epoch 7] loss: 0.7920, acc: 0.6399\n","[epoch 7] loss: 0.7928, acc: 0.6383\n","[epoch 7] loss: 0.7922, acc: 0.6387\n","[epoch 7] loss: 0.7922, acc: 0.6388\n","[epoch 7] loss: 0.7913, acc: 0.6394\n","[epoch 7] loss: 0.7927, acc: 0.6383\n","[epoch 7] loss: 0.7947, acc: 0.6377\n","[epoch 7] loss: 0.7948, acc: 0.6378\n","[epoch 7] loss: 0.7941, acc: 0.6378\n","[epoch 7] loss: 0.7937, acc: 0.6383\n","[epoch 7] loss: 0.7941, acc: 0.6374\n","[epoch 7] loss: 0.7940, acc: 0.6375\n","[epoch 7] loss: 0.7940, acc: 0.6374\n","[epoch 7] loss: 0.7940, acc: 0.6372\n","[epoch 7] loss: 0.7938, acc: 0.6377\n","[epoch 7] loss: 0.7930, acc: 0.6386\n","[epoch 7] loss: 0.7929, acc: 0.6390\n","[epoch 7] loss: 0.7917, acc: 0.6395\n","[epoch 7] loss: 0.7916, acc: 0.6397\n","> val_acc: 0.6400, val_f1: 0.6121\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 8\n","[epoch 8] loss: 0.7724, acc: 0.6813\n","[epoch 8] loss: 0.8023, acc: 0.6625\n","[epoch 8] loss: 0.8154, acc: 0.6312\n","[epoch 8] loss: 0.7968, acc: 0.6359\n","[epoch 8] loss: 0.7778, acc: 0.6500\n","[epoch 8] loss: 0.7894, acc: 0.6448\n","[epoch 8] loss: 0.7958, acc: 0.6420\n","[epoch 8] loss: 0.7886, acc: 0.6469\n","[epoch 8] loss: 0.7918, acc: 0.6458\n","[epoch 8] loss: 0.7885, acc: 0.6450\n","[epoch 8] loss: 0.7917, acc: 0.6443\n","[epoch 8] loss: 0.7901, acc: 0.6448\n","[epoch 8] loss: 0.7963, acc: 0.6394\n","[epoch 8] loss: 0.8019, acc: 0.6326\n","[epoch 8] loss: 0.8029, acc: 0.6312\n","[epoch 8] loss: 0.7990, acc: 0.6332\n","[epoch 8] loss: 0.8020, acc: 0.6331\n","[epoch 8] loss: 0.8047, acc: 0.6319\n","[epoch 8] loss: 0.8101, acc: 0.6280\n","[epoch 8] loss: 0.8123, acc: 0.6259\n","[epoch 8] loss: 0.8069, acc: 0.6312\n","[epoch 8] loss: 0.8023, acc: 0.6361\n","[epoch 8] loss: 0.8014, acc: 0.6361\n","[epoch 8] loss: 0.8003, acc: 0.6357\n","[epoch 8] loss: 0.8004, acc: 0.6355\n","[epoch 8] loss: 0.8007, acc: 0.6349\n","[epoch 8] loss: 0.8021, acc: 0.6361\n","[epoch 8] loss: 0.8041, acc: 0.6353\n","[epoch 8] loss: 0.8072, acc: 0.6334\n","[epoch 8] loss: 0.8076, acc: 0.6325\n","[epoch 8] loss: 0.8067, acc: 0.6329\n","[epoch 8] loss: 0.8058, acc: 0.6344\n","[epoch 8] loss: 0.8061, acc: 0.6324\n","[epoch 8] loss: 0.8058, acc: 0.6327\n","[epoch 8] loss: 0.8047, acc: 0.6336\n","[epoch 8] loss: 0.8040, acc: 0.6347\n","[epoch 8] loss: 0.8029, acc: 0.6365\n","[epoch 8] loss: 0.8017, acc: 0.6367\n","[epoch 8] loss: 0.7986, acc: 0.6381\n","[epoch 8] loss: 0.8010, acc: 0.6377\n","[epoch 8] loss: 0.8000, acc: 0.6378\n","[epoch 8] loss: 0.7988, acc: 0.6387\n","[epoch 8] loss: 0.7984, acc: 0.6388\n","[epoch 8] loss: 0.7985, acc: 0.6381\n","[epoch 8] loss: 0.7979, acc: 0.6379\n","[epoch 8] loss: 0.7958, acc: 0.6391\n","[epoch 8] loss: 0.7951, acc: 0.6394\n","[epoch 8] loss: 0.7942, acc: 0.6396\n","[epoch 8] loss: 0.7925, acc: 0.6403\n","[epoch 8] loss: 0.7904, acc: 0.6420\n","[epoch 8] loss: 0.7906, acc: 0.6418\n","[epoch 8] loss: 0.7907, acc: 0.6416\n","[epoch 8] loss: 0.7898, acc: 0.6420\n","[epoch 8] loss: 0.7890, acc: 0.6422\n","[epoch 8] loss: 0.7891, acc: 0.6427\n","[epoch 8] loss: 0.7894, acc: 0.6426\n","[epoch 8] loss: 0.7880, acc: 0.6438\n","[epoch 8] loss: 0.7885, acc: 0.6434\n","[epoch 8] loss: 0.7880, acc: 0.6440\n","[epoch 8] loss: 0.7875, acc: 0.6443\n","[epoch 8] loss: 0.7867, acc: 0.6453\n","[epoch 8] loss: 0.7863, acc: 0.6457\n","[epoch 8] loss: 0.7853, acc: 0.6463\n","[epoch 8] loss: 0.7861, acc: 0.6457\n","[epoch 8] loss: 0.7850, acc: 0.6457\n","[epoch 8] loss: 0.7854, acc: 0.6450\n","[epoch 8] loss: 0.7862, acc: 0.6448\n","[epoch 8] loss: 0.7858, acc: 0.6449\n","[epoch 8] loss: 0.7864, acc: 0.6443\n","[epoch 8] loss: 0.7858, acc: 0.6444\n","> val_acc: 0.6445, val_f1: 0.6167\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6445\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 9\n","[epoch 9] loss: 0.6975, acc: 0.6625\n","[epoch 9] loss: 0.7043, acc: 0.6875\n","[epoch 9] loss: 0.7594, acc: 0.6562\n","[epoch 9] loss: 0.7640, acc: 0.6625\n","[epoch 9] loss: 0.7737, acc: 0.6575\n","[epoch 9] loss: 0.7676, acc: 0.6552\n","[epoch 9] loss: 0.7608, acc: 0.6634\n","[epoch 9] loss: 0.7676, acc: 0.6594\n","[epoch 9] loss: 0.7679, acc: 0.6562\n","[epoch 9] loss: 0.7598, acc: 0.6644\n","[epoch 9] loss: 0.7625, acc: 0.6631\n","[epoch 9] loss: 0.7663, acc: 0.6630\n","[epoch 9] loss: 0.7641, acc: 0.6644\n","[epoch 9] loss: 0.7612, acc: 0.6652\n","[epoch 9] loss: 0.7621, acc: 0.6650\n","[epoch 9] loss: 0.7687, acc: 0.6625\n","[epoch 9] loss: 0.7694, acc: 0.6614\n","[epoch 9] loss: 0.7731, acc: 0.6597\n","[epoch 9] loss: 0.7706, acc: 0.6602\n","[epoch 9] loss: 0.7712, acc: 0.6587\n","[epoch 9] loss: 0.7726, acc: 0.6571\n","[epoch 9] loss: 0.7751, acc: 0.6540\n","[epoch 9] loss: 0.7746, acc: 0.6535\n","[epoch 9] loss: 0.7713, acc: 0.6555\n","[epoch 9] loss: 0.7702, acc: 0.6567\n","[epoch 9] loss: 0.7680, acc: 0.6582\n","[epoch 9] loss: 0.7652, acc: 0.6595\n","[epoch 9] loss: 0.7703, acc: 0.6576\n","[epoch 9] loss: 0.7712, acc: 0.6565\n","[epoch 9] loss: 0.7708, acc: 0.6571\n","[epoch 9] loss: 0.7701, acc: 0.6573\n","[epoch 9] loss: 0.7710, acc: 0.6572\n","[epoch 9] loss: 0.7715, acc: 0.6562\n","[epoch 9] loss: 0.7713, acc: 0.6557\n","[epoch 9] loss: 0.7703, acc: 0.6550\n","[epoch 9] loss: 0.7741, acc: 0.6531\n","[epoch 9] loss: 0.7736, acc: 0.6535\n","[epoch 9] loss: 0.7755, acc: 0.6528\n","[epoch 9] loss: 0.7761, acc: 0.6526\n","[epoch 9] loss: 0.7763, acc: 0.6517\n","[epoch 9] loss: 0.7754, acc: 0.6524\n","[epoch 9] loss: 0.7753, acc: 0.6522\n","[epoch 9] loss: 0.7727, acc: 0.6538\n","[epoch 9] loss: 0.7725, acc: 0.6536\n","[epoch 9] loss: 0.7727, acc: 0.6533\n","[epoch 9] loss: 0.7739, acc: 0.6535\n","[epoch 9] loss: 0.7753, acc: 0.6532\n","[epoch 9] loss: 0.7745, acc: 0.6533\n","[epoch 9] loss: 0.7746, acc: 0.6533\n","[epoch 9] loss: 0.7748, acc: 0.6535\n","[epoch 9] loss: 0.7745, acc: 0.6537\n","[epoch 9] loss: 0.7736, acc: 0.6547\n","[epoch 9] loss: 0.7746, acc: 0.6540\n","[epoch 9] loss: 0.7763, acc: 0.6534\n","[epoch 9] loss: 0.7766, acc: 0.6523\n","[epoch 9] loss: 0.7770, acc: 0.6513\n","[epoch 9] loss: 0.7776, acc: 0.6512\n","[epoch 9] loss: 0.7763, acc: 0.6520\n","[epoch 9] loss: 0.7770, acc: 0.6517\n","[epoch 9] loss: 0.7778, acc: 0.6510\n","[epoch 9] loss: 0.7793, acc: 0.6501\n","[epoch 9] loss: 0.7792, acc: 0.6500\n","[epoch 9] loss: 0.7794, acc: 0.6494\n","[epoch 9] loss: 0.7802, acc: 0.6490\n","[epoch 9] loss: 0.7798, acc: 0.6495\n","[epoch 9] loss: 0.7808, acc: 0.6489\n","[epoch 9] loss: 0.7798, acc: 0.6495\n","[epoch 9] loss: 0.7791, acc: 0.6500\n","[epoch 9] loss: 0.7795, acc: 0.6495\n","[epoch 9] loss: 0.7788, acc: 0.6503\n","> val_acc: 0.6400, val_f1: 0.6063\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 10\n","[epoch 10] loss: 0.7968, acc: 0.6312\n","[epoch 10] loss: 0.7975, acc: 0.6531\n","[epoch 10] loss: 0.7772, acc: 0.6604\n","[epoch 10] loss: 0.7626, acc: 0.6672\n","[epoch 10] loss: 0.7716, acc: 0.6663\n","[epoch 10] loss: 0.7677, acc: 0.6573\n","[epoch 10] loss: 0.7588, acc: 0.6616\n","[epoch 10] loss: 0.7612, acc: 0.6586\n","[epoch 10] loss: 0.7663, acc: 0.6590\n","[epoch 10] loss: 0.7762, acc: 0.6544\n","[epoch 10] loss: 0.7672, acc: 0.6625\n","[epoch 10] loss: 0.7693, acc: 0.6625\n","[epoch 10] loss: 0.7725, acc: 0.6606\n","[epoch 10] loss: 0.7770, acc: 0.6562\n","[epoch 10] loss: 0.7795, acc: 0.6546\n","[epoch 10] loss: 0.7805, acc: 0.6543\n","[epoch 10] loss: 0.7823, acc: 0.6526\n","[epoch 10] loss: 0.7827, acc: 0.6514\n","[epoch 10] loss: 0.7776, acc: 0.6553\n","[epoch 10] loss: 0.7756, acc: 0.6553\n","[epoch 10] loss: 0.7730, acc: 0.6574\n","[epoch 10] loss: 0.7699, acc: 0.6599\n","[epoch 10] loss: 0.7727, acc: 0.6584\n","[epoch 10] loss: 0.7710, acc: 0.6602\n","[epoch 10] loss: 0.7698, acc: 0.6610\n","[epoch 10] loss: 0.7690, acc: 0.6613\n","[epoch 10] loss: 0.7674, acc: 0.6623\n","[epoch 10] loss: 0.7677, acc: 0.6616\n","[epoch 10] loss: 0.7698, acc: 0.6597\n","[epoch 10] loss: 0.7680, acc: 0.6596\n","[epoch 10] loss: 0.7698, acc: 0.6581\n","[epoch 10] loss: 0.7708, acc: 0.6570\n","[epoch 10] loss: 0.7701, acc: 0.6572\n","[epoch 10] loss: 0.7696, acc: 0.6568\n","[epoch 10] loss: 0.7698, acc: 0.6568\n","[epoch 10] loss: 0.7737, acc: 0.6547\n","[epoch 10] loss: 0.7743, acc: 0.6546\n","[epoch 10] loss: 0.7745, acc: 0.6539\n","[epoch 10] loss: 0.7746, acc: 0.6538\n","[epoch 10] loss: 0.7742, acc: 0.6542\n","[epoch 10] loss: 0.7726, acc: 0.6550\n","[epoch 10] loss: 0.7723, acc: 0.6555\n","[epoch 10] loss: 0.7740, acc: 0.6547\n","[epoch 10] loss: 0.7754, acc: 0.6537\n","[epoch 10] loss: 0.7740, acc: 0.6550\n","[epoch 10] loss: 0.7738, acc: 0.6548\n","[epoch 10] loss: 0.7744, acc: 0.6544\n","[epoch 10] loss: 0.7753, acc: 0.6531\n","[epoch 10] loss: 0.7751, acc: 0.6534\n","[epoch 10] loss: 0.7737, acc: 0.6546\n","[epoch 10] loss: 0.7736, acc: 0.6545\n","[epoch 10] loss: 0.7732, acc: 0.6548\n","[epoch 10] loss: 0.7738, acc: 0.6550\n","[epoch 10] loss: 0.7736, acc: 0.6551\n","[epoch 10] loss: 0.7743, acc: 0.6545\n","[epoch 10] loss: 0.7744, acc: 0.6541\n","[epoch 10] loss: 0.7744, acc: 0.6535\n","[epoch 10] loss: 0.7738, acc: 0.6537\n","[epoch 10] loss: 0.7742, acc: 0.6529\n","[epoch 10] loss: 0.7742, acc: 0.6526\n","[epoch 10] loss: 0.7747, acc: 0.6520\n","[epoch 10] loss: 0.7741, acc: 0.6523\n","[epoch 10] loss: 0.7740, acc: 0.6526\n","[epoch 10] loss: 0.7737, acc: 0.6526\n","[epoch 10] loss: 0.7746, acc: 0.6524\n","[epoch 10] loss: 0.7742, acc: 0.6524\n","[epoch 10] loss: 0.7735, acc: 0.6525\n","[epoch 10] loss: 0.7722, acc: 0.6532\n","[epoch 10] loss: 0.7712, acc: 0.6532\n","[epoch 10] loss: 0.7718, acc: 0.6533\n","> val_acc: 0.6407, val_f1: 0.6088\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 11\n","[epoch 11] loss: 0.7960, acc: 0.6625\n","[epoch 11] loss: 0.7727, acc: 0.6687\n","[epoch 11] loss: 0.7820, acc: 0.6542\n","[epoch 11] loss: 0.7650, acc: 0.6562\n","[epoch 11] loss: 0.7671, acc: 0.6512\n","[epoch 11] loss: 0.7702, acc: 0.6552\n","[epoch 11] loss: 0.7587, acc: 0.6625\n","[epoch 11] loss: 0.7592, acc: 0.6594\n","[epoch 11] loss: 0.7545, acc: 0.6604\n","[epoch 11] loss: 0.7484, acc: 0.6687\n","[epoch 11] loss: 0.7531, acc: 0.6614\n","[epoch 11] loss: 0.7554, acc: 0.6620\n","[epoch 11] loss: 0.7614, acc: 0.6601\n","[epoch 11] loss: 0.7597, acc: 0.6621\n","[epoch 11] loss: 0.7572, acc: 0.6629\n","[epoch 11] loss: 0.7568, acc: 0.6648\n","[epoch 11] loss: 0.7563, acc: 0.6651\n","[epoch 11] loss: 0.7567, acc: 0.6646\n","[epoch 11] loss: 0.7595, acc: 0.6632\n","[epoch 11] loss: 0.7623, acc: 0.6597\n","[epoch 11] loss: 0.7586, acc: 0.6625\n","[epoch 11] loss: 0.7613, acc: 0.6591\n","[epoch 11] loss: 0.7643, acc: 0.6576\n","[epoch 11] loss: 0.7634, acc: 0.6560\n","[epoch 11] loss: 0.7655, acc: 0.6528\n","[epoch 11] loss: 0.7680, acc: 0.6507\n","[epoch 11] loss: 0.7674, acc: 0.6507\n","[epoch 11] loss: 0.7661, acc: 0.6513\n","[epoch 11] loss: 0.7633, acc: 0.6543\n","[epoch 11] loss: 0.7625, acc: 0.6540\n","[epoch 11] loss: 0.7617, acc: 0.6554\n","[epoch 11] loss: 0.7626, acc: 0.6543\n","[epoch 11] loss: 0.7641, acc: 0.6536\n","[epoch 11] loss: 0.7665, acc: 0.6522\n","[epoch 11] loss: 0.7656, acc: 0.6543\n","[epoch 11] loss: 0.7691, acc: 0.6535\n","[epoch 11] loss: 0.7672, acc: 0.6541\n","[epoch 11] loss: 0.7675, acc: 0.6541\n","[epoch 11] loss: 0.7696, acc: 0.6527\n","[epoch 11] loss: 0.7686, acc: 0.6531\n","[epoch 11] loss: 0.7662, acc: 0.6549\n","[epoch 11] loss: 0.7664, acc: 0.6551\n","[epoch 11] loss: 0.7669, acc: 0.6548\n","[epoch 11] loss: 0.7658, acc: 0.6557\n","[epoch 11] loss: 0.7663, acc: 0.6550\n","[epoch 11] loss: 0.7659, acc: 0.6556\n","[epoch 11] loss: 0.7650, acc: 0.6561\n","[epoch 11] loss: 0.7643, acc: 0.6566\n","[epoch 11] loss: 0.7637, acc: 0.6574\n","[epoch 11] loss: 0.7621, acc: 0.6580\n","[epoch 11] loss: 0.7622, acc: 0.6578\n","[epoch 11] loss: 0.7627, acc: 0.6578\n","[epoch 11] loss: 0.7632, acc: 0.6572\n","[epoch 11] loss: 0.7620, acc: 0.6578\n","[epoch 11] loss: 0.7624, acc: 0.6577\n","[epoch 11] loss: 0.7637, acc: 0.6562\n","[epoch 11] loss: 0.7634, acc: 0.6559\n","[epoch 11] loss: 0.7627, acc: 0.6561\n","[epoch 11] loss: 0.7626, acc: 0.6564\n","[epoch 11] loss: 0.7622, acc: 0.6570\n","[epoch 11] loss: 0.7633, acc: 0.6569\n","[epoch 11] loss: 0.7647, acc: 0.6566\n","[epoch 11] loss: 0.7652, acc: 0.6569\n","[epoch 11] loss: 0.7649, acc: 0.6572\n","[epoch 11] loss: 0.7645, acc: 0.6574\n","[epoch 11] loss: 0.7640, acc: 0.6580\n","[epoch 11] loss: 0.7663, acc: 0.6566\n","[epoch 11] loss: 0.7663, acc: 0.6560\n","[epoch 11] loss: 0.7656, acc: 0.6564\n","[epoch 11] loss: 0.7653, acc: 0.6564\n","> val_acc: 0.6430, val_f1: 0.6165\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 12\n","[epoch 12] loss: 0.7838, acc: 0.6625\n","[epoch 12] loss: 0.8031, acc: 0.6406\n","[epoch 12] loss: 0.7637, acc: 0.6500\n","[epoch 12] loss: 0.7570, acc: 0.6656\n","[epoch 12] loss: 0.7611, acc: 0.6587\n","[epoch 12] loss: 0.7565, acc: 0.6604\n","[epoch 12] loss: 0.7533, acc: 0.6661\n","[epoch 12] loss: 0.7494, acc: 0.6734\n","[epoch 12] loss: 0.7482, acc: 0.6708\n","[epoch 12] loss: 0.7544, acc: 0.6700\n","[epoch 12] loss: 0.7553, acc: 0.6687\n","[epoch 12] loss: 0.7549, acc: 0.6661\n","[epoch 12] loss: 0.7502, acc: 0.6673\n","[epoch 12] loss: 0.7432, acc: 0.6714\n","[epoch 12] loss: 0.7532, acc: 0.6654\n","[epoch 12] loss: 0.7488, acc: 0.6676\n","[epoch 12] loss: 0.7527, acc: 0.6640\n","[epoch 12] loss: 0.7517, acc: 0.6646\n","[epoch 12] loss: 0.7573, acc: 0.6615\n","[epoch 12] loss: 0.7577, acc: 0.6600\n","[epoch 12] loss: 0.7581, acc: 0.6604\n","[epoch 12] loss: 0.7566, acc: 0.6597\n","[epoch 12] loss: 0.7561, acc: 0.6603\n","[epoch 12] loss: 0.7575, acc: 0.6589\n","[epoch 12] loss: 0.7586, acc: 0.6595\n","[epoch 12] loss: 0.7608, acc: 0.6591\n","[epoch 12] loss: 0.7576, acc: 0.6611\n","[epoch 12] loss: 0.7577, acc: 0.6616\n","[epoch 12] loss: 0.7588, acc: 0.6603\n","[epoch 12] loss: 0.7563, acc: 0.6604\n","[epoch 12] loss: 0.7540, acc: 0.6623\n","[epoch 12] loss: 0.7529, acc: 0.6623\n","[epoch 12] loss: 0.7512, acc: 0.6636\n","[epoch 12] loss: 0.7491, acc: 0.6649\n","[epoch 12] loss: 0.7491, acc: 0.6657\n","[epoch 12] loss: 0.7501, acc: 0.6644\n","[epoch 12] loss: 0.7527, acc: 0.6637\n","[epoch 12] loss: 0.7541, acc: 0.6620\n","[epoch 12] loss: 0.7532, acc: 0.6636\n","[epoch 12] loss: 0.7548, acc: 0.6628\n","[epoch 12] loss: 0.7557, acc: 0.6627\n","[epoch 12] loss: 0.7572, acc: 0.6619\n","[epoch 12] loss: 0.7588, acc: 0.6602\n","[epoch 12] loss: 0.7596, acc: 0.6607\n","[epoch 12] loss: 0.7607, acc: 0.6603\n","[epoch 12] loss: 0.7601, acc: 0.6603\n","[epoch 12] loss: 0.7603, acc: 0.6602\n","[epoch 12] loss: 0.7601, acc: 0.6608\n","[epoch 12] loss: 0.7591, acc: 0.6615\n","[epoch 12] loss: 0.7589, acc: 0.6625\n","[epoch 12] loss: 0.7587, acc: 0.6620\n","[epoch 12] loss: 0.7596, acc: 0.6620\n","[epoch 12] loss: 0.7596, acc: 0.6623\n","[epoch 12] loss: 0.7593, acc: 0.6626\n","[epoch 12] loss: 0.7600, acc: 0.6619\n","[epoch 12] loss: 0.7587, acc: 0.6623\n","[epoch 12] loss: 0.7591, acc: 0.6627\n","[epoch 12] loss: 0.7587, acc: 0.6631\n","[epoch 12] loss: 0.7591, acc: 0.6630\n","[epoch 12] loss: 0.7595, acc: 0.6627\n","[epoch 12] loss: 0.7596, acc: 0.6614\n","[epoch 12] loss: 0.7592, acc: 0.6614\n","[epoch 12] loss: 0.7576, acc: 0.6621\n","[epoch 12] loss: 0.7583, acc: 0.6621\n","[epoch 12] loss: 0.7584, acc: 0.6624\n","[epoch 12] loss: 0.7581, acc: 0.6621\n","[epoch 12] loss: 0.7579, acc: 0.6618\n","[epoch 12] loss: 0.7590, acc: 0.6613\n","[epoch 12] loss: 0.7593, acc: 0.6618\n","[epoch 12] loss: 0.7586, acc: 0.6621\n","> val_acc: 0.6497, val_f1: 0.6269\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6497\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 13\n","[epoch 13] loss: 0.7687, acc: 0.6625\n","[epoch 13] loss: 0.7673, acc: 0.6656\n","[epoch 13] loss: 0.7528, acc: 0.6583\n","[epoch 13] loss: 0.7572, acc: 0.6656\n","[epoch 13] loss: 0.7582, acc: 0.6650\n","[epoch 13] loss: 0.7701, acc: 0.6594\n","[epoch 13] loss: 0.7621, acc: 0.6634\n","[epoch 13] loss: 0.7524, acc: 0.6703\n","[epoch 13] loss: 0.7626, acc: 0.6653\n","[epoch 13] loss: 0.7615, acc: 0.6619\n","[epoch 13] loss: 0.7561, acc: 0.6648\n","[epoch 13] loss: 0.7559, acc: 0.6656\n","[epoch 13] loss: 0.7487, acc: 0.6687\n","[epoch 13] loss: 0.7504, acc: 0.6687\n","[epoch 13] loss: 0.7503, acc: 0.6687\n","[epoch 13] loss: 0.7554, acc: 0.6672\n","[epoch 13] loss: 0.7574, acc: 0.6665\n","[epoch 13] loss: 0.7585, acc: 0.6667\n","[epoch 13] loss: 0.7544, acc: 0.6691\n","[epoch 13] loss: 0.7546, acc: 0.6666\n","[epoch 13] loss: 0.7546, acc: 0.6685\n","[epoch 13] loss: 0.7538, acc: 0.6699\n","[epoch 13] loss: 0.7522, acc: 0.6696\n","[epoch 13] loss: 0.7513, acc: 0.6695\n","[epoch 13] loss: 0.7476, acc: 0.6700\n","[epoch 13] loss: 0.7464, acc: 0.6697\n","[epoch 13] loss: 0.7495, acc: 0.6674\n","[epoch 13] loss: 0.7494, acc: 0.6690\n","[epoch 13] loss: 0.7486, acc: 0.6690\n","[epoch 13] loss: 0.7496, acc: 0.6687\n","[epoch 13] loss: 0.7506, acc: 0.6681\n","[epoch 13] loss: 0.7523, acc: 0.6664\n","[epoch 13] loss: 0.7509, acc: 0.6657\n","[epoch 13] loss: 0.7549, acc: 0.6640\n","[epoch 13] loss: 0.7572, acc: 0.6632\n","[epoch 13] loss: 0.7563, acc: 0.6628\n","[epoch 13] loss: 0.7534, acc: 0.6640\n","[epoch 13] loss: 0.7530, acc: 0.6650\n","[epoch 13] loss: 0.7533, acc: 0.6660\n","[epoch 13] loss: 0.7537, acc: 0.6659\n","[epoch 13] loss: 0.7525, acc: 0.6662\n","[epoch 13] loss: 0.7505, acc: 0.6674\n","[epoch 13] loss: 0.7490, acc: 0.6682\n","[epoch 13] loss: 0.7487, acc: 0.6679\n","[epoch 13] loss: 0.7484, acc: 0.6676\n","[epoch 13] loss: 0.7490, acc: 0.6681\n","[epoch 13] loss: 0.7499, acc: 0.6677\n","[epoch 13] loss: 0.7505, acc: 0.6672\n","[epoch 13] loss: 0.7503, acc: 0.6673\n","[epoch 13] loss: 0.7514, acc: 0.6665\n","[epoch 13] loss: 0.7508, acc: 0.6662\n","[epoch 13] loss: 0.7531, acc: 0.6650\n","[epoch 13] loss: 0.7531, acc: 0.6658\n","[epoch 13] loss: 0.7531, acc: 0.6650\n","[epoch 13] loss: 0.7528, acc: 0.6659\n","[epoch 13] loss: 0.7523, acc: 0.6660\n","[epoch 13] loss: 0.7532, acc: 0.6652\n","[epoch 13] loss: 0.7532, acc: 0.6650\n","[epoch 13] loss: 0.7546, acc: 0.6644\n","[epoch 13] loss: 0.7543, acc: 0.6643\n","[epoch 13] loss: 0.7540, acc: 0.6644\n","[epoch 13] loss: 0.7540, acc: 0.6643\n","[epoch 13] loss: 0.7529, acc: 0.6646\n","[epoch 13] loss: 0.7529, acc: 0.6638\n","[epoch 13] loss: 0.7542, acc: 0.6636\n","[epoch 13] loss: 0.7544, acc: 0.6634\n","[epoch 13] loss: 0.7532, acc: 0.6637\n","[epoch 13] loss: 0.7534, acc: 0.6638\n","[epoch 13] loss: 0.7536, acc: 0.6639\n","[epoch 13] loss: 0.7528, acc: 0.6641\n","> val_acc: 0.6445, val_f1: 0.6271\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 14\n","[epoch 14] loss: 0.7904, acc: 0.6562\n","[epoch 14] loss: 0.8051, acc: 0.6438\n","[epoch 14] loss: 0.7769, acc: 0.6646\n","[epoch 14] loss: 0.7820, acc: 0.6719\n","[epoch 14] loss: 0.7664, acc: 0.6737\n","[epoch 14] loss: 0.7666, acc: 0.6687\n","[epoch 14] loss: 0.7624, acc: 0.6723\n","[epoch 14] loss: 0.7783, acc: 0.6602\n","[epoch 14] loss: 0.7683, acc: 0.6660\n","[epoch 14] loss: 0.7692, acc: 0.6694\n","[epoch 14] loss: 0.7626, acc: 0.6722\n","[epoch 14] loss: 0.7576, acc: 0.6708\n","[epoch 14] loss: 0.7566, acc: 0.6707\n","[epoch 14] loss: 0.7488, acc: 0.6750\n","[epoch 14] loss: 0.7462, acc: 0.6771\n","[epoch 14] loss: 0.7475, acc: 0.6770\n","[epoch 14] loss: 0.7493, acc: 0.6757\n","[epoch 14] loss: 0.7467, acc: 0.6760\n","[epoch 14] loss: 0.7445, acc: 0.6773\n","[epoch 14] loss: 0.7463, acc: 0.6769\n","[epoch 14] loss: 0.7448, acc: 0.6780\n","[epoch 14] loss: 0.7484, acc: 0.6776\n","[epoch 14] loss: 0.7463, acc: 0.6774\n","[epoch 14] loss: 0.7454, acc: 0.6771\n","[epoch 14] loss: 0.7430, acc: 0.6785\n","[epoch 14] loss: 0.7437, acc: 0.6760\n","[epoch 14] loss: 0.7447, acc: 0.6759\n","[epoch 14] loss: 0.7435, acc: 0.6754\n","[epoch 14] loss: 0.7456, acc: 0.6728\n","[epoch 14] loss: 0.7452, acc: 0.6733\n","[epoch 14] loss: 0.7423, acc: 0.6748\n","[epoch 14] loss: 0.7427, acc: 0.6732\n","[epoch 14] loss: 0.7457, acc: 0.6714\n","[epoch 14] loss: 0.7456, acc: 0.6717\n","[epoch 14] loss: 0.7461, acc: 0.6720\n","[epoch 14] loss: 0.7449, acc: 0.6736\n","[epoch 14] loss: 0.7453, acc: 0.6726\n","[epoch 14] loss: 0.7469, acc: 0.6711\n","[epoch 14] loss: 0.7470, acc: 0.6712\n","[epoch 14] loss: 0.7483, acc: 0.6711\n","[epoch 14] loss: 0.7499, acc: 0.6703\n","[epoch 14] loss: 0.7482, acc: 0.6710\n","[epoch 14] loss: 0.7521, acc: 0.6689\n","[epoch 14] loss: 0.7538, acc: 0.6675\n","[epoch 14] loss: 0.7548, acc: 0.6669\n","[epoch 14] loss: 0.7546, acc: 0.6679\n","[epoch 14] loss: 0.7554, acc: 0.6670\n","[epoch 14] loss: 0.7550, acc: 0.6672\n","[epoch 14] loss: 0.7541, acc: 0.6681\n","[epoch 14] loss: 0.7531, acc: 0.6691\n","[epoch 14] loss: 0.7521, acc: 0.6697\n","[epoch 14] loss: 0.7528, acc: 0.6695\n","[epoch 14] loss: 0.7537, acc: 0.6680\n","[epoch 14] loss: 0.7546, acc: 0.6670\n","[epoch 14] loss: 0.7546, acc: 0.6667\n","[epoch 14] loss: 0.7540, acc: 0.6666\n","[epoch 14] loss: 0.7539, acc: 0.6662\n","[epoch 14] loss: 0.7539, acc: 0.6665\n","[epoch 14] loss: 0.7529, acc: 0.6668\n","[epoch 14] loss: 0.7519, acc: 0.6671\n","[epoch 14] loss: 0.7509, acc: 0.6673\n","[epoch 14] loss: 0.7507, acc: 0.6668\n","[epoch 14] loss: 0.7511, acc: 0.6668\n","[epoch 14] loss: 0.7508, acc: 0.6673\n","[epoch 14] loss: 0.7500, acc: 0.6667\n","[epoch 14] loss: 0.7507, acc: 0.6660\n","[epoch 14] loss: 0.7501, acc: 0.6662\n","[epoch 14] loss: 0.7492, acc: 0.6665\n","[epoch 14] loss: 0.7485, acc: 0.6664\n","[epoch 14] loss: 0.7483, acc: 0.6664\n","> val_acc: 0.6534, val_f1: 0.6409\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6534\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 15\n","[epoch 15] loss: 0.6438, acc: 0.7125\n","[epoch 15] loss: 0.6713, acc: 0.7031\n","[epoch 15] loss: 0.7075, acc: 0.6792\n","[epoch 15] loss: 0.7066, acc: 0.6859\n","[epoch 15] loss: 0.7089, acc: 0.6950\n","[epoch 15] loss: 0.7110, acc: 0.6854\n","[epoch 15] loss: 0.7032, acc: 0.6866\n","[epoch 15] loss: 0.7130, acc: 0.6719\n","[epoch 15] loss: 0.7123, acc: 0.6708\n","[epoch 15] loss: 0.7230, acc: 0.6656\n","[epoch 15] loss: 0.7219, acc: 0.6705\n","[epoch 15] loss: 0.7232, acc: 0.6698\n","[epoch 15] loss: 0.7259, acc: 0.6683\n","[epoch 15] loss: 0.7283, acc: 0.6665\n","[epoch 15] loss: 0.7264, acc: 0.6704\n","[epoch 15] loss: 0.7321, acc: 0.6660\n","[epoch 15] loss: 0.7380, acc: 0.6636\n","[epoch 15] loss: 0.7372, acc: 0.6639\n","[epoch 15] loss: 0.7304, acc: 0.6681\n","[epoch 15] loss: 0.7295, acc: 0.6700\n","[epoch 15] loss: 0.7287, acc: 0.6702\n","[epoch 15] loss: 0.7290, acc: 0.6693\n","[epoch 15] loss: 0.7277, acc: 0.6690\n","[epoch 15] loss: 0.7317, acc: 0.6664\n","[epoch 15] loss: 0.7367, acc: 0.6657\n","[epoch 15] loss: 0.7360, acc: 0.6683\n","[epoch 15] loss: 0.7385, acc: 0.6667\n","[epoch 15] loss: 0.7406, acc: 0.6645\n","[epoch 15] loss: 0.7412, acc: 0.6651\n","[epoch 15] loss: 0.7458, acc: 0.6640\n","[epoch 15] loss: 0.7496, acc: 0.6613\n","[epoch 15] loss: 0.7484, acc: 0.6639\n","[epoch 15] loss: 0.7488, acc: 0.6638\n","[epoch 15] loss: 0.7501, acc: 0.6632\n","[epoch 15] loss: 0.7493, acc: 0.6641\n","[epoch 15] loss: 0.7487, acc: 0.6656\n","[epoch 15] loss: 0.7486, acc: 0.6660\n","[epoch 15] loss: 0.7521, acc: 0.6637\n","[epoch 15] loss: 0.7508, acc: 0.6641\n","[epoch 15] loss: 0.7503, acc: 0.6642\n","[epoch 15] loss: 0.7492, acc: 0.6648\n","[epoch 15] loss: 0.7487, acc: 0.6659\n","[epoch 15] loss: 0.7465, acc: 0.6669\n","[epoch 15] loss: 0.7464, acc: 0.6665\n","[epoch 15] loss: 0.7468, acc: 0.6668\n","[epoch 15] loss: 0.7451, acc: 0.6681\n","[epoch 15] loss: 0.7437, acc: 0.6690\n","[epoch 15] loss: 0.7426, acc: 0.6699\n","[epoch 15] loss: 0.7416, acc: 0.6704\n","[epoch 15] loss: 0.7419, acc: 0.6704\n","[epoch 15] loss: 0.7414, acc: 0.6703\n","[epoch 15] loss: 0.7406, acc: 0.6702\n","[epoch 15] loss: 0.7405, acc: 0.6703\n","[epoch 15] loss: 0.7421, acc: 0.6686\n","[epoch 15] loss: 0.7431, acc: 0.6689\n","[epoch 15] loss: 0.7445, acc: 0.6683\n","[epoch 15] loss: 0.7444, acc: 0.6692\n","[epoch 15] loss: 0.7445, acc: 0.6686\n","[epoch 15] loss: 0.7430, acc: 0.6701\n","[epoch 15] loss: 0.7428, acc: 0.6699\n","[epoch 15] loss: 0.7432, acc: 0.6699\n","[epoch 15] loss: 0.7434, acc: 0.6689\n","[epoch 15] loss: 0.7431, acc: 0.6685\n","[epoch 15] loss: 0.7434, acc: 0.6680\n","[epoch 15] loss: 0.7440, acc: 0.6679\n","[epoch 15] loss: 0.7444, acc: 0.6675\n","[epoch 15] loss: 0.7444, acc: 0.6674\n","[epoch 15] loss: 0.7443, acc: 0.6666\n","[epoch 15] loss: 0.7429, acc: 0.6678\n","[epoch 15] loss: 0.7426, acc: 0.6685\n","> val_acc: 0.6482, val_f1: 0.6263\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 16\n","[epoch 16] loss: 0.6885, acc: 0.7312\n","[epoch 16] loss: 0.6630, acc: 0.7344\n","[epoch 16] loss: 0.6846, acc: 0.7229\n","[epoch 16] loss: 0.7004, acc: 0.7188\n","[epoch 16] loss: 0.6979, acc: 0.7100\n","[epoch 16] loss: 0.7038, acc: 0.7042\n","[epoch 16] loss: 0.7109, acc: 0.6946\n","[epoch 16] loss: 0.7150, acc: 0.6937\n","[epoch 16] loss: 0.7061, acc: 0.6979\n","[epoch 16] loss: 0.7078, acc: 0.6969\n","[epoch 16] loss: 0.7244, acc: 0.6875\n","[epoch 16] loss: 0.7157, acc: 0.6911\n","[epoch 16] loss: 0.7180, acc: 0.6889\n","[epoch 16] loss: 0.7202, acc: 0.6844\n","[epoch 16] loss: 0.7191, acc: 0.6863\n","[epoch 16] loss: 0.7182, acc: 0.6875\n","[epoch 16] loss: 0.7146, acc: 0.6912\n","[epoch 16] loss: 0.7107, acc: 0.6920\n","[epoch 16] loss: 0.7162, acc: 0.6888\n","[epoch 16] loss: 0.7158, acc: 0.6856\n","[epoch 16] loss: 0.7099, acc: 0.6875\n","[epoch 16] loss: 0.7103, acc: 0.6875\n","[epoch 16] loss: 0.7113, acc: 0.6878\n","[epoch 16] loss: 0.7130, acc: 0.6870\n","[epoch 16] loss: 0.7160, acc: 0.6857\n","[epoch 16] loss: 0.7184, acc: 0.6853\n","[epoch 16] loss: 0.7169, acc: 0.6850\n","[epoch 16] loss: 0.7163, acc: 0.6848\n","[epoch 16] loss: 0.7139, acc: 0.6871\n","[epoch 16] loss: 0.7179, acc: 0.6850\n","[epoch 16] loss: 0.7174, acc: 0.6847\n","[epoch 16] loss: 0.7168, acc: 0.6854\n","[epoch 16] loss: 0.7180, acc: 0.6839\n","[epoch 16] loss: 0.7191, acc: 0.6824\n","[epoch 16] loss: 0.7193, acc: 0.6821\n","[epoch 16] loss: 0.7191, acc: 0.6819\n","[epoch 16] loss: 0.7241, acc: 0.6794\n","[epoch 16] loss: 0.7224, acc: 0.6803\n","[epoch 16] loss: 0.7235, acc: 0.6795\n","[epoch 16] loss: 0.7251, acc: 0.6783\n","[epoch 16] loss: 0.7249, acc: 0.6780\n","[epoch 16] loss: 0.7261, acc: 0.6777\n","[epoch 16] loss: 0.7267, acc: 0.6769\n","[epoch 16] loss: 0.7272, acc: 0.6760\n","[epoch 16] loss: 0.7271, acc: 0.6757\n","[epoch 16] loss: 0.7288, acc: 0.6749\n","[epoch 16] loss: 0.7272, acc: 0.6759\n","[epoch 16] loss: 0.7268, acc: 0.6760\n","[epoch 16] loss: 0.7284, acc: 0.6753\n","[epoch 16] loss: 0.7300, acc: 0.6744\n","[epoch 16] loss: 0.7315, acc: 0.6732\n","[epoch 16] loss: 0.7326, acc: 0.6722\n","[epoch 16] loss: 0.7343, acc: 0.6711\n","[epoch 16] loss: 0.7358, acc: 0.6699\n","[epoch 16] loss: 0.7365, acc: 0.6698\n","[epoch 16] loss: 0.7372, acc: 0.6694\n","[epoch 16] loss: 0.7368, acc: 0.6695\n","[epoch 16] loss: 0.7363, acc: 0.6700\n","[epoch 16] loss: 0.7354, acc: 0.6707\n","[epoch 16] loss: 0.7365, acc: 0.6706\n","[epoch 16] loss: 0.7378, acc: 0.6700\n","[epoch 16] loss: 0.7375, acc: 0.6701\n","[epoch 16] loss: 0.7385, acc: 0.6691\n","[epoch 16] loss: 0.7380, acc: 0.6703\n","[epoch 16] loss: 0.7389, acc: 0.6703\n","[epoch 16] loss: 0.7382, acc: 0.6707\n","[epoch 16] loss: 0.7382, acc: 0.6706\n","[epoch 16] loss: 0.7390, acc: 0.6701\n","[epoch 16] loss: 0.7385, acc: 0.6706\n","[epoch 16] loss: 0.7388, acc: 0.6707\n","> val_acc: 0.6587, val_f1: 0.6386\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6587\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 17\n","[epoch 17] loss: 0.6945, acc: 0.7000\n","[epoch 17] loss: 0.7073, acc: 0.6813\n","[epoch 17] loss: 0.7335, acc: 0.6708\n","[epoch 17] loss: 0.7420, acc: 0.6641\n","[epoch 17] loss: 0.7497, acc: 0.6562\n","[epoch 17] loss: 0.7526, acc: 0.6583\n","[epoch 17] loss: 0.7514, acc: 0.6545\n","[epoch 17] loss: 0.7469, acc: 0.6617\n","[epoch 17] loss: 0.7468, acc: 0.6576\n","[epoch 17] loss: 0.7509, acc: 0.6556\n","[epoch 17] loss: 0.7625, acc: 0.6500\n","[epoch 17] loss: 0.7669, acc: 0.6479\n","[epoch 17] loss: 0.7583, acc: 0.6553\n","[epoch 17] loss: 0.7584, acc: 0.6540\n","[epoch 17] loss: 0.7588, acc: 0.6558\n","[epoch 17] loss: 0.7567, acc: 0.6578\n","[epoch 17] loss: 0.7520, acc: 0.6603\n","[epoch 17] loss: 0.7554, acc: 0.6583\n","[epoch 17] loss: 0.7519, acc: 0.6592\n","[epoch 17] loss: 0.7567, acc: 0.6581\n","[epoch 17] loss: 0.7562, acc: 0.6583\n","[epoch 17] loss: 0.7572, acc: 0.6577\n","[epoch 17] loss: 0.7561, acc: 0.6590\n","[epoch 17] loss: 0.7549, acc: 0.6583\n","[epoch 17] loss: 0.7525, acc: 0.6590\n","[epoch 17] loss: 0.7546, acc: 0.6579\n","[epoch 17] loss: 0.7544, acc: 0.6597\n","[epoch 17] loss: 0.7556, acc: 0.6585\n","[epoch 17] loss: 0.7548, acc: 0.6575\n","[epoch 17] loss: 0.7548, acc: 0.6575\n","[epoch 17] loss: 0.7557, acc: 0.6581\n","[epoch 17] loss: 0.7531, acc: 0.6602\n","[epoch 17] loss: 0.7521, acc: 0.6610\n","[epoch 17] loss: 0.7521, acc: 0.6612\n","[epoch 17] loss: 0.7512, acc: 0.6625\n","[epoch 17] loss: 0.7495, acc: 0.6634\n","[epoch 17] loss: 0.7475, acc: 0.6639\n","[epoch 17] loss: 0.7472, acc: 0.6645\n","[epoch 17] loss: 0.7450, acc: 0.6667\n","[epoch 17] loss: 0.7444, acc: 0.6670\n","[epoch 17] loss: 0.7427, acc: 0.6683\n","[epoch 17] loss: 0.7403, acc: 0.6696\n","[epoch 17] loss: 0.7418, acc: 0.6689\n","[epoch 17] loss: 0.7398, acc: 0.6710\n","[epoch 17] loss: 0.7387, acc: 0.6721\n","[epoch 17] loss: 0.7386, acc: 0.6730\n","[epoch 17] loss: 0.7395, acc: 0.6726\n","[epoch 17] loss: 0.7387, acc: 0.6720\n","[epoch 17] loss: 0.7387, acc: 0.6719\n","[epoch 17] loss: 0.7387, acc: 0.6721\n","[epoch 17] loss: 0.7382, acc: 0.6734\n","[epoch 17] loss: 0.7381, acc: 0.6736\n","[epoch 17] loss: 0.7365, acc: 0.6742\n","[epoch 17] loss: 0.7378, acc: 0.6730\n","[epoch 17] loss: 0.7383, acc: 0.6726\n","[epoch 17] loss: 0.7379, acc: 0.6732\n","[epoch 17] loss: 0.7375, acc: 0.6740\n","[epoch 17] loss: 0.7371, acc: 0.6746\n","[epoch 17] loss: 0.7368, acc: 0.6742\n","[epoch 17] loss: 0.7362, acc: 0.6743\n","[epoch 17] loss: 0.7352, acc: 0.6743\n","[epoch 17] loss: 0.7359, acc: 0.6737\n","[epoch 17] loss: 0.7357, acc: 0.6741\n","[epoch 17] loss: 0.7364, acc: 0.6731\n","[epoch 17] loss: 0.7368, acc: 0.6729\n","[epoch 17] loss: 0.7360, acc: 0.6736\n","[epoch 17] loss: 0.7359, acc: 0.6737\n","[epoch 17] loss: 0.7349, acc: 0.6740\n","[epoch 17] loss: 0.7347, acc: 0.6737\n","[epoch 17] loss: 0.7354, acc: 0.6732\n","> val_acc: 0.6549, val_f1: 0.6391\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 18\n","[epoch 18] loss: 0.7819, acc: 0.6375\n","[epoch 18] loss: 0.7737, acc: 0.6375\n","[epoch 18] loss: 0.7536, acc: 0.6542\n","[epoch 18] loss: 0.7578, acc: 0.6531\n","[epoch 18] loss: 0.7727, acc: 0.6512\n","[epoch 18] loss: 0.7682, acc: 0.6583\n","[epoch 18] loss: 0.7504, acc: 0.6670\n","[epoch 18] loss: 0.7450, acc: 0.6727\n","[epoch 18] loss: 0.7481, acc: 0.6653\n","[epoch 18] loss: 0.7431, acc: 0.6669\n","[epoch 18] loss: 0.7455, acc: 0.6619\n","[epoch 18] loss: 0.7411, acc: 0.6646\n","[epoch 18] loss: 0.7445, acc: 0.6606\n","[epoch 18] loss: 0.7372, acc: 0.6638\n","[epoch 18] loss: 0.7304, acc: 0.6671\n","[epoch 18] loss: 0.7292, acc: 0.6707\n","[epoch 18] loss: 0.7337, acc: 0.6669\n","[epoch 18] loss: 0.7315, acc: 0.6698\n","[epoch 18] loss: 0.7369, acc: 0.6697\n","[epoch 18] loss: 0.7352, acc: 0.6694\n","[epoch 18] loss: 0.7360, acc: 0.6696\n","[epoch 18] loss: 0.7357, acc: 0.6693\n","[epoch 18] loss: 0.7339, acc: 0.6690\n","[epoch 18] loss: 0.7300, acc: 0.6703\n","[epoch 18] loss: 0.7309, acc: 0.6697\n","[epoch 18] loss: 0.7314, acc: 0.6704\n","[epoch 18] loss: 0.7322, acc: 0.6704\n","[epoch 18] loss: 0.7307, acc: 0.6712\n","[epoch 18] loss: 0.7274, acc: 0.6735\n","[epoch 18] loss: 0.7253, acc: 0.6748\n","[epoch 18] loss: 0.7241, acc: 0.6762\n","[epoch 18] loss: 0.7221, acc: 0.6785\n","[epoch 18] loss: 0.7238, acc: 0.6767\n","[epoch 18] loss: 0.7227, acc: 0.6761\n","[epoch 18] loss: 0.7228, acc: 0.6766\n","[epoch 18] loss: 0.7202, acc: 0.6778\n","[epoch 18] loss: 0.7210, acc: 0.6779\n","[epoch 18] loss: 0.7202, acc: 0.6789\n","[epoch 18] loss: 0.7201, acc: 0.6784\n","[epoch 18] loss: 0.7197, acc: 0.6795\n","[epoch 18] loss: 0.7202, acc: 0.6808\n","[epoch 18] loss: 0.7203, acc: 0.6805\n","[epoch 18] loss: 0.7221, acc: 0.6799\n","[epoch 18] loss: 0.7235, acc: 0.6793\n","[epoch 18] loss: 0.7234, acc: 0.6782\n","[epoch 18] loss: 0.7255, acc: 0.6770\n","[epoch 18] loss: 0.7277, acc: 0.6758\n","[epoch 18] loss: 0.7285, acc: 0.6751\n","[epoch 18] loss: 0.7291, acc: 0.6737\n","[epoch 18] loss: 0.7298, acc: 0.6743\n","[epoch 18] loss: 0.7286, acc: 0.6748\n","[epoch 18] loss: 0.7284, acc: 0.6743\n","[epoch 18] loss: 0.7312, acc: 0.6731\n","[epoch 18] loss: 0.7310, acc: 0.6735\n","[epoch 18] loss: 0.7311, acc: 0.6734\n","[epoch 18] loss: 0.7323, acc: 0.6718\n","[epoch 18] loss: 0.7310, acc: 0.6723\n","[epoch 18] loss: 0.7303, acc: 0.6725\n","[epoch 18] loss: 0.7289, acc: 0.6738\n","[epoch 18] loss: 0.7304, acc: 0.6735\n","[epoch 18] loss: 0.7295, acc: 0.6741\n","[epoch 18] loss: 0.7294, acc: 0.6747\n","[epoch 18] loss: 0.7311, acc: 0.6733\n","[epoch 18] loss: 0.7321, acc: 0.6723\n","[epoch 18] loss: 0.7324, acc: 0.6729\n","[epoch 18] loss: 0.7335, acc: 0.6717\n","[epoch 18] loss: 0.7328, acc: 0.6725\n","[epoch 18] loss: 0.7321, acc: 0.6732\n","[epoch 18] loss: 0.7317, acc: 0.6728\n","[epoch 18] loss: 0.7326, acc: 0.6725\n","> val_acc: 0.6609, val_f1: 0.6427\n",">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6609\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 19\n","[epoch 19] loss: 0.7280, acc: 0.6813\n","[epoch 19] loss: 0.7013, acc: 0.6906\n","[epoch 19] loss: 0.6877, acc: 0.6958\n","[epoch 19] loss: 0.6959, acc: 0.6891\n","[epoch 19] loss: 0.7167, acc: 0.6875\n","[epoch 19] loss: 0.7206, acc: 0.6792\n","[epoch 19] loss: 0.7341, acc: 0.6741\n","[epoch 19] loss: 0.7325, acc: 0.6766\n","[epoch 19] loss: 0.7360, acc: 0.6708\n","[epoch 19] loss: 0.7419, acc: 0.6681\n","[epoch 19] loss: 0.7475, acc: 0.6653\n","[epoch 19] loss: 0.7364, acc: 0.6714\n","[epoch 19] loss: 0.7312, acc: 0.6760\n","[epoch 19] loss: 0.7399, acc: 0.6728\n","[epoch 19] loss: 0.7359, acc: 0.6737\n","[epoch 19] loss: 0.7412, acc: 0.6699\n","[epoch 19] loss: 0.7395, acc: 0.6702\n","[epoch 19] loss: 0.7402, acc: 0.6708\n","[epoch 19] loss: 0.7379, acc: 0.6707\n","[epoch 19] loss: 0.7388, acc: 0.6687\n","[epoch 19] loss: 0.7398, acc: 0.6696\n","[epoch 19] loss: 0.7367, acc: 0.6710\n","[epoch 19] loss: 0.7389, acc: 0.6704\n","[epoch 19] loss: 0.7377, acc: 0.6706\n","[epoch 19] loss: 0.7398, acc: 0.6710\n","[epoch 19] loss: 0.7390, acc: 0.6714\n","[epoch 19] loss: 0.7420, acc: 0.6692\n","[epoch 19] loss: 0.7433, acc: 0.6694\n","[epoch 19] loss: 0.7412, acc: 0.6694\n","[epoch 19] loss: 0.7405, acc: 0.6694\n","[epoch 19] loss: 0.7386, acc: 0.6714\n","[epoch 19] loss: 0.7370, acc: 0.6715\n","[epoch 19] loss: 0.7363, acc: 0.6708\n","[epoch 19] loss: 0.7353, acc: 0.6699\n","[epoch 19] loss: 0.7353, acc: 0.6695\n","[epoch 19] loss: 0.7344, acc: 0.6714\n","[epoch 19] loss: 0.7336, acc: 0.6723\n","[epoch 19] loss: 0.7334, acc: 0.6727\n","[epoch 19] loss: 0.7326, acc: 0.6729\n","[epoch 19] loss: 0.7337, acc: 0.6722\n","[epoch 19] loss: 0.7323, acc: 0.6730\n","[epoch 19] loss: 0.7317, acc: 0.6737\n","[epoch 19] loss: 0.7310, acc: 0.6747\n","[epoch 19] loss: 0.7297, acc: 0.6756\n","[epoch 19] loss: 0.7328, acc: 0.6733\n","[epoch 19] loss: 0.7335, acc: 0.6731\n","[epoch 19] loss: 0.7340, acc: 0.6733\n","[epoch 19] loss: 0.7340, acc: 0.6728\n","[epoch 19] loss: 0.7329, acc: 0.6733\n","[epoch 19] loss: 0.7326, acc: 0.6729\n","[epoch 19] loss: 0.7315, acc: 0.6739\n","[epoch 19] loss: 0.7307, acc: 0.6736\n","[epoch 19] loss: 0.7301, acc: 0.6731\n","[epoch 19] loss: 0.7289, acc: 0.6742\n","[epoch 19] loss: 0.7292, acc: 0.6747\n","[epoch 19] loss: 0.7288, acc: 0.6744\n","[epoch 19] loss: 0.7284, acc: 0.6753\n","[epoch 19] loss: 0.7287, acc: 0.6756\n","[epoch 19] loss: 0.7282, acc: 0.6758\n","[epoch 19] loss: 0.7285, acc: 0.6756\n","[epoch 19] loss: 0.7291, acc: 0.6756\n","[epoch 19] loss: 0.7281, acc: 0.6769\n","[epoch 19] loss: 0.7279, acc: 0.6771\n","[epoch 19] loss: 0.7281, acc: 0.6774\n","[epoch 19] loss: 0.7279, acc: 0.6769\n","[epoch 19] loss: 0.7280, acc: 0.6771\n","[epoch 19] loss: 0.7280, acc: 0.6772\n","[epoch 19] loss: 0.7276, acc: 0.6778\n","[epoch 19] loss: 0.7276, acc: 0.6778\n","[epoch 19] loss: 0.7289, acc: 0.6772\n","> val_acc: 0.6557, val_f1: 0.6398\n",">> test_acc: 0.6609, test_f1: 0.6427\n"]}]}]}