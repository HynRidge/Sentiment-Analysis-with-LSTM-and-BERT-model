{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **SETUP**"],"metadata":{"id":"5zXJrSMo495u"}},{"cell_type":"markdown","source":["### **Imports**"],"metadata":{"id":"8_7Yx7y-5BOG"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tYF42jlo5Q4i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666513390445,"user_tz":-480,"elapsed":40051,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}},"outputId":"c8c04d48-278e-46ce-d636-f45ea7fa7bc0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSD-dTWt28EH","executionInfo":{"status":"ok","timestamp":1666513402637,"user_tz":-480,"elapsed":12204,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}},"outputId":"188de6e0-1bea-40cd-a5e2-db0a2276d077"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 37.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"]}]},{"cell_type":"code","source":["import os\n","import math\n","import argparse\n","import sys\n","import random\n","import pickle\n","import spacy\n","import numpy as np"],"metadata":{"id":"eTERBhPf5Ykk","executionInfo":{"status":"ok","timestamp":1666513413885,"user_tz":-480,"elapsed":11265,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from time import strftime, localtime\n","from transformers import BertTokenizer\n","from sklearn import metrics\n","from xml.etree.ElementTree import parse\n","from spacy.tokens import Doc"],"metadata":{"id":"ZEPQFq-e6gVp","executionInfo":{"status":"ok","timestamp":1666513414375,"user_tz":-480,"elapsed":500,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split"],"metadata":{"id":"6674vF4n6le1","executionInfo":{"status":"ok","timestamp":1666513414376,"user_tz":-480,"elapsed":9,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# **CLASSES**"],"metadata":{"id":"IDenOuxLpJFY"}},{"cell_type":"markdown","source":["### **Data**"],"metadata":{"id":"USX4iChw67d2"}},{"cell_type":"code","source":["def pad_and_truncate(sequence, max_len, dtype='int64', padding='post', truncating='post', value=0):\n","    x = (np.ones(max_len) * value).astype(dtype)\n","    if truncating == 'pre':\n","        trunc = sequence[-max_len:]\n","    else:\n","        trunc = sequence[:max_len]\n","    trunc = np.asarray(trunc, dtype=dtype)\n","    if padding == 'post':\n","        x[:len(trunc)] = trunc\n","    else:\n","        x[-len(trunc):] = trunc\n","    return x"],"metadata":{"id":"L5p9tWvxkY2N","executionInfo":{"status":"ok","timestamp":1666513414892,"user_tz":-480,"elapsed":19,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Tokenizer():\n","    def __init__(self, max_len, lower=True):\n","        self.lower = lower\n","        self.max_len = max_len\n","        self.word_to_index = {}\n","        self.index_to_word = {}\n","        self.idx = 1\n","\n","    def fit_on_text(self, text):\n","        if self.lower:\n","            text = text.lower()\n","        words = text.split()\n","        for word in words:\n","            if word not in self.word_to_index:\n","                self.word_to_index[word] = self.idx\n","                self.index_to_word[self.idx] = word\n","                self.idx += 1\n","\n","    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n","        if self.lower:\n","            text = text.lower()\n","        words = text.split()\n","        unk = len(self.word_to_index)+1\n","        sequence = [self.word_to_index[w] if w in self.word_to_index else unk for w in words]\n","        if len(sequence) == 0:\n","            sequence = [0]\n","        if reverse:\n","            sequence = sequence[::-1]\n","        return pad_and_truncate(sequence, self.max_len, padding=padding, truncating=truncating)"],"metadata":{"id":"4y8GnOALjcjf","executionInfo":{"status":"ok","timestamp":1666513414893,"user_tz":-480,"elapsed":17,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class ABSADataset(Dataset):\n","    def __init__(self, file_name, tokenizer):\n","        with open(file_name, 'r', newline='\\n', encoding='utf-8', errors='ignore') as f:\n","            lines = f.readlines()\n","        with open(file_name + '.graph', 'rb') as f:\n","            index_to_graph = pickle.load(f)\n","        \n","        self.data = []\n","\n","        # Create tokens with tokenizer\n","        for i in range(0, len(lines), 3):\n","            left, right = [s.lower().strip() for s in lines[i].split(\"$T$\")]\n","            aspect = lines[i+1].lower().strip()\n","            polarity = lines[i+2].strip()\n","\n","            text_token = tokenizer.text_to_sequence(\" \".join([left, aspect, right]))\n","            context_token = tokenizer.text_to_sequence(\" \".join([left, right]))\n","\n","            left_token = tokenizer.text_to_sequence(left)\n","            left_aspect_token = tokenizer.text_to_sequence(\" \".join([left, aspect]))\n","\n","            right_token = tokenizer.text_to_sequence(right, reverse=True)\n","            right_aspect_token = tokenizer.text_to_sequence(\" \".join([aspect, right]), reverse=True)\n","\n","            aspect_token = tokenizer.text_to_sequence(aspect)\n","\n","            left_len = np.sum(left_token != 0)\n","            aspect_len = np.sum(aspect_token != 0)\n","            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype = np.int64)\n","            polarity = int(polarity) + 1\n","            \n","            text_len = np.sum(text_token != 0)\n","            concat_segments_tokens = pad_and_truncate([0] * (text_len + 1) + [1] * (aspect_len + 1), tokenizer.max_len)\n","            \n","            dependency_graph = np.pad(index_to_graph[i],\n","                                      ((0, tokenizer.max_len-index_to_graph[i].shape[0]),\n","                                       (0, tokenizer.max_len-index_to_graph[i].shape[0])),\n","                                      'constant')\n","            \n","            self.data.append({\n","                'text_token' : text_token,\n","                'context_token' : context_token, \n","                'left_token' : left_token, \n","                'left_aspect_token' : left_aspect_token,\n","                'right_token' : right_token,\n","                'right_aspect_token' : right_aspect_token,\n","                'aspect_token' : aspect_token, \n","                'aspect_boundary' : aspect_boundary, \n","                'polarity' : polarity, \n","                'dependency_graph' : dependency_graph\n","            })\n","    \n","    def __getitem__(self, index):\n","        return self.data[index]\n","    \n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"Q5FagHQ9cZGs","executionInfo":{"status":"ok","timestamp":1666513414893,"user_tz":-480,"elapsed":15,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class WhitespaceTokenizer(object):\n","    def __init__(self, vocab):\n","        self.vocab = vocab\n","\n","    def __call__(self, text):\n","        words = text.split()\n","        # All tokens 'own' a subsequent space character in this tokenizer\n","        spaces = [True] * len(words)\n","        return Doc(self.vocab, words=words, spaces=spaces)"],"metadata":{"id":"gjHh_czJ0Rbn","executionInfo":{"status":"ok","timestamp":1666513414894,"user_tz":-480,"elapsed":15,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### **Model**"],"metadata":{"id":"fuiAR0ng6vID"}},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(LSTM, self).__init__()\n","        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n","        self.squeeze_embedding = SqueezeEmbedding()\n","        self.lstm = nn.LSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=1, batch_first=True)\n","        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n","    \n","    def forward(self, inputs):\n","        text_indices, aspect_indices = inputs[0], inputs[1]\n","        x_len = torch.sum(text_indices != 0, dim=-1)\n","        x_len_max = torch.max(x_len)\n","        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n","\n","        x = self.embed(text_indices)\n","        x = self.squeeze_embedding(x, x_len)\n","        aspect = self.embed(aspect_indices)\n","        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n","        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n","        x = torch.cat((aspect, x), dim=-1)\n","\n","        x_sort_idx = torch.sort(-x_len)[1].long()\n","        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n","        x_len = x_len[x_sort_idx]\n","        x = x[x_sort_idx]\n","\n","        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True)\n","\n","        global ht\n","        out_pack, (ht, ct) = self.lstm(x_emb_p, None)\n","        # print(out_pack)\n","        # print(out_pack.shape)\n","\n","        # ht = torch.transpose(ht, 0, 1)[\n","        #     x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n","        # ht = torch.transpose(ht, 0, 1)\n","\n","        # \"\"\"unpack: out\"\"\"\n","        # out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=True)  # (sequence, lengths)\n","        # out = out[0]  #\n","        # # global out\n","        # out = out[x_unsort_idx]\n","\n","        return self.dense(ht[0])"],"metadata":{"id":"xPQIKGdi0qXg","executionInfo":{"status":"ok","timestamp":1666513414894,"user_tz":-480,"elapsed":13,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class SqueezeEmbedding(nn.Module):\n","    \"\"\"\n","    Squeeze sequence embedding length to the longest one in the batch\n","    \"\"\"\n","    def __init__(self, batch_first=True):\n","        super(SqueezeEmbedding, self).__init__()\n","        self.batch_first = batch_first\n","\n","    def forward(self, x, x_len):\n","        \"\"\"\n","        sequence -> sort -> pad and pack -> unpack ->unsort\n","        :param x: sequence embedding vectors\n","        :param x_len: numpy/tensor list\n","        :return:\n","        \"\"\"\n","        \"\"\"sort\"\"\"\n","        x_sort_idx = torch.sort(-x_len)[1].long()\n","        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n","        x_len = x_len[x_sort_idx]\n","        x = x[x_sort_idx]\n","        \"\"\"pack\"\"\"\n","        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n","        \"\"\"unpack: out\"\"\"\n","        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n","        out = out[0]  #\n","        \"\"\"unsort\"\"\"\n","        out = out[x_unsort_idx]\n","        return out"],"metadata":{"id":"tFr-dJ6B1E55","executionInfo":{"status":"ok","timestamp":1666513414895,"user_tz":-480,"elapsed":13,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### **Main**"],"metadata":{"id":"6BmgF1FSp32r"}},{"cell_type":"code","source":["class Instructor:\n","    def __init__(self):\n","        tokenizer = build_tokenizer(\n","            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n","            max_seq_len=config2[\"max_seq_len\"],\n","            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n","        embedding_matrix = build_embedding_matrix(\n","            word2idx=tokenizer.word_to_index,\n","            embed_dim=config2[\"embed_dim\"],\n","            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n","        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n","\n","        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n","        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n","        assert 0 <= config2[\"valset_ratio\"] < 1\n","        if config2[\"valset_ratio\"] > 0:\n","            valset_len = int(len(self.trainset) * config2[\"valset_ratio\"])\n","            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n","        else:\n","            self.valset = self.testset\n","\n","        if config2[\"device\"].type == 'cuda':\n","            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n","        self._print_args()\n","\n","    def _print_args(self):\n","        n_trainable_params, n_nontrainable_params = 0, 0\n","        for p in self.model.parameters():\n","            n_params = torch.prod(torch.tensor(p.shape))\n","            if p.requires_grad:\n","                n_trainable_params += n_params\n","            else:\n","                n_nontrainable_params += n_params\n","        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n","\n","\n","    def _reset_params(self):\n","        for child in self.model.children():\n","            for p in child.parameters():\n","                if p.requires_grad:\n","                    if len(p.shape) > 1:\n","                        config2[\"initializer\"](p)\n","                    else:\n","                        stdv = 1. / math.sqrt(p.shape[0])\n","                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n","\n","    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n","        max_val_acc = 0\n","        max_val_f1 = 0\n","        max_val_epoch = 0\n","        global_step = 0\n","        path = None\n","        for i_epoch in range(config2[\"num_epoch\"]):\n","            print('>' * 100)\n","            print('epoch: {}'.format(i_epoch))\n","            n_correct, n_total, loss_total = 0, 0, 0\n","            # switch model to training mode\n","            self.model.train()\n","            for i_batch, batch in enumerate(train_data_loader):\n","                global_step += 1\n","                # clear gradient accumulators\n","                optimizer.zero_grad()\n","\n","                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n","                global outputs, targets\n","                outputs = self.model(inputs)\n","                targets = batch['polarity'].to(config2[\"device\"])\n","                # print(outputs)\n","                # print(targets)\n","                loss = criterion(outputs, targets)\n","                loss.backward()\n","                optimizer.step()\n","\n","                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n","                n_total += len(outputs)\n","                loss_total += loss.item() * len(outputs)\n","                if global_step % config2[\"log_step\"] == 0:\n","                    train_acc = n_correct / n_total\n","                    train_loss = loss_total / n_total\n","                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n","\n","            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n","            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n","            if val_acc > max_val_acc:\n","                max_val_acc = val_acc\n","                max_val_epoch = i_epoch\n","                if not os.path.exists('state_dict'):\n","                    os.mkdir('state_dict')\n","                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n","                torch.save(self.model.state_dict(), path)\n","                print('>> saved: {}'.format(path))\n","            if val_f1 > max_val_f1:\n","                max_val_f1 = val_f1\n","            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n","                print('>> early stop.')\n","                break\n","\n","        return path\n","\n","    def _evaluate_acc_f1(self, data_loader):\n","        n_correct, n_total = 0, 0\n","        t_targets_all, t_outputs_all = None, None\n","        # switch model to evaluation mode\n","        self.model.eval()\n","        with torch.no_grad():\n","            for i_batch, t_batch in enumerate(data_loader):\n","                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n","                t_targets = t_batch['polarity'].to(config2[\"device\"])\n","                t_outputs = self.model(t_inputs)\n","\n","                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n","                n_total += len(t_outputs)\n","\n","                if t_targets_all is None:\n","                    t_targets_all = t_targets\n","                    t_outputs_all = t_outputs\n","                else:\n","                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n","                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n","\n","        acc = n_correct / n_total\n","        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n","        return acc, f1\n","\n","    def run(self):\n","        # Loss and Optimizer\n","        criterion = nn.CrossEntropyLoss()\n","        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n","        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n","\n","        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n","        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n","        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n","\n","        self._reset_params()\n","        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n","        self.model.load_state_dict(torch.load(best_model_path))\n","        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n","        print('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"],"metadata":{"id":"mPRxQS-Jp5vP","executionInfo":{"status":"ok","timestamp":1666513415377,"user_tz":-480,"elapsed":494,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# **Functions**"],"metadata":{"id":"bv09Mo677CkE"}},{"cell_type":"code","source":["def change_format_sentences(source_path, target_path):\n","    f = open(target_path, \"w\")\n","\n","    sentences = parse(source_path).getroot()\n","    preprocessed = []\n","\n","    for sentence in sentences:\n","        text = sentence.find('text')\n","        if text is None:\n","            continue\n","        text = text.text\n","        aspectTerms = sentence.find('aspectTerms')\n","        if aspectTerms is None:\n","            continue\n","        for aspectTerm in aspectTerms:\n","            term = aspectTerm.get('term')\n","            polarity = aspectTerm.get('polarity')\n","            if polarity == \"positive\":\n","                polarity = '1'\n","            elif polarity == \"neutral\":\n","                polarity = '0'\n","            elif polarity == \"negative\":\n","                polarity = '-1'\n","            else:\n","                raise Exception(\"invalid polarity!\")\n","            start = int(aspectTerm.get('from'))\n","            end = int(aspectTerm.get('to'))\n","            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n","            preprocessed.append(text[start:end])\n","            preprocessed.append(polarity)\n","    f.write(\"\\n\".join(preprocessed))"],"metadata":{"id":"Mlhjev_wrkDF","executionInfo":{"status":"ok","timestamp":1666513415378,"user_tz":-480,"elapsed":19,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def build_tokenizer(fnames, max_seq_len, dat_fname):\n","    if os.path.exists(dat_fname):\n","        print('loading tokenizer:', dat_fname)\n","        tokenizer = pickle.load(open(dat_fname, 'rb'))\n","    else:\n","        text = ''\n","        for fname in fnames:\n","            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","            lines = fin.readlines()\n","            fin.close()\n","            for i in range(0, len(lines), 3):\n","                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n","                aspect = lines[i + 1].lower().strip()\n","                text_raw = text_left + \" \" + aspect + \" \" + text_right\n","                text += text_raw + \" \"\n","\n","        tokenizer = Tokenizer(max_seq_len)\n","        tokenizer.fit_on_text(text)\n","        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n","    return tokenizer"],"metadata":{"id":"pV174mP37UZy","executionInfo":{"status":"ok","timestamp":1666513415378,"user_tz":-480,"elapsed":17,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def _load_word_vec(path, word2idx=None, embed_dim=300):\n","    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    word_vec = {}\n","    for line in fin:\n","        tokens = line.rstrip().split()\n","        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n","        if word in word2idx.keys():\n","            word_vec[word] = np.asarray(vec, dtype='float32')\n","    return word_vec"],"metadata":{"id":"2h-2UjGs7fu5","executionInfo":{"status":"ok","timestamp":1666513415379,"user_tz":-480,"elapsed":17,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n","    if os.path.exists(dat_fname):\n","        print('loading embedding_matrix:', dat_fname)\n","        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n","    else:\n","        print('loading word vectors...')\n","        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n","        fname = config[\"glove_path\"]\n","        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n","        print('building embedding_matrix:', dat_fname)\n","        for word, i in word2idx.items():\n","            vec = word_vec.get(word)\n","            if vec is not None:\n","                # words not found in embedding index will be all-zeros.\n","                embedding_matrix[i] = vec\n","        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n","    return embedding_matrix"],"metadata":{"id":"nB3jz5Fk7dZd","executionInfo":{"status":"ok","timestamp":1666513415379,"user_tz":-480,"elapsed":15,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n","    x = (np.ones(maxlen) * value).astype(dtype)\n","    if truncating == 'pre':\n","        trunc = sequence[-maxlen:]\n","    else:\n","        trunc = sequence[:maxlen]\n","    trunc = np.asarray(trunc, dtype=dtype)\n","    if padding == 'post':\n","        x[:len(trunc)] = trunc\n","    else:\n","        x[-len(trunc):] = trunc\n","    return x"],"metadata":{"id":"7JH84-Kx7Y-_","executionInfo":{"status":"ok","timestamp":1666513415380,"user_tz":-480,"elapsed":15,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def dependency_adj_matrix(text):\n","    # https://spacy.io/docs/usage/processing-text\n","    tokens = nlp(text)\n","    words = text.split()\n","    matrix = np.zeros((len(words), len(words))).astype('float32')\n","    assert len(words) == len(list(tokens))\n","\n","    for token in tokens:\n","        matrix[token.i][token.i] = 1\n","        for child in token.children:\n","            matrix[token.i][child.i] = 1\n","            matrix[child.i][token.i] = 1\n","\n","    return matrix"],"metadata":{"id":"VemwD4FB7sY1","executionInfo":{"status":"ok","timestamp":1666513415381,"user_tz":-480,"elapsed":14,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def process_data():\n","    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n","    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n","    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n","    process(config[\"processed_train_path\"])\n","    process(config[\"processed_val_path\"])\n","    process(config[\"processed_test_path\"])"],"metadata":{"id":"vC45U4k21oPd","executionInfo":{"status":"ok","timestamp":1666513415381,"user_tz":-480,"elapsed":13,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def process(filename):\n","    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    lines = fin.readlines()\n","    fin.close()\n","    idx2graph = {}\n","    fout = open(filename+'.graph', 'wb')\n","    for i in range(0, len(lines), 3):\n","        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n","        aspect = lines[i + 1].strip()\n","        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n","        idx2graph[i] = adj_matrix\n","    pickle.dump(idx2graph, fout)        \n","    fout.close() "],"metadata":{"id":"HlLsFiUJ7mPH","executionInfo":{"status":"ok","timestamp":1666513415382,"user_tz":-480,"elapsed":12,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def run():\n","    ins = Instructor()\n","    ins.run()"],"metadata":{"id":"ANtAieVU8lwS","executionInfo":{"status":"ok","timestamp":1666513415383,"user_tz":-480,"elapsed":12,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# **MAIN**"],"metadata":{"id":"cxb8D05u8UN7"}},{"cell_type":"markdown","source":["### **Configuration**"],"metadata":{"id":"5QyUuFxj8XuA"}},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"],"metadata":{"id":"AF3DYep7AW8k","executionInfo":{"status":"ok","timestamp":1666513416487,"user_tz":-480,"elapsed":1115,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["config = {\n","    \"base_path\": \"/content/drive/MyDrive/CS4248/MAMS-ATSA\",\n","    \"glove_path\": \"/content/drive/MyDrive/CS4248/glove.42B.300d.txt\"\n","}\n","\n","config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n","config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n","config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n","config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n","config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n","config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n","config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"],"metadata":{"id":"9y9MO-RW8bcS","executionInfo":{"status":"ok","timestamp":1666513416488,"user_tz":-480,"elapsed":6,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["dataset_files = {\n","    'train': config[\"processed_train_path\"],\n","    'test': config[\"processed_test_path\"]\n","}\n","\n","config2 = {\n","    \"model_name\" : \"LSTM\",\n","    \"lr\" : 2e-5,\n","    \"dropout\" : 0.1,\n","    \"l2reg\" : 0.01,\n","    \"num_epoch\" : 20,\n","    \"batch_size\" : 16,\n","    \"log_step\" : 10,\n","    \"embed_dim\" : 300,\n","    \"hidden_dim\" : 300,\n","    \"model_class\" : LSTM,\n","    \"dataset\": \"MAMS\",\n","    \"dataset_file\" : dataset_files,\n","    \"inputs_cols\" : ['text_token', 'aspect_token'],\n","    \"initializer\" : torch.nn.init.xavier_uniform_,\n","    \"optimizer\" : torch.optim.Adam,\n","    \"max_seq_len\" : 85,\n","    \"polarities_dim\" : 3,\n","    \"hops\" : 3,\n","    \"patience\" : 5,\n","    \"device\" : None,\n","    \"seed\" : 1234,\n","    \"valset_ratio\" : 0\n","}\n","\n","config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n","        if config2[\"device\"] is None else torch.device(config2[\"device\"])\n","\n","if config2[\"seed\"] is not None:\n","    random.seed(config2[\"seed\"])\n","    np.random.seed(config2[\"seed\"])\n","    torch.manual_seed(config2[\"seed\"])\n","    torch.cuda.manual_seed(config2[\"seed\"])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"],"metadata":{"id":"clOIqT523tWy","executionInfo":{"status":"ok","timestamp":1666513416488,"user_tz":-480,"elapsed":5,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### **Pipeline**"],"metadata":{"id":"3fT6RqeiAGmN"}},{"cell_type":"code","source":["#process_data()"],"metadata":{"id":"vT4xfKSC-Xpx","executionInfo":{"status":"ok","timestamp":1666513416489,"user_tz":-480,"elapsed":6,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["run()"],"metadata":{"id":"8gEIkSn38pHt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666514283081,"user_tz":-480,"elapsed":866597,"user":{"displayName":"Vishandi Rudy Keneta","userId":"08058779538980989747"}},"outputId":"4ede9ed1-2b0e-4129-9ad6-2c5d852cdfdf"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["loading word vectors...\n","building embedding_matrix: 300_MAMS_embedding_matrix.dat\n","> n_trainable_params: 1083303, n_nontrainable_params: 3994500\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 0\n","[epoch 0] loss: 1.1148, acc: 0.3937\n","[epoch 0] loss: 1.1065, acc: 0.4000\n","[epoch 0] loss: 1.0862, acc: 0.4250\n","[epoch 0] loss: 1.0815, acc: 0.4297\n","[epoch 0] loss: 1.0800, acc: 0.4338\n","[epoch 0] loss: 1.0823, acc: 0.4344\n","[epoch 0] loss: 1.0775, acc: 0.4402\n","[epoch 0] loss: 1.0797, acc: 0.4344\n","[epoch 0] loss: 1.0833, acc: 0.4271\n","[epoch 0] loss: 1.0828, acc: 0.4288\n","[epoch 0] loss: 1.0782, acc: 0.4352\n","[epoch 0] loss: 1.0781, acc: 0.4365\n","[epoch 0] loss: 1.0816, acc: 0.4317\n","[epoch 0] loss: 1.0815, acc: 0.4313\n","[epoch 0] loss: 1.0804, acc: 0.4321\n","[epoch 0] loss: 1.0815, acc: 0.4297\n","[epoch 0] loss: 1.0814, acc: 0.4313\n","[epoch 0] loss: 1.0808, acc: 0.4326\n","[epoch 0] loss: 1.0803, acc: 0.4326\n","[epoch 0] loss: 1.0782, acc: 0.4359\n","[epoch 0] loss: 1.0765, acc: 0.4378\n","[epoch 0] loss: 1.0749, acc: 0.4401\n","[epoch 0] loss: 1.0729, acc: 0.4437\n","[epoch 0] loss: 1.0727, acc: 0.4437\n","[epoch 0] loss: 1.0724, acc: 0.4445\n","[epoch 0] loss: 1.0732, acc: 0.4437\n","[epoch 0] loss: 1.0732, acc: 0.4447\n","[epoch 0] loss: 1.0735, acc: 0.4437\n","[epoch 0] loss: 1.0738, acc: 0.4444\n","[epoch 0] loss: 1.0731, acc: 0.4452\n","[epoch 0] loss: 1.0725, acc: 0.4462\n","[epoch 0] loss: 1.0717, acc: 0.4484\n","[epoch 0] loss: 1.0716, acc: 0.4492\n","[epoch 0] loss: 1.0726, acc: 0.4482\n","[epoch 0] loss: 1.0731, acc: 0.4475\n","[epoch 0] loss: 1.0730, acc: 0.4479\n","[epoch 0] loss: 1.0719, acc: 0.4497\n","[epoch 0] loss: 1.0724, acc: 0.4480\n","[epoch 0] loss: 1.0733, acc: 0.4470\n","[epoch 0] loss: 1.0728, acc: 0.4475\n","[epoch 0] loss: 1.0731, acc: 0.4471\n","[epoch 0] loss: 1.0724, acc: 0.4482\n","[epoch 0] loss: 1.0723, acc: 0.4488\n","[epoch 0] loss: 1.0721, acc: 0.4491\n","[epoch 0] loss: 1.0721, acc: 0.4489\n","[epoch 0] loss: 1.0725, acc: 0.4478\n","[epoch 0] loss: 1.0735, acc: 0.4468\n","[epoch 0] loss: 1.0730, acc: 0.4482\n","[epoch 0] loss: 1.0735, acc: 0.4474\n","[epoch 0] loss: 1.0744, acc: 0.4465\n","[epoch 0] loss: 1.0743, acc: 0.4460\n","[epoch 0] loss: 1.0745, acc: 0.4453\n","[epoch 0] loss: 1.0740, acc: 0.4463\n","[epoch 0] loss: 1.0739, acc: 0.4463\n","[epoch 0] loss: 1.0738, acc: 0.4468\n","[epoch 0] loss: 1.0740, acc: 0.4461\n","[epoch 0] loss: 1.0744, acc: 0.4450\n","[epoch 0] loss: 1.0745, acc: 0.4447\n","[epoch 0] loss: 1.0737, acc: 0.4467\n","[epoch 0] loss: 1.0734, acc: 0.4472\n","[epoch 0] loss: 1.0729, acc: 0.4476\n","[epoch 0] loss: 1.0726, acc: 0.4478\n","[epoch 0] loss: 1.0716, acc: 0.4491\n","[epoch 0] loss: 1.0715, acc: 0.4489\n","[epoch 0] loss: 1.0713, acc: 0.4491\n","[epoch 0] loss: 1.0707, acc: 0.4502\n","[epoch 0] loss: 1.0709, acc: 0.4504\n","[epoch 0] loss: 1.0712, acc: 0.4496\n","[epoch 0] loss: 1.0710, acc: 0.4497\n","[epoch 0] loss: 1.0712, acc: 0.4496\n","> val_acc: 0.4543, val_f1: 0.2130\n",">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.4543\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 1\n","[epoch 1] loss: 1.0920, acc: 0.4188\n","[epoch 1] loss: 1.0811, acc: 0.4469\n","[epoch 1] loss: 1.0785, acc: 0.4417\n","[epoch 1] loss: 1.0688, acc: 0.4547\n","[epoch 1] loss: 1.0603, acc: 0.4637\n","[epoch 1] loss: 1.0648, acc: 0.4542\n","[epoch 1] loss: 1.0673, acc: 0.4518\n","[epoch 1] loss: 1.0672, acc: 0.4531\n","[epoch 1] loss: 1.0690, acc: 0.4500\n","[epoch 1] loss: 1.0678, acc: 0.4544\n","[epoch 1] loss: 1.0719, acc: 0.4460\n","[epoch 1] loss: 1.0730, acc: 0.4448\n","[epoch 1] loss: 1.0714, acc: 0.4481\n","[epoch 1] loss: 1.0690, acc: 0.4522\n","[epoch 1] loss: 1.0684, acc: 0.4546\n","[epoch 1] loss: 1.0704, acc: 0.4512\n","[epoch 1] loss: 1.0685, acc: 0.4544\n","[epoch 1] loss: 1.0690, acc: 0.4528\n","[epoch 1] loss: 1.0700, acc: 0.4513\n","[epoch 1] loss: 1.0715, acc: 0.4484\n","[epoch 1] loss: 1.0719, acc: 0.4473\n","[epoch 1] loss: 1.0722, acc: 0.4472\n","[epoch 1] loss: 1.0720, acc: 0.4473\n","[epoch 1] loss: 1.0709, acc: 0.4492\n","[epoch 1] loss: 1.0704, acc: 0.4502\n","[epoch 1] loss: 1.0708, acc: 0.4495\n","[epoch 1] loss: 1.0708, acc: 0.4498\n","[epoch 1] loss: 1.0709, acc: 0.4491\n","[epoch 1] loss: 1.0703, acc: 0.4502\n","[epoch 1] loss: 1.0700, acc: 0.4510\n","[epoch 1] loss: 1.0694, acc: 0.4522\n","[epoch 1] loss: 1.0707, acc: 0.4504\n","[epoch 1] loss: 1.0711, acc: 0.4496\n","[epoch 1] loss: 1.0708, acc: 0.4506\n","[epoch 1] loss: 1.0705, acc: 0.4507\n","[epoch 1] loss: 1.0702, acc: 0.4512\n","[epoch 1] loss: 1.0700, acc: 0.4515\n","[epoch 1] loss: 1.0693, acc: 0.4530\n","[epoch 1] loss: 1.0699, acc: 0.4522\n","[epoch 1] loss: 1.0703, acc: 0.4516\n","[epoch 1] loss: 1.0711, acc: 0.4502\n","[epoch 1] loss: 1.0715, acc: 0.4499\n","[epoch 1] loss: 1.0711, acc: 0.4506\n","[epoch 1] loss: 1.0711, acc: 0.4509\n","[epoch 1] loss: 1.0708, acc: 0.4507\n","[epoch 1] loss: 1.0706, acc: 0.4514\n","[epoch 1] loss: 1.0701, acc: 0.4519\n","[epoch 1] loss: 1.0700, acc: 0.4518\n","[epoch 1] loss: 1.0700, acc: 0.4511\n","[epoch 1] loss: 1.0700, acc: 0.4512\n","[epoch 1] loss: 1.0701, acc: 0.4502\n","[epoch 1] loss: 1.0700, acc: 0.4502\n","[epoch 1] loss: 1.0700, acc: 0.4500\n","[epoch 1] loss: 1.0700, acc: 0.4502\n","[epoch 1] loss: 1.0707, acc: 0.4492\n","[epoch 1] loss: 1.0703, acc: 0.4498\n","[epoch 1] loss: 1.0694, acc: 0.4505\n","[epoch 1] loss: 1.0693, acc: 0.4503\n","[epoch 1] loss: 1.0695, acc: 0.4500\n","[epoch 1] loss: 1.0690, acc: 0.4507\n","[epoch 1] loss: 1.0688, acc: 0.4508\n","[epoch 1] loss: 1.0687, acc: 0.4508\n","[epoch 1] loss: 1.0682, acc: 0.4517\n","[epoch 1] loss: 1.0685, acc: 0.4512\n","[epoch 1] loss: 1.0693, acc: 0.4501\n","[epoch 1] loss: 1.0694, acc: 0.4496\n","[epoch 1] loss: 1.0692, acc: 0.4502\n","[epoch 1] loss: 1.0692, acc: 0.4502\n","[epoch 1] loss: 1.0693, acc: 0.4498\n","[epoch 1] loss: 1.0690, acc: 0.4505\n","> val_acc: 0.4551, val_f1: 0.2100\n",">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.4551\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 2\n","[epoch 2] loss: 1.0665, acc: 0.4688\n","[epoch 2] loss: 1.0838, acc: 0.4313\n","[epoch 2] loss: 1.0791, acc: 0.4417\n","[epoch 2] loss: 1.0716, acc: 0.4531\n","[epoch 2] loss: 1.0742, acc: 0.4487\n","[epoch 2] loss: 1.0672, acc: 0.4635\n","[epoch 2] loss: 1.0638, acc: 0.4652\n","[epoch 2] loss: 1.0588, acc: 0.4734\n","[epoch 2] loss: 1.0592, acc: 0.4701\n","[epoch 2] loss: 1.0602, acc: 0.4700\n","[epoch 2] loss: 1.0626, acc: 0.4625\n","[epoch 2] loss: 1.0637, acc: 0.4604\n","[epoch 2] loss: 1.0651, acc: 0.4587\n","[epoch 2] loss: 1.0627, acc: 0.4603\n","[epoch 2] loss: 1.0640, acc: 0.4600\n","[epoch 2] loss: 1.0637, acc: 0.4602\n","[epoch 2] loss: 1.0642, acc: 0.4599\n","[epoch 2] loss: 1.0649, acc: 0.4594\n","[epoch 2] loss: 1.0653, acc: 0.4595\n","[epoch 2] loss: 1.0657, acc: 0.4584\n","[epoch 2] loss: 1.0653, acc: 0.4598\n","[epoch 2] loss: 1.0654, acc: 0.4599\n","[epoch 2] loss: 1.0661, acc: 0.4598\n","[epoch 2] loss: 1.0673, acc: 0.4578\n","[epoch 2] loss: 1.0686, acc: 0.4545\n","[epoch 2] loss: 1.0677, acc: 0.4565\n","[epoch 2] loss: 1.0680, acc: 0.4553\n","[epoch 2] loss: 1.0676, acc: 0.4558\n","[epoch 2] loss: 1.0679, acc: 0.4550\n","[epoch 2] loss: 1.0680, acc: 0.4546\n","[epoch 2] loss: 1.0677, acc: 0.4540\n","[epoch 2] loss: 1.0680, acc: 0.4535\n","[epoch 2] loss: 1.0684, acc: 0.4523\n","[epoch 2] loss: 1.0676, acc: 0.4529\n","[epoch 2] loss: 1.0666, acc: 0.4545\n","[epoch 2] loss: 1.0665, acc: 0.4547\n","[epoch 2] loss: 1.0666, acc: 0.4544\n","[epoch 2] loss: 1.0677, acc: 0.4528\n","[epoch 2] loss: 1.0667, acc: 0.4542\n","[epoch 2] loss: 1.0664, acc: 0.4547\n","[epoch 2] loss: 1.0662, acc: 0.4546\n","[epoch 2] loss: 1.0668, acc: 0.4533\n","[epoch 2] loss: 1.0674, acc: 0.4523\n","[epoch 2] loss: 1.0672, acc: 0.4524\n","[epoch 2] loss: 1.0672, acc: 0.4517\n","[epoch 2] loss: 1.0673, acc: 0.4511\n","[epoch 2] loss: 1.0675, acc: 0.4497\n","[epoch 2] loss: 1.0671, acc: 0.4507\n","[epoch 2] loss: 1.0670, acc: 0.4511\n","[epoch 2] loss: 1.0670, acc: 0.4514\n","[epoch 2] loss: 1.0677, acc: 0.4504\n","[epoch 2] loss: 1.0682, acc: 0.4500\n","[epoch 2] loss: 1.0686, acc: 0.4504\n","[epoch 2] loss: 1.0687, acc: 0.4505\n","[epoch 2] loss: 1.0685, acc: 0.4507\n","[epoch 2] loss: 1.0681, acc: 0.4513\n","[epoch 2] loss: 1.0685, acc: 0.4505\n","[epoch 2] loss: 1.0679, acc: 0.4516\n","[epoch 2] loss: 1.0681, acc: 0.4512\n","[epoch 2] loss: 1.0687, acc: 0.4499\n","[epoch 2] loss: 1.0688, acc: 0.4498\n","[epoch 2] loss: 1.0683, acc: 0.4505\n","[epoch 2] loss: 1.0683, acc: 0.4508\n","[epoch 2] loss: 1.0683, acc: 0.4508\n","[epoch 2] loss: 1.0681, acc: 0.4511\n","[epoch 2] loss: 1.0685, acc: 0.4502\n","[epoch 2] loss: 1.0688, acc: 0.4498\n","[epoch 2] loss: 1.0684, acc: 0.4502\n","[epoch 2] loss: 1.0683, acc: 0.4504\n","[epoch 2] loss: 1.0682, acc: 0.4506\n","> val_acc: 0.4551, val_f1: 0.2100\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 3\n","[epoch 3] loss: 1.0568, acc: 0.4688\n","[epoch 3] loss: 1.0697, acc: 0.4500\n","[epoch 3] loss: 1.0617, acc: 0.4646\n","[epoch 3] loss: 1.0645, acc: 0.4609\n","[epoch 3] loss: 1.0646, acc: 0.4612\n","[epoch 3] loss: 1.0637, acc: 0.4583\n","[epoch 3] loss: 1.0644, acc: 0.4571\n","[epoch 3] loss: 1.0691, acc: 0.4492\n","[epoch 3] loss: 1.0706, acc: 0.4437\n","[epoch 3] loss: 1.0701, acc: 0.4456\n","[epoch 3] loss: 1.0709, acc: 0.4437\n","[epoch 3] loss: 1.0719, acc: 0.4437\n","[epoch 3] loss: 1.0711, acc: 0.4437\n","[epoch 3] loss: 1.0686, acc: 0.4473\n","[epoch 3] loss: 1.0688, acc: 0.4475\n","[epoch 3] loss: 1.0709, acc: 0.4434\n","[epoch 3] loss: 1.0704, acc: 0.4423\n","[epoch 3] loss: 1.0701, acc: 0.4424\n","[epoch 3] loss: 1.0710, acc: 0.4421\n","[epoch 3] loss: 1.0713, acc: 0.4428\n","[epoch 3] loss: 1.0725, acc: 0.4402\n","[epoch 3] loss: 1.0721, acc: 0.4409\n","[epoch 3] loss: 1.0724, acc: 0.4402\n","[epoch 3] loss: 1.0733, acc: 0.4388\n","[epoch 3] loss: 1.0723, acc: 0.4405\n","[epoch 3] loss: 1.0710, acc: 0.4437\n","[epoch 3] loss: 1.0706, acc: 0.4440\n","[epoch 3] loss: 1.0711, acc: 0.4437\n","[epoch 3] loss: 1.0701, acc: 0.4453\n","[epoch 3] loss: 1.0710, acc: 0.4437\n","[epoch 3] loss: 1.0704, acc: 0.4450\n","[epoch 3] loss: 1.0684, acc: 0.4479\n","[epoch 3] loss: 1.0679, acc: 0.4496\n","[epoch 3] loss: 1.0676, acc: 0.4498\n","[epoch 3] loss: 1.0670, acc: 0.4511\n","[epoch 3] loss: 1.0676, acc: 0.4498\n","[epoch 3] loss: 1.0683, acc: 0.4497\n","[epoch 3] loss: 1.0684, acc: 0.4498\n","[epoch 3] loss: 1.0679, acc: 0.4508\n","[epoch 3] loss: 1.0691, acc: 0.4489\n","[epoch 3] loss: 1.0696, acc: 0.4485\n","[epoch 3] loss: 1.0699, acc: 0.4482\n","[epoch 3] loss: 1.0702, acc: 0.4471\n","[epoch 3] loss: 1.0698, acc: 0.4479\n","[epoch 3] loss: 1.0703, acc: 0.4474\n","[epoch 3] loss: 1.0705, acc: 0.4467\n","[epoch 3] loss: 1.0703, acc: 0.4468\n","[epoch 3] loss: 1.0699, acc: 0.4475\n","[epoch 3] loss: 1.0702, acc: 0.4469\n","[epoch 3] loss: 1.0701, acc: 0.4475\n","[epoch 3] loss: 1.0698, acc: 0.4480\n","[epoch 3] loss: 1.0694, acc: 0.4484\n","[epoch 3] loss: 1.0685, acc: 0.4502\n","[epoch 3] loss: 1.0684, acc: 0.4501\n","[epoch 3] loss: 1.0687, acc: 0.4500\n","[epoch 3] loss: 1.0690, acc: 0.4494\n","[epoch 3] loss: 1.0686, acc: 0.4503\n","[epoch 3] loss: 1.0682, acc: 0.4505\n","[epoch 3] loss: 1.0684, acc: 0.4498\n","[epoch 3] loss: 1.0682, acc: 0.4503\n","[epoch 3] loss: 1.0680, acc: 0.4499\n","[epoch 3] loss: 1.0682, acc: 0.4493\n","[epoch 3] loss: 1.0679, acc: 0.4501\n","[epoch 3] loss: 1.0679, acc: 0.4505\n","[epoch 3] loss: 1.0680, acc: 0.4506\n","[epoch 3] loss: 1.0686, acc: 0.4498\n","[epoch 3] loss: 1.0684, acc: 0.4501\n","[epoch 3] loss: 1.0684, acc: 0.4499\n","[epoch 3] loss: 1.0687, acc: 0.4496\n","[epoch 3] loss: 1.0684, acc: 0.4503\n","> val_acc: 0.4543, val_f1: 0.2083\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 4\n","[epoch 4] loss: 1.0521, acc: 0.4938\n","[epoch 4] loss: 1.0602, acc: 0.4781\n","[epoch 4] loss: 1.0678, acc: 0.4604\n","[epoch 4] loss: 1.0679, acc: 0.4562\n","[epoch 4] loss: 1.0668, acc: 0.4587\n","[epoch 4] loss: 1.0653, acc: 0.4583\n","[epoch 4] loss: 1.0662, acc: 0.4562\n","[epoch 4] loss: 1.0674, acc: 0.4516\n","[epoch 4] loss: 1.0656, acc: 0.4549\n","[epoch 4] loss: 1.0661, acc: 0.4519\n","[epoch 4] loss: 1.0668, acc: 0.4523\n","[epoch 4] loss: 1.0664, acc: 0.4547\n","[epoch 4] loss: 1.0660, acc: 0.4553\n","[epoch 4] loss: 1.0661, acc: 0.4545\n","[epoch 4] loss: 1.0667, acc: 0.4525\n","[epoch 4] loss: 1.0666, acc: 0.4539\n","[epoch 4] loss: 1.0656, acc: 0.4562\n","[epoch 4] loss: 1.0684, acc: 0.4503\n","[epoch 4] loss: 1.0683, acc: 0.4510\n","[epoch 4] loss: 1.0677, acc: 0.4528\n","[epoch 4] loss: 1.0659, acc: 0.4560\n","[epoch 4] loss: 1.0666, acc: 0.4543\n","[epoch 4] loss: 1.0667, acc: 0.4535\n","[epoch 4] loss: 1.0663, acc: 0.4542\n","[epoch 4] loss: 1.0670, acc: 0.4535\n","[epoch 4] loss: 1.0665, acc: 0.4548\n","[epoch 4] loss: 1.0658, acc: 0.4558\n","[epoch 4] loss: 1.0655, acc: 0.4565\n","[epoch 4] loss: 1.0660, acc: 0.4560\n","[epoch 4] loss: 1.0670, acc: 0.4546\n","[epoch 4] loss: 1.0661, acc: 0.4558\n","[epoch 4] loss: 1.0661, acc: 0.4559\n","[epoch 4] loss: 1.0666, acc: 0.4559\n","[epoch 4] loss: 1.0666, acc: 0.4557\n","[epoch 4] loss: 1.0670, acc: 0.4546\n","[epoch 4] loss: 1.0675, acc: 0.4533\n","[epoch 4] loss: 1.0674, acc: 0.4537\n","[epoch 4] loss: 1.0678, acc: 0.4535\n","[epoch 4] loss: 1.0674, acc: 0.4542\n","[epoch 4] loss: 1.0665, acc: 0.4558\n","[epoch 4] loss: 1.0661, acc: 0.4566\n","[epoch 4] loss: 1.0660, acc: 0.4565\n","[epoch 4] loss: 1.0658, acc: 0.4568\n","[epoch 4] loss: 1.0657, acc: 0.4567\n","[epoch 4] loss: 1.0656, acc: 0.4565\n","[epoch 4] loss: 1.0660, acc: 0.4557\n","[epoch 4] loss: 1.0657, acc: 0.4557\n","[epoch 4] loss: 1.0657, acc: 0.4553\n","[epoch 4] loss: 1.0660, acc: 0.4552\n","[epoch 4] loss: 1.0661, acc: 0.4546\n","[epoch 4] loss: 1.0661, acc: 0.4545\n","[epoch 4] loss: 1.0660, acc: 0.4542\n","[epoch 4] loss: 1.0661, acc: 0.4535\n","[epoch 4] loss: 1.0657, acc: 0.4545\n","[epoch 4] loss: 1.0658, acc: 0.4544\n","[epoch 4] loss: 1.0659, acc: 0.4546\n","[epoch 4] loss: 1.0662, acc: 0.4542\n","[epoch 4] loss: 1.0669, acc: 0.4531\n","[epoch 4] loss: 1.0669, acc: 0.4534\n","[epoch 4] loss: 1.0672, acc: 0.4531\n","[epoch 4] loss: 1.0674, acc: 0.4525\n","[epoch 4] loss: 1.0675, acc: 0.4522\n","[epoch 4] loss: 1.0678, acc: 0.4516\n","[epoch 4] loss: 1.0676, acc: 0.4515\n","[epoch 4] loss: 1.0681, acc: 0.4503\n","[epoch 4] loss: 1.0686, acc: 0.4495\n","[epoch 4] loss: 1.0682, acc: 0.4507\n","[epoch 4] loss: 1.0681, acc: 0.4504\n","[epoch 4] loss: 1.0678, acc: 0.4513\n","[epoch 4] loss: 1.0682, acc: 0.4506\n","> val_acc: 0.4543, val_f1: 0.2083\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 5\n","[epoch 5] loss: 1.0536, acc: 0.4813\n","[epoch 5] loss: 1.0560, acc: 0.4719\n","[epoch 5] loss: 1.0694, acc: 0.4417\n","[epoch 5] loss: 1.0724, acc: 0.4375\n","[epoch 5] loss: 1.0732, acc: 0.4363\n","[epoch 5] loss: 1.0736, acc: 0.4365\n","[epoch 5] loss: 1.0732, acc: 0.4366\n","[epoch 5] loss: 1.0717, acc: 0.4398\n","[epoch 5] loss: 1.0699, acc: 0.4417\n","[epoch 5] loss: 1.0675, acc: 0.4487\n","[epoch 5] loss: 1.0668, acc: 0.4511\n","[epoch 5] loss: 1.0650, acc: 0.4531\n","[epoch 5] loss: 1.0637, acc: 0.4553\n","[epoch 5] loss: 1.0624, acc: 0.4585\n","[epoch 5] loss: 1.0622, acc: 0.4571\n","[epoch 5] loss: 1.0619, acc: 0.4590\n","[epoch 5] loss: 1.0617, acc: 0.4599\n","[epoch 5] loss: 1.0640, acc: 0.4559\n","[epoch 5] loss: 1.0628, acc: 0.4576\n","[epoch 5] loss: 1.0626, acc: 0.4572\n","[epoch 5] loss: 1.0637, acc: 0.4571\n","[epoch 5] loss: 1.0643, acc: 0.4557\n","[epoch 5] loss: 1.0659, acc: 0.4533\n","[epoch 5] loss: 1.0657, acc: 0.4544\n","[epoch 5] loss: 1.0672, acc: 0.4522\n","[epoch 5] loss: 1.0661, acc: 0.4546\n","[epoch 5] loss: 1.0657, acc: 0.4558\n","[epoch 5] loss: 1.0648, acc: 0.4569\n","[epoch 5] loss: 1.0647, acc: 0.4571\n","[epoch 5] loss: 1.0649, acc: 0.4567\n","[epoch 5] loss: 1.0655, acc: 0.4552\n","[epoch 5] loss: 1.0658, acc: 0.4543\n","[epoch 5] loss: 1.0660, acc: 0.4542\n","[epoch 5] loss: 1.0655, acc: 0.4550\n","[epoch 5] loss: 1.0654, acc: 0.4548\n","[epoch 5] loss: 1.0654, acc: 0.4556\n","[epoch 5] loss: 1.0663, acc: 0.4539\n","[epoch 5] loss: 1.0660, acc: 0.4544\n","[epoch 5] loss: 1.0665, acc: 0.4540\n","[epoch 5] loss: 1.0670, acc: 0.4528\n","[epoch 5] loss: 1.0674, acc: 0.4512\n","[epoch 5] loss: 1.0672, acc: 0.4518\n","[epoch 5] loss: 1.0670, acc: 0.4525\n","[epoch 5] loss: 1.0675, acc: 0.4514\n","[epoch 5] loss: 1.0677, acc: 0.4514\n","[epoch 5] loss: 1.0675, acc: 0.4518\n","[epoch 5] loss: 1.0681, acc: 0.4508\n","[epoch 5] loss: 1.0680, acc: 0.4505\n","[epoch 5] loss: 1.0683, acc: 0.4503\n","[epoch 5] loss: 1.0679, acc: 0.4511\n","[epoch 5] loss: 1.0678, acc: 0.4515\n","[epoch 5] loss: 1.0679, acc: 0.4508\n","[epoch 5] loss: 1.0677, acc: 0.4512\n","[epoch 5] loss: 1.0681, acc: 0.4505\n","[epoch 5] loss: 1.0684, acc: 0.4497\n","[epoch 5] loss: 1.0679, acc: 0.4506\n","[epoch 5] loss: 1.0679, acc: 0.4509\n","[epoch 5] loss: 1.0681, acc: 0.4502\n","[epoch 5] loss: 1.0680, acc: 0.4507\n","[epoch 5] loss: 1.0679, acc: 0.4508\n","[epoch 5] loss: 1.0678, acc: 0.4510\n","[epoch 5] loss: 1.0680, acc: 0.4503\n","[epoch 5] loss: 1.0680, acc: 0.4502\n","[epoch 5] loss: 1.0679, acc: 0.4503\n","[epoch 5] loss: 1.0676, acc: 0.4503\n","[epoch 5] loss: 1.0679, acc: 0.4498\n","[epoch 5] loss: 1.0682, acc: 0.4499\n","[epoch 5] loss: 1.0686, acc: 0.4494\n","[epoch 5] loss: 1.0681, acc: 0.4505\n","[epoch 5] loss: 1.0680, acc: 0.4507\n","> val_acc: 0.4551, val_f1: 0.2100\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","epoch: 6\n","[epoch 6] loss: 1.0658, acc: 0.4562\n","[epoch 6] loss: 1.0734, acc: 0.4437\n","[epoch 6] loss: 1.0770, acc: 0.4333\n","[epoch 6] loss: 1.0722, acc: 0.4391\n","[epoch 6] loss: 1.0748, acc: 0.4313\n","[epoch 6] loss: 1.0715, acc: 0.4385\n","[epoch 6] loss: 1.0728, acc: 0.4348\n","[epoch 6] loss: 1.0708, acc: 0.4352\n","[epoch 6] loss: 1.0744, acc: 0.4285\n","[epoch 6] loss: 1.0758, acc: 0.4231\n","[epoch 6] loss: 1.0758, acc: 0.4261\n","[epoch 6] loss: 1.0768, acc: 0.4250\n","[epoch 6] loss: 1.0756, acc: 0.4255\n","[epoch 6] loss: 1.0751, acc: 0.4290\n","[epoch 6] loss: 1.0765, acc: 0.4279\n","[epoch 6] loss: 1.0767, acc: 0.4270\n","[epoch 6] loss: 1.0772, acc: 0.4257\n","[epoch 6] loss: 1.0785, acc: 0.4233\n","[epoch 6] loss: 1.0796, acc: 0.4217\n","[epoch 6] loss: 1.0782, acc: 0.4269\n","[epoch 6] loss: 1.0774, acc: 0.4280\n","[epoch 6] loss: 1.0752, acc: 0.4315\n","[epoch 6] loss: 1.0745, acc: 0.4337\n","[epoch 6] loss: 1.0733, acc: 0.4352\n","[epoch 6] loss: 1.0717, acc: 0.4385\n","[epoch 6] loss: 1.0720, acc: 0.4385\n","[epoch 6] loss: 1.0709, acc: 0.4398\n","[epoch 6] loss: 1.0707, acc: 0.4400\n","[epoch 6] loss: 1.0709, acc: 0.4403\n","[epoch 6] loss: 1.0694, acc: 0.4433\n","[epoch 6] loss: 1.0694, acc: 0.4440\n","[epoch 6] loss: 1.0692, acc: 0.4441\n","[epoch 6] loss: 1.0684, acc: 0.4458\n","[epoch 6] loss: 1.0683, acc: 0.4465\n","[epoch 6] loss: 1.0687, acc: 0.4461\n","[epoch 6] loss: 1.0694, acc: 0.4444\n","[epoch 6] loss: 1.0689, acc: 0.4449\n","[epoch 6] loss: 1.0693, acc: 0.4444\n","[epoch 6] loss: 1.0691, acc: 0.4452\n","[epoch 6] loss: 1.0684, acc: 0.4473\n","[epoch 6] loss: 1.0673, acc: 0.4494\n","[epoch 6] loss: 1.0675, acc: 0.4490\n","[epoch 6] loss: 1.0682, acc: 0.4477\n","[epoch 6] loss: 1.0677, acc: 0.4483\n","[epoch 6] loss: 1.0678, acc: 0.4482\n","[epoch 6] loss: 1.0681, acc: 0.4480\n","[epoch 6] loss: 1.0675, acc: 0.4489\n","[epoch 6] loss: 1.0673, acc: 0.4492\n","[epoch 6] loss: 1.0670, acc: 0.4500\n","[epoch 6] loss: 1.0672, acc: 0.4495\n","[epoch 6] loss: 1.0677, acc: 0.4487\n","[epoch 6] loss: 1.0679, acc: 0.4476\n","[epoch 6] loss: 1.0675, acc: 0.4480\n","[epoch 6] loss: 1.0676, acc: 0.4477\n","[epoch 6] loss: 1.0680, acc: 0.4468\n","[epoch 6] loss: 1.0683, acc: 0.4468\n","[epoch 6] loss: 1.0683, acc: 0.4469\n","[epoch 6] loss: 1.0684, acc: 0.4468\n","[epoch 6] loss: 1.0685, acc: 0.4466\n","[epoch 6] loss: 1.0686, acc: 0.4469\n","[epoch 6] loss: 1.0685, acc: 0.4471\n","[epoch 6] loss: 1.0690, acc: 0.4464\n","[epoch 6] loss: 1.0685, acc: 0.4474\n","[epoch 6] loss: 1.0677, acc: 0.4489\n","[epoch 6] loss: 1.0673, acc: 0.4496\n","[epoch 6] loss: 1.0675, acc: 0.4495\n","[epoch 6] loss: 1.0670, acc: 0.4508\n","[epoch 6] loss: 1.0669, acc: 0.4511\n","[epoch 6] loss: 1.0669, acc: 0.4511\n","[epoch 6] loss: 1.0672, acc: 0.4507\n","> val_acc: 0.4543, val_f1: 0.2083\n",">> early stop.\n",">> test_acc: 0.4551, test_f1: 0.2100\n"]}]}]}