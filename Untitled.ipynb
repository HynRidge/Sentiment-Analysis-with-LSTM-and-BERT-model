{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (0.6.2)\n",
      "Requirement already satisfied: boto3 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.24.89)\n",
      "Requirement already satisfied: torch>=0.4.1 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from pytorch_pretrained_bert) (4.50.2)\n",
      "Requirement already satisfied: numpy in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.19.2)\n",
      "Requirement already satisfied: requests in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from pytorch_pretrained_bert) (2.24.0)\n",
      "Requirement already satisfied: regex in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from pytorch_pretrained_bert) (2020.10.15)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.89 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (1.27.89)\n",
      "Requirement already satisfied: typing-extensions in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.89->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/didi/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.89->boto3->pytorch_pretrained_bert) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "from xml.etree.ElementTree import parse\n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "ASPECT = '<aspect>'\n",
    "\n",
    "PAD_INDEX = 0\n",
    "UNK_INDEX = 1\n",
    "ASPECT_INDEX = 2\n",
    "\n",
    "INF = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._count_dict = dict()\n",
    "        self._predefined_list = [PAD, UNK, ASPECT]\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self._count_dict:\n",
    "            self._count_dict[word] += 1\n",
    "        else:\n",
    "            self._count_dict[word] = 1\n",
    "\n",
    "    def add_list(self, words):\n",
    "        for word in words:\n",
    "            self.add(word)\n",
    "\n",
    "    def get_vocab(self, max_size=None, min_freq=0):\n",
    "        sorted_words = sorted(self._count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        word2index = {}\n",
    "        for word in self._predefined_list:\n",
    "            word2index[word] = len(word2index)\n",
    "        for word, freq in sorted_words:\n",
    "            if word in word2index:\n",
    "                continue\n",
    "            if (max_size is not None and len(word2index) >= max_size) or freq < min_freq:\n",
    "                word2index[word] = word2index[UNK]\n",
    "            else:\n",
    "                word2index[word] = len(word2index)\n",
    "        index2word = {}\n",
    "        index2word[word2index[UNK]] = UNK\n",
    "        for word, index in word2index.items():\n",
    "            if index == word2index[UNK]:\n",
    "                continue\n",
    "            else:\n",
    "                index2word[index] = word\n",
    "        return word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_term(path, lowercase=False):\n",
    "    tree = parse(path)\n",
    "    sentences = tree.getroot()\n",
    "    data = []\n",
    "    split_char = '__split__'\n",
    "    for sentence in sentences:\n",
    "        text = sentence.find('text')\n",
    "        if text is None:\n",
    "            continue\n",
    "        text = text.text\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        aspectTerms = sentence.find('aspectTerms')\n",
    "        if aspectTerms is None:\n",
    "            continue\n",
    "        for aspectTerm in aspectTerms:\n",
    "            term = aspectTerm.get('term')\n",
    "            if lowercase:\n",
    "                term = term.lower()\n",
    "            polarity = aspectTerm.get('polarity')\n",
    "            start = aspectTerm.get('from')\n",
    "            end = aspectTerm.get('to')\n",
    "            piece = text + split_char + term + split_char + polarity + split_char + start + split_char + end\n",
    "            data.append(piece)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_filter(data, remove_list):\n",
    "    remove_set = set(remove_list)\n",
    "    filtered_data = []\n",
    "    for text in data:\n",
    "        if not text.split('__split__')[2] in remove_set:\n",
    "            filtered_data.append(text)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = re.compile('(<url>.*</url>)')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def check(x):\n",
    "    return len(x) >= 1 and not x.isspace()\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "    return list(filter(check, tokens))\n",
    "\n",
    "def build_vocab(data, max_size, min_freq):\n",
    "    if max_size == 'None':\n",
    "        max_size = None\n",
    "    vocab = Vocab()\n",
    "    for piece in data:\n",
    "        text = piece.split('__split__')[0]\n",
    "        text = tokenizer(text)\n",
    "        vocab.add_list(text)\n",
    "    return vocab.get_vocab(max_size=max_size, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def save_term_data(data, word2index, path):\n",
    "    dirname = os.path.dirname(path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    sentence = []\n",
    "    aspect = []\n",
    "    label = []\n",
    "    context = []\n",
    "    bert_token = []\n",
    "    bert_segment = []\n",
    "    td_left = []\n",
    "    td_right = []\n",
    "    f = lambda x: word2index[x] if x in word2index else word2index[UNK]\n",
    "    g = lambda x: list(map(f, tokenizer(x)))\n",
    "    d = {\n",
    "        'positive': 0,\n",
    "        'negative': 1,\n",
    "        'neutral': 2,\n",
    "        'conflict': 3\n",
    "    }\n",
    "    for piece in data:\n",
    "        text, term, polarity, start, end = piece.split('__split__')\n",
    "        start, end = int(start), int(end)\n",
    "        assert text[start: end] == term\n",
    "        sentence.append(g(text))\n",
    "        aspect.append(g(term))\n",
    "        label.append(d[polarity])\n",
    "        left_part = g(text[:start])\n",
    "        right_part = g(text[end:])\n",
    "        context.append(left_part + [ASPECT_INDEX] + right_part)\n",
    "        bert_sentence = bert_tokenizer.tokenize(text)\n",
    "        bert_aspect = bert_tokenizer.tokenize(term)\n",
    "        bert_token.append(bert_tokenizer.convert_tokens_to_ids(['[CLS]'] + bert_sentence + ['[SEP]'] + bert_aspect + ['[SEP]']))\n",
    "        bert_segment.append([0] * (len(bert_sentence) + 2) + [1] * (len(bert_aspect) + 1))\n",
    "        td_left.append(g(text[:end]))\n",
    "        td_right.append(g(text[start:])[::-1])\n",
    "        assert len(bert_token[-1]) == len(bert_segment[-1])\n",
    "    max_length = lambda x: max([len(y) for y in x])\n",
    "    sentence_max_len = max_length(sentence)\n",
    "    aspect_max_len = max_length(aspect)\n",
    "    context_max_len = max_length(context)\n",
    "    bert_max_len = max_length(bert_token)\n",
    "    td_left_max_len = max_length(td_left)\n",
    "    td_right_max_len = max_length(td_right)\n",
    "    num = len(data)\n",
    "    for i in range(num):\n",
    "        sentence[i].extend([0] * (sentence_max_len - len(sentence[i])))\n",
    "        aspect[i].extend([0] * (aspect_max_len - len(aspect[i])))\n",
    "        context[i].extend([0] * (context_max_len - len(context[i])))\n",
    "        bert_token[i].extend([0] * (bert_max_len - len(bert_token[i])))\n",
    "        bert_segment[i].extend([0] * (bert_max_len - len(bert_segment[i])))\n",
    "        td_left[i].extend([0] * (td_left_max_len - len(td_left[i])))\n",
    "        td_right[i].extend([0] * (td_right_max_len - len(td_right[i])))\n",
    "    sentence = np.asarray(sentence, dtype=np.int32)\n",
    "    aspect = np.asarray(aspect, dtype=np.int32)\n",
    "    label = np.asarray(label, dtype=np.int32)\n",
    "    context = np.asarray(context, dtype=np.int32)\n",
    "    bert_token = np.asarray(bert_token, dtype=np.int32)\n",
    "    bert_segment = np.asarray(bert_segment, dtype=np.int32)\n",
    "    td_left = np.asarray(td_left, dtype=np.int32)\n",
    "    td_right = np.asarray(td_right, dtype=np.int32)\n",
    "    np.savez(path, sentence=sentence, aspect=aspect, label=label, context=context, bert_token=bert_token, bert_segment=bert_segment,\n",
    "             td_left=td_left, td_right=td_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(path, vocab_size, word2index):\n",
    "    if not os.path.isfile(path):\n",
    "        raise IOError('Not a file', path)\n",
    "    glove = np.random.uniform(-0.01, 0.01, [vocab_size, 300])\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            content = line.split(' ')\n",
    "            if content[0] in word2index:\n",
    "                glove[word2index[content[0]]] = np.array(list(map(float, content[1:])))\n",
    "    glove[PAD_INDEX, :] = 0\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_matrix(glove_path, sentiment_path):\n",
    "    sentiment_matrix = np.zeros((3, 300), dtype=np.float32)\n",
    "    sd = json.load(open(sentiment_path, 'r', encoding='utf-8'))\n",
    "    sd['positive'] = set(sd['positive'])\n",
    "    sd['negative'] = set(sd['negative'])\n",
    "    sd['neutral'] = set(sd['neutral'])\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            content = line.split(' ')\n",
    "            word = content[0]\n",
    "            vec = np.array(list(map(float, content[1:])))\n",
    "            if word in sd['positive']:\n",
    "                sentiment_matrix[0] += vec\n",
    "            elif word in sd['negative']:\n",
    "                sentiment_matrix[1] += vec\n",
    "            elif word in sd['neutral']:\n",
    "                sentiment_matrix[2] += vec\n",
    "    sentiment_matrix -= sentiment_matrix.mean()\n",
    "    sentiment_matrix = sentiment_matrix / sentiment_matrix.std() * np.sqrt(2.0 / (300.0 + 3.0))\n",
    "    return sentiment_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_term(data):\n",
    "    num = len(data)\n",
    "    sentence_lens = []\n",
    "    aspect_lens = []\n",
    "    log = {'total': num}\n",
    "    for piece in data:\n",
    "        text, term, polarity, _, _ = piece.split('__split__')\n",
    "        sentence_lens.append(len(tokenizer(text)))\n",
    "        aspect_lens.append(len(tokenizer(term)))\n",
    "        if not polarity in log:\n",
    "            log[polarity] = 0\n",
    "        log[polarity] += 1\n",
    "    log['sentence_max_len'] = max(sentence_lens)\n",
    "    log['sentence_avg_len'] = sum(sentence_lens) / len(sentence_lens)\n",
    "    log['aspect_max_len'] = max(aspect_lens)\n",
    "    log['aspect_avg_len'] = sum(aspect_lens) / len(aspect_lens)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.sentence_clip file\n",
    "\n",
    "def sentence_clip(sentence):\n",
    "    mask = (sentence != PAD_INDEX)\n",
    "    sentence_lens = mask.long().sum(dim=1, keepdim=False)\n",
    "    max_len = sentence_lens.max().item()\n",
    "    return sentence[:, :max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.squash file\n",
    "import torch\n",
    "\n",
    "def squash(x, dim=-1):\n",
    "    squared = torch.sum(x * x, dim=dim, keepdim=True)\n",
    "    scale = torch.sqrt(squared) / (1.0 + squared)\n",
    "    return scale * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    The base class of attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: FloatTensor (batch_size, query_size) or FloatTensor (batch_size, num_queries, query_size)\n",
    "        key: FloatTensor (batch_size, time_step, key_size)\n",
    "        value: FloatTensor (batch_size, time_step, hidden_size)\n",
    "        mask: ByteTensor (batch_size, time_step) or ByteTensor (batch_size, num_queries, time_step)\n",
    "        \"\"\"\n",
    "        single_query = False\n",
    "        if len(query.size()) == 2:\n",
    "            query = query.unsqueeze(1)\n",
    "            single_query = True\n",
    "        if mask is not None:\n",
    "            if len(mask.size()) == 2:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            else:\n",
    "                assert mask.size(1) == query.size(1)\n",
    "        score = self._score(query, key) # FloatTensor (batch_size, num_queries, time_step)\n",
    "        weights = self._weights_normalize(score, mask)\n",
    "        weights = F.dropout(weights, p=self.dropout, training=self.training)\n",
    "        output = weights.matmul(value)\n",
    "        if single_query:\n",
    "            output = output.squeeze(1)\n",
    "        return output\n",
    "\n",
    "    def _score(self, query, key):\n",
    "        raise NotImplementedError('Attention score method is not implemented.')\n",
    "\n",
    "    def _weights_normalize(self, score, mask):\n",
    "        if not mask is None:\n",
    "            score = score.masked_fill(mask == 0, -constants.INF)\n",
    "        weights = F.softmax(score, dim=-1)\n",
    "        return weights\n",
    "\n",
    "    def get_attention_weights(self, query, key, mask=None):\n",
    "        single_query = False\n",
    "        if len(query.size()) == 2:\n",
    "            query = query.unsqueeze(1)\n",
    "            single_query = True\n",
    "        if mask is not None:\n",
    "            if len(mask.size()) == 2:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            else:\n",
    "                assert mask.size(1) == query.size(1)\n",
    "        score = self._score(query, key)  # FloatTensor (batch_size, num_queries, time_step)\n",
    "        weights = self._weights_normalize(score, mask)\n",
    "        weights = F.dropout(weights, p=self.dropout, training=self.training)\n",
    "        if single_query:\n",
    "            weights = weights.squeeze(1)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class BilinearAttention(Attention):\n",
    "\n",
    "    def __init__(self, query_size, key_size, dropout=0):\n",
    "        super(BilinearAttention, self).__init__(dropout)\n",
    "        self.weights = nn.Parameter(torch.FloatTensor(query_size, key_size))\n",
    "        init.xavier_uniform_(self.weights)\n",
    "\n",
    "    def _score(self, query, key):\n",
    "        \"\"\"\n",
    "        query: FloatTensor (batch_size, num_queries, query_size)\n",
    "        key: FloatTensor (batch_size, time_step, key_size)\n",
    "        \"\"\"\n",
    "        score = query.matmul(self.weights).matmul(key.transpose(1, 2))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class BertCapsuleNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, bert, bert_size, capsule_size, dropout, num_categories):\n",
    "        super(BertCapsuleNetwork, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.bert_size = bert_size\n",
    "        self.capsule_size = capsule_size\n",
    "        self.aspect_transform = nn.Sequential(\n",
    "            nn.Linear(bert_size, capsule_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.sentence_transform = nn.Sequential(\n",
    "            nn.Linear(bert_size, capsule_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm_attention = BilinearAttention(capsule_size, capsule_size)\n",
    "        self.guide_capsule = nn.Parameter(\n",
    "            torch.Tensor(num_categories, capsule_size)\n",
    "        )\n",
    "        self.guide_weight = nn.Parameter(\n",
    "            torch.Tensor(capsule_size, capsule_size)\n",
    "        )\n",
    "        self.scale = nn.Parameter(torch.tensor(5.0))\n",
    "        self.capsule_projection = nn.Linear(bert_size, bert_size * num_categories)\n",
    "        self.dropout = dropout\n",
    "        self.num_categories = num_categories\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        init.xavier_uniform_(self.guide_capsule)\n",
    "        init.xavier_uniform_(self.guide_weight)\n",
    "\n",
    "    def load_sentiment(self, path):\n",
    "        sentiment = np.load(path)\n",
    "        e1 = np.mean(sentiment)\n",
    "        d1 = np.std(sentiment)\n",
    "        e2 = 0\n",
    "        d2 = np.sqrt(2.0 / (sentiment.shape[0] + sentiment.shape[1]))\n",
    "        sentiment = (sentiment - e1) / d1 * d2 + e2\n",
    "        self.guide_capsule.data.copy_(torch.tensor(sentiment))\n",
    "\n",
    "    def forward(self, bert_token, bert_segment):\n",
    "        # BERT encoding\n",
    "        encoder_layer, _ = self.bert(bert_token, bert_segment, output_all_encoded_layers=False)\n",
    "        batch_size, segment_len = bert_segment.size()\n",
    "        max_segment_len = bert_segment.argmax(dim=-1, keepdim=True)\n",
    "        batch_arrange = torch.arange(segment_len).unsqueeze(0).expand(batch_size, segment_len).to(bert_segment.device)\n",
    "        segment_mask = batch_arrange <= max_segment_len\n",
    "        sentence_mask = segment_mask & (1 - bert_segment).byte()\n",
    "        aspect_mask = bert_segment\n",
    "        sentence_lens = sentence_mask.long().sum(dim=1, keepdim=True)\n",
    "        # aspect average pooling\n",
    "        aspect_lens = aspect_mask.long().sum(dim=1, keepdim=True)\n",
    "        aspect = encoder_layer.masked_fill(aspect_mask.unsqueeze(-1) == 0, 0)\n",
    "        aspect = aspect.sum(dim=1, keepdim=False) / aspect_lens.float()\n",
    "        # sentence encode layer\n",
    "        max_len = sentence_lens.max().item()\n",
    "        sentence = encoder_layer[:, 0: max_len].contiguous()\n",
    "        sentence_mask = sentence_mask[:, 0: max_len].contiguous()\n",
    "        sentence = sentence.masked_fill(sentence_mask.unsqueeze(-1) == 0, 0)\n",
    "        # primary capsule layer\n",
    "        sentence = self.sentence_transform(sentence)\n",
    "        primary_capsule = squash(sentence, dim=-1)\n",
    "        aspect = self.aspect_transform(aspect)\n",
    "        aspect_capsule = squash(aspect, dim=-1)\n",
    "        # aspect aware normalization\n",
    "        norm_weight = self.norm_attention.get_attention_weights(aspect_capsule, primary_capsule, sentence_mask)\n",
    "        # capsule guided routing\n",
    "        category_capsule = self._capsule_guided_routing(primary_capsule, norm_weight)\n",
    "        category_capsule_norm = torch.sqrt(torch.sum(category_capsule * category_capsule, dim=-1, keepdim=False))\n",
    "        return category_capsule_norm\n",
    "\n",
    "    def _capsule_guided_routing(self, primary_capsule, norm_weight):\n",
    "        guide_capsule = squash(self.guide_capsule)\n",
    "        guide_matrix = primary_capsule.matmul(self.guide_weight).matmul(guide_capsule.transpose(0, 1))\n",
    "        guide_matrix = F.softmax(guide_matrix, dim=-1)\n",
    "        guide_matrix = guide_matrix * norm_weight.unsqueeze(-1) * self.scale  # (batch_size, time_step, num_categories)\n",
    "        category_capsule = guide_matrix.transpose(1, 2).matmul(primary_capsule)\n",
    "        category_capsule = F.dropout(category_capsule, p=self.dropout, training=self.training)\n",
    "        category_capsule = squash(category_capsule)\n",
    "        return category_capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, input_list):\n",
    "        super(ABSADataset, self).__init__()\n",
    "        data = np.load(path)\n",
    "        self.data = {}\n",
    "        for key, value in data.items():\n",
    "            self.data[key] = torch.tensor(value).long()\n",
    "        self.len = self.data['label'].size(0)\n",
    "        self.input_list = input_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return_value = []\n",
    "        for input in self.input_list:\n",
    "            return_value.append(self.data[input][index])\n",
    "        return_value.append(self.data['label'][index])\n",
    "        return return_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, smooth=0.1, lamda=0.6):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        one_hot = torch.zeros_like(input).to(input.device)\n",
    "        one_hot = one_hot.scatter(1, target.unsqueeze(-1), 1)\n",
    "        a = torch.max(torch.zeros_like(input).to(input.device), 1 - self.smooth - input)\n",
    "        b = torch.max(torch.zeros_like(input).to(input.device), input - self.smooth)\n",
    "        loss = one_hot * a * a + self.lamda * (1 - one_hot) * b * b\n",
    "        loss = loss.sum(dim=1, keepdim=False)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    base_path = os.path.join(config['base_path'])\n",
    "    log_path = os.path.join(base_path, 'log/log.yml')\n",
    "    log = yaml.safe_load(open(log_path))\n",
    "    bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model = BertCapsuleNetwork(\n",
    "        bert=bert,\n",
    "        bert_size=config2['bert_size'],\n",
    "        capsule_size=config2['capsule_size'],\n",
    "        dropout=config2['dropout'],\n",
    "        num_categories=log['num_categories']\n",
    "    )\n",
    "    model.load_sentiment(os.path.join(base_path, 'processed/sentiment_matrix.npy'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def make_term_data():\n",
    "    base_path = config['base_path']\n",
    "    train_path = os.path.join(base_path, 'processed/train.npz')\n",
    "    val_path = os.path.join(base_path, 'processed/val.npz')\n",
    "    train_data = ABSADataset(train_path, ['bert_token', 'bert_segment'])\n",
    "    val_data = ABSADataset(val_path, ['bert_token', 'bert_segment'])\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=config2['batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_data,\n",
    "        batch_size=config2['batch_size'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import adabound\n",
    "\n",
    "def make_optimizer(model):\n",
    "    lr = config2['learning_rate']\n",
    "    weight_decay = config2['weight_decay']\n",
    "    opt = {\n",
    "        'sgd': optim.SGD,\n",
    "        'adadelta': optim.Adadelta,\n",
    "        'adam': optim.Adam,\n",
    "        'adamax': optim.Adamax,\n",
    "        'adagrad': optim.Adagrad,\n",
    "        'asgd': optim.ASGD,\n",
    "        'rmsprop': optim.RMSprop,\n",
    "        'adabound': adabound.AdaBound\n",
    "    }\n",
    "    if 'momentum' in config:\n",
    "        optimizer = opt[config2['optimizer']](model.parameters(), lr=lr, weight_decay=weight_decay, momentum=config2['momentum'])\n",
    "    else:\n",
    "        optimizer = opt[config2['optimizer']](model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loader, val_loader = make_term_data()\n",
    "    global model\n",
    "#     model = model.cuda()\n",
    "    base_path = config['base_path']\n",
    "    model_path = os.path.join(base_path, 'checkpoints/%s.pth' % \"bert_capsnet\")\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "    with open(os.path.join(base_path, 'processed/index2word.pickle'), 'rb') as handle:\n",
    "        index2word = pickle.load(handle)\n",
    "    criterion = CapsuleLoss()\n",
    "    optimizer = make_optimizer(model)\n",
    "    max_val_accuracy = 0\n",
    "    min_val_loss = 100\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config2['num_epoches']):\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_samples = 0\n",
    "        start = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "            model.train()\n",
    "            input0, input1, label = data\n",
    "#             input0, input1, label = input0.cuda(), input1.cuda(), label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(input0, input1)\n",
    "            loss = criterion(logit, label)\n",
    "            batch_size = input0.size(0)\n",
    "            total_loss += batch_size * loss.item()\n",
    "            total_samples += batch_size\n",
    "            pred = logit.argmax(dim=1)\n",
    "            correct_samples += (label == pred).long().sum().item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                train_loss = total_loss / total_samples\n",
    "                train_accuracy = correct_samples / total_samples\n",
    "                total_loss = 0\n",
    "                total_samples = 0\n",
    "                correct_samples = 0\n",
    "                val_accuracy, val_loss = eval(model, val_loader, criterion)\n",
    "                print('[epoch %2d] [step %3d] train_loss: %.4f train_acc: %.4f val_loss: %.4f val_acc: %.4f'\n",
    "                      % (epoch, i, train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "                if val_accuracy > max_val_accuracy:\n",
    "                    max_val_accuracy = val_accuracy\n",
    "                    # torch.save(aspect_term_model.state_dict(), model_path)\n",
    "                if val_loss < min_val_loss:\n",
    "                    min_val_loss = val_loss\n",
    "                    if epoch > 0:\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "        end = time.time()\n",
    "        print('time: %.4fs' % (end - start))\n",
    "    print('max_val_accuracy:', max_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'base_path': \"./MAMS-ATSA\",\n",
    "            \"mode\": \"term\",\n",
    "            \"base_path\": \"./MAMS-ATSA\",\n",
    "            \"glove_path\": \"./glove.840B.300d.txt\",\n",
    "            \"sentiment_path\": \"./sentiment_dict.json\",\n",
    "            \"max_vocab_size\": None,\n",
    "            \"min_vocab_freq\": 0\n",
    "         }\n",
    "config2 = {\n",
    "            'bert_size': 768,\n",
    "            'capsule_size': 300,\n",
    "            'dropout': 0.1,\n",
    "            'optimizer': 'adam',\n",
    "            'batch_size': 32,\n",
    "            'learning_rate': 0.00002,\n",
    "            'weight_decay': 0,\n",
    "            'num_epoches': 5,\n",
    "            'gpu': 0,\n",
    "         }\n",
    "lowercase = True\n",
    "config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n",
    "config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n",
    "config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = parse_sentence_term(config['raw_train_path'], lowercase=lowercase)\n",
    "val_data = parse_sentence_term(config['raw_val_path'], lowercase=lowercase)\n",
    "test_data = parse_sentence_term(config['raw_test_path'], lowercase=lowercase)\n",
    "\n",
    "remove_list = ['conflict']\n",
    "train_data = category_filter(train_data, remove_list)\n",
    "val_data = category_filter(val_data, remove_list)\n",
    "test_data = category_filter(test_data, remove_list)\n",
    "\n",
    "word2index, index2word = build_vocab(train_data, max_size=config['max_vocab_size'], min_freq=config['min_vocab_freq'])\n",
    "\n",
    "if not os.path.exists(os.path.join(config[\"base_path\"], 'processed')):\n",
    "    os.makedirs(os.path.join(config[\"base_path\"], 'processed'))\n",
    "        \n",
    "save_term_data(train_data, word2index, os.path.join(config[\"base_path\"], 'processed/train.npz'))\n",
    "save_term_data(val_data, word2index, os.path.join(config[\"base_path\"], 'processed/val.npz'))\n",
    "save_term_data(test_data, word2index, os.path.join(config[\"base_path\"], 'processed/test.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = load_glove(config['glove_path'], len(index2word), word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_matrix = load_sentiment_matrix(config['glove_path'], config['sentiment_path'])\n",
    "np.save(os.path.join(config[\"base_path\"], 'processed/glove.npy'), glove)\n",
    "np.save(os.path.join(config[\"base_path\"], 'processed/sentiment_matrix.npy'), sentiment_matrix)\n",
    "with open(os.path.join(config[\"base_path\"], 'processed/word2index.pickle'), 'wb') as handle:\n",
    "    pickle.dump(word2index, handle)\n",
    "with open(os.path.join(config[\"base_path\"], 'processed/index2word.pickle'), 'wb') as handle:\n",
    "    pickle.dump(index2word, handle)\n",
    "analyze = analyze_term\n",
    "log = {\n",
    "    'vocab_size': len(index2word),\n",
    "    'oov_size': len(word2index) - len(index2word),\n",
    "    'train_data': analyze(train_data),\n",
    "    'val_data': analyze(val_data),\n",
    "    'test_data': analyze(test_data),\n",
    "    'num_categories': 3\n",
    "}\n",
    "\n",
    "if not os.path.exists(os.path.join(config[\"base_path\"], 'log')):\n",
    "    os.makedirs(os.path.join(config[\"base_path\"], 'log'))\n",
    "with open(os.path.join(config[\"base_path\"], 'log/log.yml'), 'w') as handle:\n",
    "    yaml.safe_dump(log, handle, encoding='utf-8', allow_unicode=True, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
