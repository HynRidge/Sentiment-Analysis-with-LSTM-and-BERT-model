{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ACKNOWLEDGEMENT**"
      ],
      "metadata": {
        "id": "qVNZSg3UV5sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is inspired by https://github.com/songyouwei/ABSA-PyTorch."
      ],
      "metadata": {
        "id": "x6ytAZbKWC1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SETUP**"
      ],
      "metadata": {
        "id": "5zXJrSMo495u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "8_7Yx7y-5BOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tYF42jlo5Q4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53dacbd-cec7-4888-a2f8-a1aa4b2027e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSD-dTWt28EH",
        "outputId": "6afb3eaa-7978-4f53-cb63-6567477ff75c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "eTERBhPf5Ykk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import strftime, localtime\n",
        "from transformers import BertTokenizer\n",
        "from sklearn import metrics\n",
        "from xml.etree.ElementTree import parse\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "ZEPQFq-e6gVp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "6674vF4n6le1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASSES**"
      ],
      "metadata": {
        "id": "IDenOuxLpJFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**"
      ],
      "metadata": {
        "id": "USX4iChw67d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, fname, tokenizer):\n",
        "        fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "        lines = fin.readlines()\n",
        "        fin.close()\n",
        "        fin = open(fname+'.graph', 'rb')\n",
        "        idx2graph = pickle.load(fin)\n",
        "        fin.close()\n",
        "\n",
        "        all_data = []\n",
        "        for i in range(0, len(lines), 3):\n",
        "            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "            aspect = lines[i + 1].lower().strip()\n",
        "            polarity = lines[i + 2].strip()\n",
        "\n",
        "            text_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
        "            context_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
        "            left_indices = tokenizer.text_to_sequence(text_left)\n",
        "            left_with_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect)\n",
        "            right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
        "            right_with_aspect_indices = tokenizer.text_to_sequence(aspect + \" \" + text_right, reverse=True)\n",
        "            aspect_indices = tokenizer.text_to_sequence(aspect)\n",
        "            left_len = np.sum(left_indices != 0)\n",
        "            aspect_len = np.sum(aspect_indices != 0)\n",
        "            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype=np.int64)\n",
        "            polarity = int(polarity) + 1\n",
        "\n",
        "            text_len = np.sum(text_indices != 0)\n",
        "            concat_segments_indices = [0] * (text_len + 2) + [1] * (aspect_len + 1)\n",
        "            concat_segments_indices = pad_and_truncate(concat_segments_indices, tokenizer.max_seq_len)\n",
        "\n",
        "            dependency_graph = np.pad(idx2graph[i], \\\n",
        "                ((0,tokenizer.max_seq_len-idx2graph[i].shape[0]),(0,tokenizer.max_seq_len-idx2graph[i].shape[0])), 'constant')\n",
        "\n",
        "            data = {\n",
        "                'text_indices': text_indices,\n",
        "                'context_indices': context_indices,\n",
        "                'left_indices': left_indices,\n",
        "                'left_with_aspect_indices': left_with_aspect_indices,\n",
        "                'right_indices': right_indices,\n",
        "                'right_with_aspect_indices': right_with_aspect_indices,\n",
        "                'aspect_indices': aspect_indices,\n",
        "                'aspect_boundary': aspect_boundary,\n",
        "                'dependency_graph': dependency_graph,\n",
        "                'polarity': polarity,\n",
        "            }\n",
        "\n",
        "            all_data.append(data)\n",
        "        self.data = all_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "julOof-E-0Ry"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, max_seq_len, lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 1\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unknownidx = len(self.word2idx)+1\n",
        "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)"
      ],
      "metadata": {
        "id": "t_BKpmMdqNuW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split()\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "H3kxztDgDc2t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "fuiAR0ng6vID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n",
        "                 bidirectional=False, only_use_last_hidden_state=False, rnn_type = 'LSTM'):\n",
        "        \"\"\"\n",
        "        LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, length...).\n",
        "        :param input_size:The number of expected features in the input x\n",
        "        :param hidden_size:The number of features in the hidden state h\n",
        "        :param num_layers:Number of recurrent layers.\n",
        "        :param bias:If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "        :param batch_first:If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "        :param dropout:If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
        "        :param bidirectional:If True, becomes a bidirectional RNN. Default: False\n",
        "        :param rnn_type: {LSTM, GRU, RNN}\n",
        "        \"\"\"\n",
        "        super(DynamicLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.batch_first = batch_first\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.only_use_last_hidden_state = only_use_last_hidden_state\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        if self.rnn_type == 'LSTM': \n",
        "            self.RNN = nn.LSTM(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)  \n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.RNN = nn.GRU(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
        "        elif self.rnn_type == 'RNN':\n",
        "            self.RNN = nn.RNN(\n",
        "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack ->process using RNN -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
        "        \n",
        "        # process using the selected RNN\n",
        "        if self.rnn_type == 'LSTM': \n",
        "            out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n",
        "        else: \n",
        "            out_pack, ht = self.RNN(x_emb_p, None)\n",
        "            ct = None\n",
        "        \"\"\"unsort: h\"\"\"\n",
        "        ht = torch.transpose(ht, 0, 1)[\n",
        "            x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n",
        "        ht = torch.transpose(ht, 0, 1)\n",
        "\n",
        "        if self.only_use_last_hidden_state:\n",
        "            return ht\n",
        "        else:\n",
        "            \"\"\"unpack: out\"\"\"\n",
        "            out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "            out = out[0]  #\n",
        "            out = out[x_unsort_idx]\n",
        "            \"\"\"unsort: out c\"\"\"\n",
        "            if self.rnn_type =='LSTM':\n",
        "                ct = torch.transpose(ct, 0, 1)[\n",
        "                    x_unsort_idx]  # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n",
        "                ct = torch.transpose(ct, 0, 1)\n",
        "\n",
        "            return out, (ht, ct)"
      ],
      "metadata": {
        "id": "qQsLY_w-8Bep"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ATAE_LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(ATAE_LSTM, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.squeeze_embedding = SqueezeEmbedding()\n",
        "        self.lstm = DynamicLSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=3, batch_first=True)\n",
        "        self.attention = NoQueryAttention(config2[\"hidden_dim\"]+config2[\"embed_dim\"], score_function='bi_linear')\n",
        "        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        text_indices, aspect_indices = inputs[0], inputs[1]\n",
        "        x_len = torch.sum(text_indices != 0, dim=-1)\n",
        "        x_len_max = torch.max(x_len)\n",
        "        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n",
        "\n",
        "        x = self.embed(text_indices)\n",
        "        x = self.squeeze_embedding(x, x_len)\n",
        "        aspect = self.embed(aspect_indices)\n",
        "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n",
        "        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n",
        "        x = torch.cat((aspect, x), dim=-1)\n",
        "\n",
        "        h, (_, _) = self.lstm(x, x_len)\n",
        "        ha = torch.cat((h, aspect), dim=-1)\n",
        "        _, score = self.attention(ha)\n",
        "        output = torch.squeeze(torch.bmm(score, h), dim=1)\n",
        "\n",
        "        out = self.dense(output)\n",
        "        return out"
      ],
      "metadata": {
        "id": "V3WeKKJK8ES1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vDMVKKkHmnJP"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):\n",
        "        ''' Attention Mechanism\n",
        "        :param embed_dim:\n",
        "        :param hidden_dim:\n",
        "        :param out_dim:\n",
        "        :param n_head: num of head (Multi-Head Attention)\n",
        "        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)\n",
        "        :return (?, q_len, out_dim,)\n",
        "        '''\n",
        "        super(Attention, self).__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = embed_dim // n_head\n",
        "        if out_dim is None:\n",
        "            out_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_head = n_head\n",
        "        self.score_function = score_function\n",
        "        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if score_function == 'mlp':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        else:  # dot_product / scaled_dot_product\n",
        "            self.register_parameter('weight', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.hidden_dim)\n",
        "        if self.weight is not None:\n",
        "            self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, q):\n",
        "        if len(q.shape) == 2:  # q_len missing\n",
        "            q = torch.unsqueeze(q, dim=1)\n",
        "        if len(k.shape) == 2:  # k_len missing\n",
        "            k = torch.unsqueeze(k, dim=1)\n",
        "        mb_size = k.shape[0]  # ?\n",
        "        k_len = k.shape[1]\n",
        "        q_len = q.shape[1]\n",
        "        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n",
        "        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n",
        "        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n",
        "        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n",
        "        if self.score_function == 'dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qx, kt)\n",
        "        elif self.score_function == 'scaled_dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            qkt = torch.bmm(qx, kt)\n",
        "            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n",
        "        elif self.score_function == 'mlp':\n",
        "            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)\n",
        "            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)\n",
        "            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)\n",
        "            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)\n",
        "            score = F.tanh(torch.matmul(kq, self.weight))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            qw = torch.matmul(qx, self.weight)\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qw, kt)\n",
        "        else:\n",
        "            raise RuntimeError('invalid score_function')\n",
        "        score = F.softmax(score, dim=-1)\n",
        "        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)\n",
        "        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)\n",
        "        output = self.proj(output)  # (?, q_len, out_dim)\n",
        "        output = self.dropout(output)\n",
        "        return output, score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoQueryAttention(Attention):\n",
        "    '''q is a parameter'''\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', q_len=1, dropout=0):\n",
        "        super(NoQueryAttention, self).__init__(embed_dim, hidden_dim, out_dim, n_head, score_function, dropout)\n",
        "        self.q_len = q_len\n",
        "        self.q = nn.Parameter(torch.Tensor(q_len, embed_dim))\n",
        "        self.reset_q()\n",
        "\n",
        "    def reset_q(self):\n",
        "        stdv = 1. / math.sqrt(self.embed_dim)\n",
        "        self.q.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, **kwargs):\n",
        "        mb_size = k.shape[0]\n",
        "        q = self.q.expand(mb_size, -1, -1)\n",
        "        return super(NoQueryAttention, self).forward(k, q)"
      ],
      "metadata": {
        "id": "aOhD2c_x8MeN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze sequence embedding length to the longest one in the batch\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_first=True):\n",
        "        super(SqueezeEmbedding, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n",
        "        \"\"\"unpack: out\"\"\"\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "        out = out[0]  #\n",
        "        \"\"\"unsort\"\"\"\n",
        "        out = out[x_unsort_idx]\n",
        "        return out"
      ],
      "metadata": {
        "id": "7TlMbCBN8HdH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main**"
      ],
      "metadata": {
        "id": "6BmgF1FSp32r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Instructor:\n",
        "    def __init__(self):\n",
        "        tokenizer = build_tokenizer(\n",
        "            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n",
        "            max_seq_len=config2[\"max_seq_len\"],\n",
        "            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n",
        "        embedding_matrix = build_embedding_matrix(\n",
        "            word2idx=tokenizer.word2idx,\n",
        "            embed_dim=config2[\"embed_dim\"],\n",
        "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n",
        "        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n",
        "\n",
        "        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n",
        "        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n",
        "        self.valset = ABSADataset(config2[\"dataset_file\"]['val'], tokenizer)\n",
        "\n",
        "        if config2[\"device\"].type == 'cuda':\n",
        "            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n",
        "        self._print_args()\n",
        "\n",
        "    def _print_args(self):\n",
        "        n_trainable_params, n_nontrainable_params = 0, 0\n",
        "        for p in self.model.parameters():\n",
        "            n_params = torch.prod(torch.tensor(p.shape))\n",
        "            if p.requires_grad:\n",
        "                n_trainable_params += n_params\n",
        "            else:\n",
        "                n_nontrainable_params += n_params\n",
        "        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
        "\n",
        "\n",
        "    def _reset_params(self):\n",
        "        for child in self.model.children():\n",
        "            for p in child.parameters():\n",
        "                if p.requires_grad:\n",
        "                    if len(p.shape) > 1:\n",
        "                        config2[\"initializer\"](p)\n",
        "                    else:\n",
        "                        stdv = 1. / math.sqrt(p.shape[0])\n",
        "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "\n",
        "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
        "        max_val_acc = 0\n",
        "        max_val_f1 = 0\n",
        "        max_val_epoch = 0\n",
        "        global_step = 0\n",
        "        path = None\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        for i_epoch in range(config2[\"num_epoch\"]):\n",
        "            print('>' * 100)\n",
        "            print('epoch: {}'.format(i_epoch))\n",
        "            n_correct, n_total, loss_total = 0, 0, 0\n",
        "            # switch model to training mode\n",
        "            self.model.train()\n",
        "            for i_batch, batch in enumerate(train_data_loader):\n",
        "                global_step += 1\n",
        "                # clear gradient accumulators\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                global outputs, targets\n",
        "                outputs = self.model(inputs)\n",
        "                targets = batch['polarity'].to(config2[\"device\"])\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
        "                n_total += len(outputs)\n",
        "                loss_total += loss.item() * len(outputs)\n",
        "                if global_step % config2[\"log_step\"] == 0:\n",
        "                    train_acc = n_correct / n_total\n",
        "                    train_loss = loss_total / n_total\n",
        "                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n",
        "\n",
        "            val_acc, val_loss, _, val_f1, _, _, _, _, _, _, _, _ = self._evaluate(criterion, val_data_loader)\n",
        "            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                max_val_epoch = i_epoch\n",
        "                if not os.path.exists('state_dict'):\n",
        "                    os.mkdir('state_dict')\n",
        "                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n",
        "                torch.save(self.model.state_dict(), path)\n",
        "                print('>> saved: {}'.format(path))\n",
        "            if val_f1 > max_val_f1:\n",
        "                max_val_f1 = val_f1\n",
        "            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n",
        "                print('>> early stop.')\n",
        "                break\n",
        "            train_losses.append(loss_total / n_total)\n",
        "            train_accuracies.append(n_correct / n_total)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        return path, train_losses, train_accuracies, val_losses, val_accuracies\n",
        "\n",
        "    def _evaluate(self, criterion, data_loader):\n",
        "        n_correct, n_total, loss_total = 0, 0, 0\n",
        "        t_targets_all, t_outputs_all = None, None\n",
        "        # switch model to evaluation mode\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i_batch, t_batch in enumerate(data_loader):\n",
        "                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                t_targets = t_batch['polarity'].to(config2[\"device\"])\n",
        "                t_outputs = self.model(t_inputs)\n",
        "\n",
        "                loss = criterion(t_outputs, t_targets)\n",
        "\n",
        "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
        "                n_total += len(t_outputs)\n",
        "                loss_total += loss.item() * len(t_outputs)\n",
        "\n",
        "                if t_targets_all is None:\n",
        "                    t_targets_all = t_targets\n",
        "                    t_outputs_all = t_outputs\n",
        "                else:\n",
        "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
        "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
        "\n",
        "        acc = n_correct / n_total\n",
        "        loss = loss_total / n_total\n",
        "        f1_macro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
        "        f1_micro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='micro')\n",
        "        f1_weighted = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='weighted')\n",
        "        precision_macro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        precision_micro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        precision_weighted = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        recall_macro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        recall_micro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        recall_weighted = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        confusion_matrix = metrics.confusion_matrix(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2])\n",
        "        return acc, loss, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix\n",
        "\n",
        "    def run(self):\n",
        "        # Loss and Optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n",
        "\n",
        "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n",
        "        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "\n",
        "        self._reset_params()\n",
        "        best_model_path, train_losses, train_accuracies, val_losses, val_accuracies = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "        acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = self._evaluate(nn.CrossEntropyLoss(), test_data_loader)\n",
        "        print('>> test_acc: {:.4f}'.format(acc))\n",
        "        print('>> test_f1_macro: {:.4f}'.format(f1_macro))\n",
        "        print('>> test_f1_micro: {:.4f}'.format(f1_micro))\n",
        "        print('>> test_f1_weighted: {:.4f}'.format(f1_weighted))\n",
        "        print('>> test_precision_macro: {:.4f}'.format(precision_macro))\n",
        "        print('>> test_precision_micro: {:.4f}'.format(precision_micro))\n",
        "        print('>> test_precision_weighted: {:.4f}'.format(precision_weighted))\n",
        "        print('>> test_recall_macro: {:.4f}'.format(recall_macro))\n",
        "        print('>> test_recall_micro: {:.4f}'.format(recall_micro))\n",
        "        print('>> test_recall_weighted: {:.4f}'.format(recall_weighted))\n",
        "        print('confusion matrix:')\n",
        "        print(confusion_matrix)\n",
        "\n",
        "        epochs = len(train_losses)\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        for i, metrics in enumerate(zip([train_losses, train_accuracies], [val_losses, val_accuracies], ['Loss', 'Accuracy'])):\n",
        "            plt.subplot(1, 2, i + 1)\n",
        "            plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "            plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "            plt.legend()\n",
        "        plt.show()\n",
        "        plt.savefig('lstm_accuracy.png')\n",
        "        return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "mPRxQS-Jp5vP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "bv09Mo677CkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_format_sentences(source_path, target_path):\n",
        "    f = open(target_path, \"w\")\n",
        "\n",
        "    sentences = parse(source_path).getroot()\n",
        "    preprocessed = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            if polarity == \"positive\":\n",
        "                polarity = '1'\n",
        "            elif polarity == \"neutral\":\n",
        "                polarity = '0'\n",
        "            elif polarity == \"negative\":\n",
        "                polarity = '-1'\n",
        "            else:\n",
        "                raise Exception(\"invalid polarity!\")\n",
        "            start = int(aspectTerm.get('from'))\n",
        "            end = int(aspectTerm.get('to'))\n",
        "            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n",
        "            preprocessed.append(text[start:end])\n",
        "            preprocessed.append(polarity)\n",
        "    f.write(\"\\n\".join(preprocessed))"
      ],
      "metadata": {
        "id": "Mlhjev_wrkDF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading tokenizer:', dat_fname)\n",
        "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        text = ''\n",
        "        for fname in fnames:\n",
        "            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "            lines = fin.readlines()\n",
        "            fin.close()\n",
        "            for i in range(0, len(lines), 3):\n",
        "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "                aspect = lines[i + 1].lower().strip()\n",
        "                text_raw = text_left + \" \" + aspect + \" \" + text_right\n",
        "                text += text_raw + \" \"\n",
        "\n",
        "        tokenizer = Tokenizer(max_seq_len)\n",
        "        tokenizer.fit_on_text(text)\n",
        "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "pV174mP37UZy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_word_vec(path, word2idx=None, embed_dim=300):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n",
        "        if word in word2idx.keys():\n",
        "            word_vec[word] = np.asarray(vec, dtype='float32')\n",
        "    return word_vec"
      ],
      "metadata": {
        "id": "2h-2UjGs7fu5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix:', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = config[\"glove_path\"]\n",
        "        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "nB3jz5Fk7dZd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "7JH84-Kx7Y-_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dependency_adj_matrix(text):\n",
        "    # https://spacy.io/docs/usage/processing-text\n",
        "    tokens = nlp(text)\n",
        "    words = text.split()\n",
        "    matrix = np.zeros((len(words), len(words))).astype('float32')\n",
        "    assert len(words) == len(list(tokens))\n",
        "\n",
        "    for token in tokens:\n",
        "        matrix[token.i][token.i] = 1\n",
        "        for child in token.children:\n",
        "            matrix[token.i][child.i] = 1\n",
        "            matrix[child.i][token.i] = 1\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "VemwD4FB7sY1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n",
        "    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n",
        "    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n",
        "    process(config[\"processed_train_path\"])\n",
        "    process(config[\"processed_val_path\"])\n",
        "    process(config[\"processed_test_path\"])"
      ],
      "metadata": {
        "id": "vC45U4k21oPd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(filename):\n",
        "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    lines = fin.readlines()\n",
        "    fin.close()\n",
        "    idx2graph = {}\n",
        "    fout = open(filename+'.graph', 'wb')\n",
        "    for i in range(0, len(lines), 3):\n",
        "        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n",
        "        aspect = lines[i + 1].strip()\n",
        "        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n",
        "        idx2graph[i] = adj_matrix\n",
        "    pickle.dump(idx2graph, fout)        \n",
        "    fout.close() "
      ],
      "metadata": {
        "id": "HlLsFiUJ7mPH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    ins = Instructor()\n",
        "    best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = ins.run()\n",
        "    return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "ANtAieVU8lwS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN**"
      ],
      "metadata": {
        "id": "cxb8D05u8UN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "5QyUuFxj8XuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "AF3DYep7AW8k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"base_path\": \"drive/MyDrive/CS4248/MAMS-ATSA\",\n",
        "    \"glove_path\": \"drive/MyDrive/CS4248/glove.42B.300d.txt\"\n",
        "}\n",
        "\n",
        "config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n",
        "config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n",
        "config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n",
        "config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n",
        "config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n",
        "config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n",
        "config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"
      ],
      "metadata": {
        "id": "9y9MO-RW8bcS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_files = {\n",
        "    'train': config[\"processed_train_path\"],\n",
        "    'test': config[\"processed_test_path\"],\n",
        "    'val': config[\"processed_val_path\"]\n",
        "}\n",
        "\n",
        "config2 = {\n",
        "    \"model_name\" : \"ATAE_LSTM\",\n",
        "    \"lr\" : 3e-5,\n",
        "    \"dropout\" : 0.1,\n",
        "    \"l2reg\" : 0.001,\n",
        "    \"num_epoch\" : 20,\n",
        "    \"batch_size\" : 25,\n",
        "    \"log_step\" : 10,\n",
        "    \"embed_dim\" : 300,\n",
        "    \"hidden_dim\" : 300,\n",
        "    \"model_class\" : ATAE_LSTM,\n",
        "    \"dataset\": \"MAMS\",\n",
        "    \"dataset_file\" : dataset_files,\n",
        "    \"inputs_cols\" : ['text_indices', 'aspect_indices'],\n",
        "    \"initializer\" : torch.nn.init.xavier_uniform_,\n",
        "    \"optimizer\" : torch.optim.Adam,\n",
        "    \"max_seq_len\" : 85,\n",
        "    \"polarities_dim\" : 3,\n",
        "    \"patience\" : 5, # for early stopping\n",
        "    \"device\" : None,\n",
        "    \"seed\" : 1234,\n",
        "    \"valset_ratio\" : 0\n",
        "}\n",
        "\n",
        "config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if config2[\"seed\"] is not None:\n",
        "    random.seed(config2[\"seed\"])\n",
        "    np.random.seed(config2[\"seed\"])\n",
        "    torch.manual_seed(config2[\"seed\"])\n",
        "    torch.cuda.manual_seed(config2[\"seed\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"
      ],
      "metadata": {
        "id": "clOIqT523tWy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**"
      ],
      "metadata": {
        "id": "3fT6RqeiAGmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this line only for the first time you run this file\n",
        "# process_data()"
      ],
      "metadata": {
        "id": "vT4xfKSC-Xpx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = run()"
      ],
      "metadata": {
        "id": "8gEIkSn38pHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cee05e2e-f021-4aa8-e245-9b8554607358"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word vectors...\n",
            "building embedding_matrix: 300_MAMS_embedding_matrix.dat\n",
            "> n_trainable_params: 3970503, n_nontrainable_params: 3994500\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 0\n",
            "[epoch 0] loss: 1.1607, acc: 0.1920\n",
            "[epoch 0] loss: 1.1414, acc: 0.2240\n",
            "[epoch 0] loss: 1.1356, acc: 0.2320\n",
            "[epoch 0] loss: 1.1282, acc: 0.2420\n",
            "[epoch 0] loss: 1.1202, acc: 0.2688\n",
            "[epoch 0] loss: 1.1158, acc: 0.2860\n",
            "[epoch 0] loss: 1.1108, acc: 0.3023\n",
            "[epoch 0] loss: 1.1037, acc: 0.3185\n",
            "[epoch 0] loss: 1.0970, acc: 0.3329\n",
            "[epoch 0] loss: 1.0900, acc: 0.3428\n",
            "[epoch 0] loss: 1.0858, acc: 0.3495\n",
            "[epoch 0] loss: 1.0796, acc: 0.3613\n",
            "[epoch 0] loss: 1.0728, acc: 0.3726\n",
            "[epoch 0] loss: 1.0658, acc: 0.3843\n",
            "[epoch 0] loss: 1.0587, acc: 0.3949\n",
            "[epoch 0] loss: 1.0531, acc: 0.4027\n",
            "[epoch 0] loss: 1.0461, acc: 0.4118\n",
            "[epoch 0] loss: 1.0416, acc: 0.4187\n",
            "[epoch 0] loss: 1.0346, acc: 0.4272\n",
            "[epoch 0] loss: 1.0310, acc: 0.4328\n",
            "[epoch 0] loss: 1.0285, acc: 0.4370\n",
            "[epoch 0] loss: 1.0209, acc: 0.4445\n",
            "[epoch 0] loss: 1.0155, acc: 0.4494\n",
            "[epoch 0] loss: 1.0136, acc: 0.4515\n",
            "[epoch 0] loss: 1.0100, acc: 0.4558\n",
            "[epoch 0] loss: 1.0061, acc: 0.4617\n",
            "[epoch 0] loss: 1.0016, acc: 0.4674\n",
            "[epoch 0] loss: 0.9985, acc: 0.4727\n",
            "[epoch 0] loss: 0.9953, acc: 0.4770\n",
            "[epoch 0] loss: 0.9928, acc: 0.4803\n",
            "[epoch 0] loss: 0.9890, acc: 0.4844\n",
            "[epoch 0] loss: 0.9854, acc: 0.4895\n",
            "[epoch 0] loss: 0.9821, acc: 0.4935\n",
            "[epoch 0] loss: 0.9771, acc: 0.4988\n",
            "[epoch 0] loss: 0.9746, acc: 0.5010\n",
            "[epoch 0] loss: 0.9719, acc: 0.5036\n",
            "[epoch 0] loss: 0.9705, acc: 0.5050\n",
            "[epoch 0] loss: 0.9683, acc: 0.5064\n",
            "[epoch 0] loss: 0.9646, acc: 0.5092\n",
            "[epoch 0] loss: 0.9611, acc: 0.5120\n",
            "[epoch 0] loss: 0.9604, acc: 0.5132\n",
            "[epoch 0] loss: 0.9580, acc: 0.5158\n",
            "[epoch 0] loss: 0.9557, acc: 0.5173\n",
            "[epoch 0] loss: 0.9545, acc: 0.5185\n",
            "> val_acc: 0.6321, val_f1: 0.6321\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6321\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 1\n",
            "[epoch 1] loss: 0.8068, acc: 0.6800\n",
            "[epoch 1] loss: 0.8125, acc: 0.6333\n",
            "[epoch 1] loss: 0.8295, acc: 0.6273\n",
            "[epoch 1] loss: 0.8345, acc: 0.6250\n",
            "[epoch 1] loss: 0.8392, acc: 0.6171\n",
            "[epoch 1] loss: 0.8472, acc: 0.6062\n",
            "[epoch 1] loss: 0.8309, acc: 0.6194\n",
            "[epoch 1] loss: 0.8358, acc: 0.6233\n",
            "[epoch 1] loss: 0.8387, acc: 0.6224\n",
            "[epoch 1] loss: 0.8377, acc: 0.6230\n",
            "[epoch 1] loss: 0.8399, acc: 0.6227\n",
            "[epoch 1] loss: 0.8407, acc: 0.6211\n",
            "[epoch 1] loss: 0.8401, acc: 0.6213\n",
            "[epoch 1] loss: 0.8409, acc: 0.6206\n",
            "[epoch 1] loss: 0.8432, acc: 0.6180\n",
            "[epoch 1] loss: 0.8378, acc: 0.6216\n",
            "[epoch 1] loss: 0.8385, acc: 0.6202\n",
            "[epoch 1] loss: 0.8386, acc: 0.6207\n",
            "[epoch 1] loss: 0.8381, acc: 0.6204\n",
            "[epoch 1] loss: 0.8385, acc: 0.6206\n",
            "[epoch 1] loss: 0.8387, acc: 0.6192\n",
            "[epoch 1] loss: 0.8398, acc: 0.6168\n",
            "[epoch 1] loss: 0.8396, acc: 0.6160\n",
            "[epoch 1] loss: 0.8383, acc: 0.6166\n",
            "[epoch 1] loss: 0.8381, acc: 0.6169\n",
            "[epoch 1] loss: 0.8379, acc: 0.6170\n",
            "[epoch 1] loss: 0.8384, acc: 0.6165\n",
            "[epoch 1] loss: 0.8407, acc: 0.6146\n",
            "[epoch 1] loss: 0.8432, acc: 0.6121\n",
            "[epoch 1] loss: 0.8431, acc: 0.6126\n",
            "[epoch 1] loss: 0.8424, acc: 0.6132\n",
            "[epoch 1] loss: 0.8402, acc: 0.6149\n",
            "[epoch 1] loss: 0.8372, acc: 0.6166\n",
            "[epoch 1] loss: 0.8383, acc: 0.6172\n",
            "[epoch 1] loss: 0.8364, acc: 0.6188\n",
            "[epoch 1] loss: 0.8366, acc: 0.6183\n",
            "[epoch 1] loss: 0.8374, acc: 0.6170\n",
            "[epoch 1] loss: 0.8375, acc: 0.6160\n",
            "[epoch 1] loss: 0.8400, acc: 0.6142\n",
            "[epoch 1] loss: 0.8397, acc: 0.6139\n",
            "[epoch 1] loss: 0.8382, acc: 0.6147\n",
            "[epoch 1] loss: 0.8384, acc: 0.6155\n",
            "[epoch 1] loss: 0.8373, acc: 0.6172\n",
            "[epoch 1] loss: 0.8374, acc: 0.6171\n",
            "[epoch 1] loss: 0.8383, acc: 0.6166\n",
            "> val_acc: 0.6359, val_f1: 0.6359\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6359\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 2\n",
            "[epoch 2] loss: 0.8177, acc: 0.6200\n",
            "[epoch 2] loss: 0.7853, acc: 0.6600\n",
            "[epoch 2] loss: 0.8232, acc: 0.6200\n",
            "[epoch 2] loss: 0.8346, acc: 0.6129\n",
            "[epoch 2] loss: 0.8270, acc: 0.6236\n",
            "[epoch 2] loss: 0.8279, acc: 0.6222\n",
            "[epoch 2] loss: 0.8220, acc: 0.6225\n",
            "[epoch 2] loss: 0.8272, acc: 0.6124\n",
            "[epoch 2] loss: 0.8185, acc: 0.6195\n",
            "[epoch 2] loss: 0.8206, acc: 0.6226\n",
            "[epoch 2] loss: 0.8154, acc: 0.6250\n",
            "[epoch 2] loss: 0.8082, acc: 0.6277\n",
            "[epoch 2] loss: 0.8052, acc: 0.6303\n",
            "[epoch 2] loss: 0.8072, acc: 0.6313\n",
            "[epoch 2] loss: 0.8013, acc: 0.6347\n",
            "[epoch 2] loss: 0.8014, acc: 0.6335\n",
            "[epoch 2] loss: 0.7980, acc: 0.6349\n",
            "[epoch 2] loss: 0.7968, acc: 0.6338\n",
            "[epoch 2] loss: 0.7965, acc: 0.6330\n",
            "[epoch 2] loss: 0.7995, acc: 0.6311\n",
            "[epoch 2] loss: 0.7994, acc: 0.6324\n",
            "[epoch 2] loss: 0.7977, acc: 0.6342\n",
            "[epoch 2] loss: 0.7958, acc: 0.6384\n",
            "[epoch 2] loss: 0.7923, acc: 0.6393\n",
            "[epoch 2] loss: 0.7897, acc: 0.6418\n",
            "[epoch 2] loss: 0.7928, acc: 0.6398\n",
            "[epoch 2] loss: 0.7947, acc: 0.6389\n",
            "[epoch 2] loss: 0.7948, acc: 0.6390\n",
            "[epoch 2] loss: 0.7920, acc: 0.6397\n",
            "[epoch 2] loss: 0.7915, acc: 0.6403\n",
            "[epoch 2] loss: 0.7954, acc: 0.6387\n",
            "[epoch 2] loss: 0.7985, acc: 0.6373\n",
            "[epoch 2] loss: 0.8008, acc: 0.6360\n",
            "[epoch 2] loss: 0.8018, acc: 0.6354\n",
            "[epoch 2] loss: 0.8035, acc: 0.6341\n",
            "[epoch 2] loss: 0.8029, acc: 0.6350\n",
            "[epoch 2] loss: 0.8023, acc: 0.6357\n",
            "[epoch 2] loss: 0.8030, acc: 0.6348\n",
            "[epoch 2] loss: 0.8031, acc: 0.6347\n",
            "[epoch 2] loss: 0.8033, acc: 0.6347\n",
            "[epoch 2] loss: 0.8043, acc: 0.6344\n",
            "[epoch 2] loss: 0.8055, acc: 0.6345\n",
            "[epoch 2] loss: 0.8048, acc: 0.6345\n",
            "[epoch 2] loss: 0.8050, acc: 0.6347\n",
            "[epoch 2] loss: 0.8051, acc: 0.6344\n",
            "> val_acc: 0.6411, val_f1: 0.6411\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6411\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 3\n",
            "[epoch 3] loss: 0.7505, acc: 0.6800\n",
            "[epoch 3] loss: 0.7255, acc: 0.6675\n",
            "[epoch 3] loss: 0.7438, acc: 0.6631\n",
            "[epoch 3] loss: 0.7700, acc: 0.6556\n",
            "[epoch 3] loss: 0.7601, acc: 0.6591\n",
            "[epoch 3] loss: 0.7526, acc: 0.6586\n",
            "[epoch 3] loss: 0.7525, acc: 0.6594\n",
            "[epoch 3] loss: 0.7559, acc: 0.6574\n",
            "[epoch 3] loss: 0.7617, acc: 0.6572\n",
            "[epoch 3] loss: 0.7668, acc: 0.6538\n",
            "[epoch 3] loss: 0.7648, acc: 0.6558\n",
            "[epoch 3] loss: 0.7577, acc: 0.6597\n",
            "[epoch 3] loss: 0.7556, acc: 0.6622\n",
            "[epoch 3] loss: 0.7599, acc: 0.6615\n",
            "[epoch 3] loss: 0.7605, acc: 0.6616\n",
            "[epoch 3] loss: 0.7652, acc: 0.6608\n",
            "[epoch 3] loss: 0.7708, acc: 0.6569\n",
            "[epoch 3] loss: 0.7708, acc: 0.6570\n",
            "[epoch 3] loss: 0.7710, acc: 0.6563\n",
            "[epoch 3] loss: 0.7716, acc: 0.6563\n",
            "[epoch 3] loss: 0.7701, acc: 0.6581\n",
            "[epoch 3] loss: 0.7691, acc: 0.6600\n",
            "[epoch 3] loss: 0.7703, acc: 0.6584\n",
            "[epoch 3] loss: 0.7692, acc: 0.6590\n",
            "[epoch 3] loss: 0.7753, acc: 0.6556\n",
            "[epoch 3] loss: 0.7774, acc: 0.6553\n",
            "[epoch 3] loss: 0.7761, acc: 0.6570\n",
            "[epoch 3] loss: 0.7765, acc: 0.6570\n",
            "[epoch 3] loss: 0.7799, acc: 0.6543\n",
            "[epoch 3] loss: 0.7800, acc: 0.6546\n",
            "[epoch 3] loss: 0.7816, acc: 0.6528\n",
            "[epoch 3] loss: 0.7811, acc: 0.6523\n",
            "[epoch 3] loss: 0.7829, acc: 0.6501\n",
            "[epoch 3] loss: 0.7831, acc: 0.6495\n",
            "[epoch 3] loss: 0.7828, acc: 0.6492\n",
            "[epoch 3] loss: 0.7836, acc: 0.6493\n",
            "[epoch 3] loss: 0.7849, acc: 0.6486\n",
            "[epoch 3] loss: 0.7846, acc: 0.6487\n",
            "[epoch 3] loss: 0.7845, acc: 0.6482\n",
            "[epoch 3] loss: 0.7855, acc: 0.6475\n",
            "[epoch 3] loss: 0.7865, acc: 0.6468\n",
            "[epoch 3] loss: 0.7857, acc: 0.6470\n",
            "[epoch 3] loss: 0.7856, acc: 0.6470\n",
            "[epoch 3] loss: 0.7859, acc: 0.6468\n",
            "[epoch 3] loss: 0.7862, acc: 0.6467\n",
            "> val_acc: 0.6434, val_f1: 0.6434\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6434\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 4\n",
            "[epoch 4] loss: 0.7669, acc: 0.6750\n",
            "[epoch 4] loss: 0.7659, acc: 0.6756\n",
            "[epoch 4] loss: 0.7599, acc: 0.6743\n",
            "[epoch 4] loss: 0.7776, acc: 0.6611\n",
            "[epoch 4] loss: 0.7855, acc: 0.6483\n",
            "[epoch 4] loss: 0.7776, acc: 0.6490\n",
            "[epoch 4] loss: 0.7696, acc: 0.6553\n",
            "[epoch 4] loss: 0.7648, acc: 0.6564\n",
            "[epoch 4] loss: 0.7560, acc: 0.6636\n",
            "[epoch 4] loss: 0.7522, acc: 0.6669\n",
            "[epoch 4] loss: 0.7596, acc: 0.6630\n",
            "[epoch 4] loss: 0.7658, acc: 0.6603\n",
            "[epoch 4] loss: 0.7624, acc: 0.6613\n",
            "[epoch 4] loss: 0.7694, acc: 0.6580\n",
            "[epoch 4] loss: 0.7657, acc: 0.6592\n",
            "[epoch 4] loss: 0.7684, acc: 0.6580\n",
            "[epoch 4] loss: 0.7686, acc: 0.6581\n",
            "[epoch 4] loss: 0.7696, acc: 0.6575\n",
            "[epoch 4] loss: 0.7676, acc: 0.6572\n",
            "[epoch 4] loss: 0.7721, acc: 0.6541\n",
            "[epoch 4] loss: 0.7705, acc: 0.6550\n",
            "[epoch 4] loss: 0.7691, acc: 0.6561\n",
            "[epoch 4] loss: 0.7703, acc: 0.6574\n",
            "[epoch 4] loss: 0.7685, acc: 0.6583\n",
            "[epoch 4] loss: 0.7673, acc: 0.6576\n",
            "[epoch 4] loss: 0.7684, acc: 0.6550\n",
            "[epoch 4] loss: 0.7711, acc: 0.6543\n",
            "[epoch 4] loss: 0.7736, acc: 0.6541\n",
            "[epoch 4] loss: 0.7710, acc: 0.6556\n",
            "[epoch 4] loss: 0.7695, acc: 0.6560\n",
            "[epoch 4] loss: 0.7692, acc: 0.6566\n",
            "[epoch 4] loss: 0.7672, acc: 0.6572\n",
            "[epoch 4] loss: 0.7667, acc: 0.6573\n",
            "[epoch 4] loss: 0.7669, acc: 0.6578\n",
            "[epoch 4] loss: 0.7676, acc: 0.6578\n",
            "[epoch 4] loss: 0.7708, acc: 0.6560\n",
            "[epoch 4] loss: 0.7695, acc: 0.6567\n",
            "[epoch 4] loss: 0.7704, acc: 0.6557\n",
            "[epoch 4] loss: 0.7712, acc: 0.6549\n",
            "[epoch 4] loss: 0.7709, acc: 0.6546\n",
            "[epoch 4] loss: 0.7716, acc: 0.6545\n",
            "[epoch 4] loss: 0.7712, acc: 0.6547\n",
            "[epoch 4] loss: 0.7700, acc: 0.6555\n",
            "[epoch 4] loss: 0.7703, acc: 0.6551\n",
            "[epoch 4] loss: 0.7712, acc: 0.6545\n",
            "> val_acc: 0.6494, val_f1: 0.6494\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 5\n",
            "[epoch 5] loss: 0.8391, acc: 0.6360\n",
            "[epoch 5] loss: 0.8259, acc: 0.6300\n",
            "[epoch 5] loss: 0.8096, acc: 0.6373\n",
            "[epoch 5] loss: 0.7820, acc: 0.6510\n",
            "[epoch 5] loss: 0.7852, acc: 0.6480\n",
            "[epoch 5] loss: 0.7747, acc: 0.6513\n",
            "[epoch 5] loss: 0.7721, acc: 0.6549\n",
            "[epoch 5] loss: 0.7709, acc: 0.6565\n",
            "[epoch 5] loss: 0.7632, acc: 0.6578\n",
            "[epoch 5] loss: 0.7625, acc: 0.6572\n",
            "[epoch 5] loss: 0.7663, acc: 0.6564\n",
            "[epoch 5] loss: 0.7618, acc: 0.6573\n",
            "[epoch 5] loss: 0.7616, acc: 0.6588\n",
            "[epoch 5] loss: 0.7618, acc: 0.6589\n",
            "[epoch 5] loss: 0.7601, acc: 0.6595\n",
            "[epoch 5] loss: 0.7604, acc: 0.6610\n",
            "[epoch 5] loss: 0.7594, acc: 0.6612\n",
            "[epoch 5] loss: 0.7556, acc: 0.6616\n",
            "[epoch 5] loss: 0.7540, acc: 0.6627\n",
            "[epoch 5] loss: 0.7524, acc: 0.6628\n",
            "[epoch 5] loss: 0.7535, acc: 0.6636\n",
            "[epoch 5] loss: 0.7534, acc: 0.6633\n",
            "[epoch 5] loss: 0.7520, acc: 0.6633\n",
            "[epoch 5] loss: 0.7496, acc: 0.6655\n",
            "[epoch 5] loss: 0.7500, acc: 0.6656\n",
            "[epoch 5] loss: 0.7485, acc: 0.6671\n",
            "[epoch 5] loss: 0.7479, acc: 0.6673\n",
            "[epoch 5] loss: 0.7508, acc: 0.6654\n",
            "[epoch 5] loss: 0.7497, acc: 0.6659\n",
            "[epoch 5] loss: 0.7500, acc: 0.6657\n",
            "[epoch 5] loss: 0.7470, acc: 0.6668\n",
            "[epoch 5] loss: 0.7470, acc: 0.6663\n",
            "[epoch 5] loss: 0.7499, acc: 0.6648\n",
            "[epoch 5] loss: 0.7511, acc: 0.6639\n",
            "[epoch 5] loss: 0.7535, acc: 0.6625\n",
            "[epoch 5] loss: 0.7532, acc: 0.6630\n",
            "[epoch 5] loss: 0.7520, acc: 0.6637\n",
            "[epoch 5] loss: 0.7529, acc: 0.6632\n",
            "[epoch 5] loss: 0.7529, acc: 0.6635\n",
            "[epoch 5] loss: 0.7537, acc: 0.6632\n",
            "[epoch 5] loss: 0.7540, acc: 0.6629\n",
            "[epoch 5] loss: 0.7539, acc: 0.6630\n",
            "[epoch 5] loss: 0.7542, acc: 0.6625\n",
            "[epoch 5] loss: 0.7564, acc: 0.6609\n",
            "> val_acc: 0.6374, val_f1: 0.6374\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 6\n",
            "[epoch 6] loss: 0.7773, acc: 0.6400\n",
            "[epoch 6] loss: 0.7071, acc: 0.6900\n",
            "[epoch 6] loss: 0.7225, acc: 0.6764\n",
            "[epoch 6] loss: 0.7334, acc: 0.6775\n",
            "[epoch 6] loss: 0.7312, acc: 0.6819\n",
            "[epoch 6] loss: 0.7327, acc: 0.6823\n",
            "[epoch 6] loss: 0.7359, acc: 0.6787\n",
            "[epoch 6] loss: 0.7322, acc: 0.6778\n",
            "[epoch 6] loss: 0.7326, acc: 0.6766\n",
            "[epoch 6] loss: 0.7421, acc: 0.6722\n",
            "[epoch 6] loss: 0.7415, acc: 0.6710\n",
            "[epoch 6] loss: 0.7391, acc: 0.6736\n",
            "[epoch 6] loss: 0.7358, acc: 0.6751\n",
            "[epoch 6] loss: 0.7384, acc: 0.6755\n",
            "[epoch 6] loss: 0.7382, acc: 0.6755\n",
            "[epoch 6] loss: 0.7392, acc: 0.6755\n",
            "[epoch 6] loss: 0.7390, acc: 0.6768\n",
            "[epoch 6] loss: 0.7431, acc: 0.6749\n",
            "[epoch 6] loss: 0.7472, acc: 0.6723\n",
            "[epoch 6] loss: 0.7460, acc: 0.6706\n",
            "[epoch 6] loss: 0.7472, acc: 0.6711\n",
            "[epoch 6] loss: 0.7433, acc: 0.6742\n",
            "[epoch 6] loss: 0.7439, acc: 0.6723\n",
            "[epoch 6] loss: 0.7400, acc: 0.6740\n",
            "[epoch 6] loss: 0.7382, acc: 0.6750\n",
            "[epoch 6] loss: 0.7374, acc: 0.6754\n",
            "[epoch 6] loss: 0.7387, acc: 0.6756\n",
            "[epoch 6] loss: 0.7378, acc: 0.6757\n",
            "[epoch 6] loss: 0.7389, acc: 0.6745\n",
            "[epoch 6] loss: 0.7397, acc: 0.6737\n",
            "[epoch 6] loss: 0.7387, acc: 0.6748\n",
            "[epoch 6] loss: 0.7431, acc: 0.6723\n",
            "[epoch 6] loss: 0.7411, acc: 0.6735\n",
            "[epoch 6] loss: 0.7426, acc: 0.6733\n",
            "[epoch 6] loss: 0.7422, acc: 0.6743\n",
            "[epoch 6] loss: 0.7416, acc: 0.6737\n",
            "[epoch 6] loss: 0.7443, acc: 0.6724\n",
            "[epoch 6] loss: 0.7462, acc: 0.6712\n",
            "[epoch 6] loss: 0.7467, acc: 0.6706\n",
            "[epoch 6] loss: 0.7488, acc: 0.6683\n",
            "[epoch 6] loss: 0.7476, acc: 0.6694\n",
            "[epoch 6] loss: 0.7473, acc: 0.6688\n",
            "[epoch 6] loss: 0.7477, acc: 0.6682\n",
            "[epoch 6] loss: 0.7468, acc: 0.6685\n",
            "[epoch 6] loss: 0.7468, acc: 0.6683\n",
            "> val_acc: 0.6464, val_f1: 0.6464\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 7\n",
            "[epoch 7] loss: 0.6903, acc: 0.6900\n",
            "[epoch 7] loss: 0.7310, acc: 0.6714\n",
            "[epoch 7] loss: 0.7485, acc: 0.6550\n",
            "[epoch 7] loss: 0.7361, acc: 0.6718\n",
            "[epoch 7] loss: 0.7271, acc: 0.6764\n",
            "[epoch 7] loss: 0.7269, acc: 0.6837\n",
            "[epoch 7] loss: 0.7281, acc: 0.6806\n",
            "[epoch 7] loss: 0.7320, acc: 0.6784\n",
            "[epoch 7] loss: 0.7229, acc: 0.6833\n",
            "[epoch 7] loss: 0.7241, acc: 0.6855\n",
            "[epoch 7] loss: 0.7258, acc: 0.6842\n",
            "[epoch 7] loss: 0.7289, acc: 0.6807\n",
            "[epoch 7] loss: 0.7274, acc: 0.6803\n",
            "[epoch 7] loss: 0.7238, acc: 0.6833\n",
            "[epoch 7] loss: 0.7279, acc: 0.6817\n",
            "[epoch 7] loss: 0.7302, acc: 0.6810\n",
            "[epoch 7] loss: 0.7301, acc: 0.6805\n",
            "[epoch 7] loss: 0.7356, acc: 0.6777\n",
            "[epoch 7] loss: 0.7378, acc: 0.6767\n",
            "[epoch 7] loss: 0.7375, acc: 0.6773\n",
            "[epoch 7] loss: 0.7368, acc: 0.6757\n",
            "[epoch 7] loss: 0.7375, acc: 0.6742\n",
            "[epoch 7] loss: 0.7376, acc: 0.6752\n",
            "[epoch 7] loss: 0.7347, acc: 0.6761\n",
            "[epoch 7] loss: 0.7353, acc: 0.6769\n",
            "[epoch 7] loss: 0.7332, acc: 0.6798\n",
            "[epoch 7] loss: 0.7322, acc: 0.6803\n",
            "[epoch 7] loss: 0.7315, acc: 0.6807\n",
            "[epoch 7] loss: 0.7321, acc: 0.6811\n",
            "[epoch 7] loss: 0.7321, acc: 0.6808\n",
            "[epoch 7] loss: 0.7322, acc: 0.6803\n",
            "[epoch 7] loss: 0.7330, acc: 0.6790\n",
            "[epoch 7] loss: 0.7319, acc: 0.6796\n",
            "[epoch 7] loss: 0.7307, acc: 0.6804\n",
            "[epoch 7] loss: 0.7316, acc: 0.6807\n",
            "[epoch 7] loss: 0.7306, acc: 0.6812\n",
            "[epoch 7] loss: 0.7311, acc: 0.6811\n",
            "[epoch 7] loss: 0.7305, acc: 0.6820\n",
            "[epoch 7] loss: 0.7301, acc: 0.6822\n",
            "[epoch 7] loss: 0.7315, acc: 0.6817\n",
            "[epoch 7] loss: 0.7318, acc: 0.6812\n",
            "[epoch 7] loss: 0.7337, acc: 0.6799\n",
            "[epoch 7] loss: 0.7329, acc: 0.6792\n",
            "[epoch 7] loss: 0.7347, acc: 0.6783\n",
            "[epoch 7] loss: 0.7352, acc: 0.6781\n",
            "> val_acc: 0.6486, val_f1: 0.6486\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 8\n",
            "[epoch 8] loss: 0.6978, acc: 0.6733\n",
            "[epoch 8] loss: 0.7088, acc: 0.6800\n",
            "[epoch 8] loss: 0.7026, acc: 0.6846\n",
            "[epoch 8] loss: 0.7135, acc: 0.6733\n",
            "[epoch 8] loss: 0.7157, acc: 0.6713\n",
            "[epoch 8] loss: 0.7080, acc: 0.6836\n",
            "[epoch 8] loss: 0.7158, acc: 0.6788\n",
            "[epoch 8] loss: 0.7085, acc: 0.6868\n",
            "[epoch 8] loss: 0.7177, acc: 0.6777\n",
            "[epoch 8] loss: 0.7122, acc: 0.6817\n",
            "[epoch 8] loss: 0.7119, acc: 0.6823\n",
            "[epoch 8] loss: 0.7131, acc: 0.6834\n",
            "[epoch 8] loss: 0.7092, acc: 0.6889\n",
            "[epoch 8] loss: 0.7137, acc: 0.6862\n",
            "[epoch 8] loss: 0.7134, acc: 0.6855\n",
            "[epoch 8] loss: 0.7159, acc: 0.6851\n",
            "[epoch 8] loss: 0.7137, acc: 0.6877\n",
            "[epoch 8] loss: 0.7125, acc: 0.6857\n",
            "[epoch 8] loss: 0.7151, acc: 0.6839\n",
            "[epoch 8] loss: 0.7177, acc: 0.6820\n",
            "[epoch 8] loss: 0.7214, acc: 0.6798\n",
            "[epoch 8] loss: 0.7224, acc: 0.6776\n",
            "[epoch 8] loss: 0.7209, acc: 0.6773\n",
            "[epoch 8] loss: 0.7266, acc: 0.6766\n",
            "[epoch 8] loss: 0.7279, acc: 0.6763\n",
            "[epoch 8] loss: 0.7250, acc: 0.6787\n",
            "[epoch 8] loss: 0.7285, acc: 0.6774\n",
            "[epoch 8] loss: 0.7266, acc: 0.6784\n",
            "[epoch 8] loss: 0.7273, acc: 0.6778\n",
            "[epoch 8] loss: 0.7272, acc: 0.6778\n",
            "[epoch 8] loss: 0.7287, acc: 0.6769\n",
            "[epoch 8] loss: 0.7284, acc: 0.6761\n",
            "[epoch 8] loss: 0.7298, acc: 0.6747\n",
            "[epoch 8] loss: 0.7276, acc: 0.6769\n",
            "[epoch 8] loss: 0.7271, acc: 0.6776\n",
            "[epoch 8] loss: 0.7275, acc: 0.6784\n",
            "[epoch 8] loss: 0.7280, acc: 0.6783\n",
            "[epoch 8] loss: 0.7287, acc: 0.6786\n",
            "[epoch 8] loss: 0.7290, acc: 0.6782\n",
            "[epoch 8] loss: 0.7294, acc: 0.6780\n",
            "[epoch 8] loss: 0.7285, acc: 0.6778\n",
            "[epoch 8] loss: 0.7298, acc: 0.6776\n",
            "[epoch 8] loss: 0.7295, acc: 0.6777\n",
            "[epoch 8] loss: 0.7295, acc: 0.6783\n",
            "[epoch 8] loss: 0.7297, acc: 0.6777\n",
            "> val_acc: 0.6486, val_f1: 0.6486\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 9\n",
            "[epoch 9] loss: 0.7463, acc: 0.6800\n",
            "[epoch 9] loss: 0.7473, acc: 0.6600\n",
            "[epoch 9] loss: 0.7633, acc: 0.6529\n",
            "[epoch 9] loss: 0.7499, acc: 0.6674\n",
            "[epoch 9] loss: 0.7408, acc: 0.6675\n",
            "[epoch 9] loss: 0.7311, acc: 0.6738\n",
            "[epoch 9] loss: 0.7287, acc: 0.6771\n",
            "[epoch 9] loss: 0.7368, acc: 0.6749\n",
            "[epoch 9] loss: 0.7341, acc: 0.6773\n",
            "[epoch 9] loss: 0.7297, acc: 0.6824\n",
            "[epoch 9] loss: 0.7310, acc: 0.6822\n",
            "[epoch 9] loss: 0.7256, acc: 0.6824\n",
            "[epoch 9] loss: 0.7269, acc: 0.6853\n",
            "[epoch 9] loss: 0.7285, acc: 0.6826\n",
            "[epoch 9] loss: 0.7248, acc: 0.6827\n",
            "[epoch 9] loss: 0.7243, acc: 0.6815\n",
            "[epoch 9] loss: 0.7217, acc: 0.6821\n",
            "[epoch 9] loss: 0.7180, acc: 0.6838\n",
            "[epoch 9] loss: 0.7138, acc: 0.6866\n",
            "[epoch 9] loss: 0.7169, acc: 0.6859\n",
            "[epoch 9] loss: 0.7107, acc: 0.6892\n",
            "[epoch 9] loss: 0.7143, acc: 0.6877\n",
            "[epoch 9] loss: 0.7137, acc: 0.6874\n",
            "[epoch 9] loss: 0.7153, acc: 0.6866\n",
            "[epoch 9] loss: 0.7151, acc: 0.6866\n",
            "[epoch 9] loss: 0.7171, acc: 0.6851\n",
            "[epoch 9] loss: 0.7169, acc: 0.6860\n",
            "[epoch 9] loss: 0.7184, acc: 0.6852\n",
            "[epoch 9] loss: 0.7182, acc: 0.6847\n",
            "[epoch 9] loss: 0.7168, acc: 0.6855\n",
            "[epoch 9] loss: 0.7159, acc: 0.6864\n",
            "[epoch 9] loss: 0.7151, acc: 0.6857\n",
            "[epoch 9] loss: 0.7145, acc: 0.6857\n",
            "[epoch 9] loss: 0.7136, acc: 0.6871\n",
            "[epoch 9] loss: 0.7125, acc: 0.6877\n",
            "[epoch 9] loss: 0.7108, acc: 0.6883\n",
            "[epoch 9] loss: 0.7112, acc: 0.6878\n",
            "[epoch 9] loss: 0.7130, acc: 0.6870\n",
            "[epoch 9] loss: 0.7157, acc: 0.6860\n",
            "[epoch 9] loss: 0.7149, acc: 0.6856\n",
            "[epoch 9] loss: 0.7146, acc: 0.6863\n",
            "[epoch 9] loss: 0.7150, acc: 0.6861\n",
            "[epoch 9] loss: 0.7148, acc: 0.6864\n",
            "[epoch 9] loss: 0.7151, acc: 0.6859\n",
            "[epoch 9] loss: 0.7151, acc: 0.6859\n",
            "> val_acc: 0.6539, val_f1: 0.6539\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6539\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 10\n",
            "[epoch 10] loss: 0.6863, acc: 0.7080\n",
            "[epoch 10] loss: 0.6906, acc: 0.6960\n",
            "[epoch 10] loss: 0.7123, acc: 0.6800\n",
            "[epoch 10] loss: 0.7078, acc: 0.6860\n",
            "[epoch 10] loss: 0.7001, acc: 0.6936\n",
            "[epoch 10] loss: 0.7011, acc: 0.6947\n",
            "[epoch 10] loss: 0.6983, acc: 0.7006\n",
            "[epoch 10] loss: 0.6999, acc: 0.6965\n",
            "[epoch 10] loss: 0.6972, acc: 0.6938\n",
            "[epoch 10] loss: 0.6956, acc: 0.6908\n",
            "[epoch 10] loss: 0.6993, acc: 0.6909\n",
            "[epoch 10] loss: 0.7076, acc: 0.6850\n",
            "[epoch 10] loss: 0.7063, acc: 0.6865\n",
            "[epoch 10] loss: 0.7115, acc: 0.6869\n",
            "[epoch 10] loss: 0.7137, acc: 0.6848\n",
            "[epoch 10] loss: 0.7137, acc: 0.6855\n",
            "[epoch 10] loss: 0.7202, acc: 0.6852\n",
            "[epoch 10] loss: 0.7203, acc: 0.6856\n",
            "[epoch 10] loss: 0.7176, acc: 0.6863\n",
            "[epoch 10] loss: 0.7172, acc: 0.6858\n",
            "[epoch 10] loss: 0.7151, acc: 0.6865\n",
            "[epoch 10] loss: 0.7109, acc: 0.6882\n",
            "[epoch 10] loss: 0.7136, acc: 0.6871\n",
            "[epoch 10] loss: 0.7109, acc: 0.6882\n",
            "[epoch 10] loss: 0.7113, acc: 0.6869\n",
            "[epoch 10] loss: 0.7102, acc: 0.6874\n",
            "[epoch 10] loss: 0.7117, acc: 0.6880\n",
            "[epoch 10] loss: 0.7111, acc: 0.6881\n",
            "[epoch 10] loss: 0.7102, acc: 0.6884\n",
            "[epoch 10] loss: 0.7097, acc: 0.6883\n",
            "[epoch 10] loss: 0.7095, acc: 0.6885\n",
            "[epoch 10] loss: 0.7114, acc: 0.6880\n",
            "[epoch 10] loss: 0.7116, acc: 0.6881\n",
            "[epoch 10] loss: 0.7093, acc: 0.6889\n",
            "[epoch 10] loss: 0.7101, acc: 0.6891\n",
            "[epoch 10] loss: 0.7107, acc: 0.6897\n",
            "[epoch 10] loss: 0.7120, acc: 0.6885\n",
            "[epoch 10] loss: 0.7130, acc: 0.6876\n",
            "[epoch 10] loss: 0.7144, acc: 0.6865\n",
            "[epoch 10] loss: 0.7133, acc: 0.6866\n",
            "[epoch 10] loss: 0.7131, acc: 0.6873\n",
            "[epoch 10] loss: 0.7117, acc: 0.6886\n",
            "[epoch 10] loss: 0.7104, acc: 0.6897\n",
            "[epoch 10] loss: 0.7105, acc: 0.6902\n",
            "> val_acc: 0.6569, val_f1: 0.6569\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6569\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 11\n",
            "[epoch 11] loss: 0.6133, acc: 0.7600\n",
            "[epoch 11] loss: 0.6464, acc: 0.7100\n",
            "[epoch 11] loss: 0.6672, acc: 0.7127\n",
            "[epoch 11] loss: 0.6660, acc: 0.7188\n",
            "[epoch 11] loss: 0.6571, acc: 0.7190\n",
            "[epoch 11] loss: 0.6682, acc: 0.7131\n",
            "[epoch 11] loss: 0.6659, acc: 0.7155\n",
            "[epoch 11] loss: 0.6570, acc: 0.7189\n",
            "[epoch 11] loss: 0.6666, acc: 0.7141\n",
            "[epoch 11] loss: 0.6716, acc: 0.7083\n",
            "[epoch 11] loss: 0.6814, acc: 0.7043\n",
            "[epoch 11] loss: 0.6861, acc: 0.7054\n",
            "[epoch 11] loss: 0.6882, acc: 0.7023\n",
            "[epoch 11] loss: 0.6875, acc: 0.7033\n",
            "[epoch 11] loss: 0.6874, acc: 0.7054\n",
            "[epoch 11] loss: 0.6863, acc: 0.7061\n",
            "[epoch 11] loss: 0.6839, acc: 0.7086\n",
            "[epoch 11] loss: 0.6858, acc: 0.7079\n",
            "[epoch 11] loss: 0.6825, acc: 0.7097\n",
            "[epoch 11] loss: 0.6866, acc: 0.7065\n",
            "[epoch 11] loss: 0.6917, acc: 0.7032\n",
            "[epoch 11] loss: 0.6921, acc: 0.7017\n",
            "[epoch 11] loss: 0.6953, acc: 0.7005\n",
            "[epoch 11] loss: 0.6965, acc: 0.7014\n",
            "[epoch 11] loss: 0.6954, acc: 0.7025\n",
            "[epoch 11] loss: 0.6967, acc: 0.7017\n",
            "[epoch 11] loss: 0.6980, acc: 0.7006\n",
            "[epoch 11] loss: 0.6964, acc: 0.7018\n",
            "[epoch 11] loss: 0.6990, acc: 0.7014\n",
            "[epoch 11] loss: 0.6986, acc: 0.7019\n",
            "[epoch 11] loss: 0.6955, acc: 0.7025\n",
            "[epoch 11] loss: 0.6968, acc: 0.7014\n",
            "[epoch 11] loss: 0.6961, acc: 0.7016\n",
            "[epoch 11] loss: 0.6955, acc: 0.7013\n",
            "[epoch 11] loss: 0.6962, acc: 0.7000\n",
            "[epoch 11] loss: 0.6965, acc: 0.6992\n",
            "[epoch 11] loss: 0.6972, acc: 0.6988\n",
            "[epoch 11] loss: 0.6977, acc: 0.6986\n",
            "[epoch 11] loss: 0.6983, acc: 0.6981\n",
            "[epoch 11] loss: 0.6985, acc: 0.6980\n",
            "[epoch 11] loss: 0.6984, acc: 0.6977\n",
            "[epoch 11] loss: 0.7004, acc: 0.6960\n",
            "[epoch 11] loss: 0.7008, acc: 0.6962\n",
            "[epoch 11] loss: 0.7005, acc: 0.6966\n",
            "[epoch 11] loss: 0.6995, acc: 0.6966\n",
            "> val_acc: 0.6607, val_f1: 0.6607\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6607\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 12\n",
            "[epoch 12] loss: 0.6992, acc: 0.7100\n",
            "[epoch 12] loss: 0.7181, acc: 0.6886\n",
            "[epoch 12] loss: 0.6914, acc: 0.7017\n",
            "[epoch 12] loss: 0.6888, acc: 0.7000\n",
            "[epoch 12] loss: 0.7054, acc: 0.6864\n",
            "[epoch 12] loss: 0.7017, acc: 0.6904\n",
            "[epoch 12] loss: 0.6957, acc: 0.6906\n",
            "[epoch 12] loss: 0.6940, acc: 0.6892\n",
            "[epoch 12] loss: 0.6828, acc: 0.6967\n",
            "[epoch 12] loss: 0.6799, acc: 0.7000\n",
            "[epoch 12] loss: 0.6828, acc: 0.6981\n",
            "[epoch 12] loss: 0.6834, acc: 0.6968\n",
            "[epoch 12] loss: 0.6871, acc: 0.6990\n",
            "[epoch 12] loss: 0.6866, acc: 0.6976\n",
            "[epoch 12] loss: 0.6812, acc: 0.7011\n",
            "[epoch 12] loss: 0.6825, acc: 0.7008\n",
            "[epoch 12] loss: 0.6822, acc: 0.6993\n",
            "[epoch 12] loss: 0.6847, acc: 0.6998\n",
            "[epoch 12] loss: 0.6841, acc: 0.7002\n",
            "[epoch 12] loss: 0.6813, acc: 0.7025\n",
            "[epoch 12] loss: 0.6820, acc: 0.7004\n",
            "[epoch 12] loss: 0.6868, acc: 0.6993\n",
            "[epoch 12] loss: 0.6869, acc: 0.6993\n",
            "[epoch 12] loss: 0.6878, acc: 0.6983\n",
            "[epoch 12] loss: 0.6871, acc: 0.6987\n",
            "[epoch 12] loss: 0.6856, acc: 0.6986\n",
            "[epoch 12] loss: 0.6843, acc: 0.7000\n",
            "[epoch 12] loss: 0.6868, acc: 0.6980\n",
            "[epoch 12] loss: 0.6898, acc: 0.6958\n",
            "[epoch 12] loss: 0.6879, acc: 0.6970\n",
            "[epoch 12] loss: 0.6857, acc: 0.6988\n",
            "[epoch 12] loss: 0.6863, acc: 0.6996\n",
            "[epoch 12] loss: 0.6872, acc: 0.6983\n",
            "[epoch 12] loss: 0.6882, acc: 0.6976\n",
            "[epoch 12] loss: 0.6891, acc: 0.6969\n",
            "[epoch 12] loss: 0.6912, acc: 0.6957\n",
            "[epoch 12] loss: 0.6925, acc: 0.6953\n",
            "[epoch 12] loss: 0.6923, acc: 0.6963\n",
            "[epoch 12] loss: 0.6925, acc: 0.6967\n",
            "[epoch 12] loss: 0.6926, acc: 0.6971\n",
            "[epoch 12] loss: 0.6915, acc: 0.6977\n",
            "[epoch 12] loss: 0.6909, acc: 0.6984\n",
            "[epoch 12] loss: 0.6892, acc: 0.6992\n",
            "[epoch 12] loss: 0.6895, acc: 0.6986\n",
            "[epoch 12] loss: 0.6891, acc: 0.6987\n",
            "> val_acc: 0.6554, val_f1: 0.6554\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 13\n",
            "[epoch 13] loss: 0.6946, acc: 0.7267\n",
            "[epoch 13] loss: 0.7027, acc: 0.7200\n",
            "[epoch 13] loss: 0.7073, acc: 0.7062\n",
            "[epoch 13] loss: 0.6757, acc: 0.7178\n",
            "[epoch 13] loss: 0.6988, acc: 0.7043\n",
            "[epoch 13] loss: 0.6984, acc: 0.6964\n",
            "[epoch 13] loss: 0.6972, acc: 0.6970\n",
            "[epoch 13] loss: 0.6989, acc: 0.6953\n",
            "[epoch 13] loss: 0.6919, acc: 0.7019\n",
            "[epoch 13] loss: 0.6879, acc: 0.7033\n",
            "[epoch 13] loss: 0.6878, acc: 0.7019\n",
            "[epoch 13] loss: 0.6927, acc: 0.6990\n",
            "[epoch 13] loss: 0.6922, acc: 0.6990\n",
            "[epoch 13] loss: 0.6920, acc: 0.6976\n",
            "[epoch 13] loss: 0.6907, acc: 0.7003\n",
            "[epoch 13] loss: 0.6933, acc: 0.6967\n",
            "[epoch 13] loss: 0.6927, acc: 0.6973\n",
            "[epoch 13] loss: 0.6898, acc: 0.6984\n",
            "[epoch 13] loss: 0.6903, acc: 0.6998\n",
            "[epoch 13] loss: 0.6878, acc: 0.7000\n",
            "[epoch 13] loss: 0.6848, acc: 0.7017\n",
            "[epoch 13] loss: 0.6856, acc: 0.7004\n",
            "[epoch 13] loss: 0.6862, acc: 0.7000\n",
            "[epoch 13] loss: 0.6861, acc: 0.7005\n",
            "[epoch 13] loss: 0.6869, acc: 0.7003\n",
            "[epoch 13] loss: 0.6884, acc: 0.6998\n",
            "[epoch 13] loss: 0.6891, acc: 0.7000\n",
            "[epoch 13] loss: 0.6876, acc: 0.7016\n",
            "[epoch 13] loss: 0.6869, acc: 0.7017\n",
            "[epoch 13] loss: 0.6870, acc: 0.7009\n",
            "[epoch 13] loss: 0.6863, acc: 0.7014\n",
            "[epoch 13] loss: 0.6871, acc: 0.7009\n",
            "[epoch 13] loss: 0.6867, acc: 0.7010\n",
            "[epoch 13] loss: 0.6867, acc: 0.7008\n",
            "[epoch 13] loss: 0.6877, acc: 0.7006\n",
            "[epoch 13] loss: 0.6878, acc: 0.7000\n",
            "[epoch 13] loss: 0.6870, acc: 0.7002\n",
            "[epoch 13] loss: 0.6858, acc: 0.7006\n",
            "[epoch 13] loss: 0.6849, acc: 0.7009\n",
            "[epoch 13] loss: 0.6828, acc: 0.7021\n",
            "[epoch 13] loss: 0.6826, acc: 0.7022\n",
            "[epoch 13] loss: 0.6811, acc: 0.7022\n",
            "[epoch 13] loss: 0.6798, acc: 0.7023\n",
            "[epoch 13] loss: 0.6799, acc: 0.7022\n",
            "[epoch 13] loss: 0.6815, acc: 0.7016\n",
            "> val_acc: 0.6629, val_f1: 0.6629\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6629\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 14\n",
            "[epoch 14] loss: 0.5841, acc: 0.7600\n",
            "[epoch 14] loss: 0.6114, acc: 0.7400\n",
            "[epoch 14] loss: 0.6749, acc: 0.7086\n",
            "[epoch 14] loss: 0.6769, acc: 0.7063\n",
            "[epoch 14] loss: 0.6751, acc: 0.7050\n",
            "[epoch 14] loss: 0.6699, acc: 0.7131\n",
            "[epoch 14] loss: 0.6751, acc: 0.7082\n",
            "[epoch 14] loss: 0.6687, acc: 0.7154\n",
            "[epoch 14] loss: 0.6705, acc: 0.7118\n",
            "[epoch 14] loss: 0.6759, acc: 0.7114\n",
            "[epoch 14] loss: 0.6741, acc: 0.7100\n",
            "[epoch 14] loss: 0.6733, acc: 0.7098\n",
            "[epoch 14] loss: 0.6792, acc: 0.7053\n",
            "[epoch 14] loss: 0.6770, acc: 0.7064\n",
            "[epoch 14] loss: 0.6768, acc: 0.7059\n",
            "[epoch 14] loss: 0.6768, acc: 0.7053\n",
            "[epoch 14] loss: 0.6691, acc: 0.7081\n",
            "[epoch 14] loss: 0.6699, acc: 0.7061\n",
            "[epoch 14] loss: 0.6713, acc: 0.7040\n",
            "[epoch 14] loss: 0.6750, acc: 0.7024\n",
            "[epoch 14] loss: 0.6775, acc: 0.7006\n",
            "[epoch 14] loss: 0.6747, acc: 0.7002\n",
            "[epoch 14] loss: 0.6743, acc: 0.7009\n",
            "[epoch 14] loss: 0.6755, acc: 0.7003\n",
            "[epoch 14] loss: 0.6724, acc: 0.7027\n",
            "[epoch 14] loss: 0.6721, acc: 0.7020\n",
            "[epoch 14] loss: 0.6724, acc: 0.7027\n",
            "[epoch 14] loss: 0.6723, acc: 0.7019\n",
            "[epoch 14] loss: 0.6726, acc: 0.7014\n",
            "[epoch 14] loss: 0.6730, acc: 0.7005\n",
            "[epoch 14] loss: 0.6726, acc: 0.7010\n",
            "[epoch 14] loss: 0.6760, acc: 0.7006\n",
            "[epoch 14] loss: 0.6750, acc: 0.7015\n",
            "[epoch 14] loss: 0.6750, acc: 0.7015\n",
            "[epoch 14] loss: 0.6774, acc: 0.7009\n",
            "[epoch 14] loss: 0.6754, acc: 0.7017\n",
            "[epoch 14] loss: 0.6754, acc: 0.7020\n",
            "[epoch 14] loss: 0.6753, acc: 0.7015\n",
            "[epoch 14] loss: 0.6754, acc: 0.7020\n",
            "[epoch 14] loss: 0.6746, acc: 0.7022\n",
            "[epoch 14] loss: 0.6742, acc: 0.7032\n",
            "[epoch 14] loss: 0.6752, acc: 0.7023\n",
            "[epoch 14] loss: 0.6741, acc: 0.7033\n",
            "[epoch 14] loss: 0.6757, acc: 0.7023\n",
            "[epoch 14] loss: 0.6762, acc: 0.7021\n",
            "> val_acc: 0.6569, val_f1: 0.6569\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 15\n",
            "[epoch 15] loss: 0.6479, acc: 0.7400\n",
            "[epoch 15] loss: 0.6588, acc: 0.7280\n",
            "[epoch 15] loss: 0.6653, acc: 0.7213\n",
            "[epoch 15] loss: 0.6756, acc: 0.7100\n",
            "[epoch 15] loss: 0.6713, acc: 0.7112\n",
            "[epoch 15] loss: 0.6672, acc: 0.7113\n",
            "[epoch 15] loss: 0.6653, acc: 0.7091\n",
            "[epoch 15] loss: 0.6685, acc: 0.7090\n",
            "[epoch 15] loss: 0.6751, acc: 0.7080\n",
            "[epoch 15] loss: 0.6763, acc: 0.7076\n",
            "[epoch 15] loss: 0.6738, acc: 0.7065\n",
            "[epoch 15] loss: 0.6698, acc: 0.7073\n",
            "[epoch 15] loss: 0.6679, acc: 0.7077\n",
            "[epoch 15] loss: 0.6721, acc: 0.7083\n",
            "[epoch 15] loss: 0.6729, acc: 0.7059\n",
            "[epoch 15] loss: 0.6742, acc: 0.7045\n",
            "[epoch 15] loss: 0.6698, acc: 0.7075\n",
            "[epoch 15] loss: 0.6692, acc: 0.7078\n",
            "[epoch 15] loss: 0.6710, acc: 0.7076\n",
            "[epoch 15] loss: 0.6742, acc: 0.7050\n",
            "[epoch 15] loss: 0.6727, acc: 0.7059\n",
            "[epoch 15] loss: 0.6710, acc: 0.7076\n",
            "[epoch 15] loss: 0.6733, acc: 0.7070\n",
            "[epoch 15] loss: 0.6701, acc: 0.7085\n",
            "[epoch 15] loss: 0.6696, acc: 0.7085\n",
            "[epoch 15] loss: 0.6712, acc: 0.7091\n",
            "[epoch 15] loss: 0.6729, acc: 0.7077\n",
            "[epoch 15] loss: 0.6725, acc: 0.7086\n",
            "[epoch 15] loss: 0.6721, acc: 0.7083\n",
            "[epoch 15] loss: 0.6724, acc: 0.7077\n",
            "[epoch 15] loss: 0.6720, acc: 0.7079\n",
            "[epoch 15] loss: 0.6737, acc: 0.7077\n",
            "[epoch 15] loss: 0.6722, acc: 0.7092\n",
            "[epoch 15] loss: 0.6736, acc: 0.7080\n",
            "[epoch 15] loss: 0.6738, acc: 0.7080\n",
            "[epoch 15] loss: 0.6749, acc: 0.7064\n",
            "[epoch 15] loss: 0.6752, acc: 0.7065\n",
            "[epoch 15] loss: 0.6750, acc: 0.7058\n",
            "[epoch 15] loss: 0.6761, acc: 0.7055\n",
            "[epoch 15] loss: 0.6757, acc: 0.7055\n",
            "[epoch 15] loss: 0.6747, acc: 0.7061\n",
            "[epoch 15] loss: 0.6727, acc: 0.7071\n",
            "[epoch 15] loss: 0.6709, acc: 0.7082\n",
            "[epoch 15] loss: 0.6713, acc: 0.7081\n",
            "> val_acc: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 16\n",
            "[epoch 16] loss: 0.6542, acc: 0.6600\n",
            "[epoch 16] loss: 0.6198, acc: 0.6933\n",
            "[epoch 16] loss: 0.6162, acc: 0.7109\n",
            "[epoch 16] loss: 0.6043, acc: 0.7262\n",
            "[epoch 16] loss: 0.5929, acc: 0.7381\n",
            "[epoch 16] loss: 0.6014, acc: 0.7346\n",
            "[epoch 16] loss: 0.6016, acc: 0.7381\n",
            "[epoch 16] loss: 0.5999, acc: 0.7433\n",
            "[epoch 16] loss: 0.6081, acc: 0.7405\n",
            "[epoch 16] loss: 0.6094, acc: 0.7383\n",
            "[epoch 16] loss: 0.6155, acc: 0.7369\n",
            "[epoch 16] loss: 0.6241, acc: 0.7307\n",
            "[epoch 16] loss: 0.6370, acc: 0.7246\n",
            "[epoch 16] loss: 0.6411, acc: 0.7227\n",
            "[epoch 16] loss: 0.6404, acc: 0.7237\n",
            "[epoch 16] loss: 0.6437, acc: 0.7224\n",
            "[epoch 16] loss: 0.6439, acc: 0.7200\n",
            "[epoch 16] loss: 0.6499, acc: 0.7184\n",
            "[epoch 16] loss: 0.6481, acc: 0.7174\n",
            "[epoch 16] loss: 0.6515, acc: 0.7163\n",
            "[epoch 16] loss: 0.6494, acc: 0.7184\n",
            "[epoch 16] loss: 0.6514, acc: 0.7172\n",
            "[epoch 16] loss: 0.6514, acc: 0.7173\n",
            "[epoch 16] loss: 0.6525, acc: 0.7171\n",
            "[epoch 16] loss: 0.6530, acc: 0.7169\n",
            "[epoch 16] loss: 0.6567, acc: 0.7156\n",
            "[epoch 16] loss: 0.6565, acc: 0.7159\n",
            "[epoch 16] loss: 0.6568, acc: 0.7151\n",
            "[epoch 16] loss: 0.6576, acc: 0.7152\n",
            "[epoch 16] loss: 0.6594, acc: 0.7137\n",
            "[epoch 16] loss: 0.6594, acc: 0.7132\n",
            "[epoch 16] loss: 0.6581, acc: 0.7145\n",
            "[epoch 16] loss: 0.6593, acc: 0.7142\n",
            "[epoch 16] loss: 0.6580, acc: 0.7149\n",
            "[epoch 16] loss: 0.6589, acc: 0.7147\n",
            "[epoch 16] loss: 0.6572, acc: 0.7153\n",
            "[epoch 16] loss: 0.6563, acc: 0.7156\n",
            "[epoch 16] loss: 0.6553, acc: 0.7159\n",
            "[epoch 16] loss: 0.6564, acc: 0.7152\n",
            "[epoch 16] loss: 0.6579, acc: 0.7145\n",
            "[epoch 16] loss: 0.6581, acc: 0.7147\n",
            "[epoch 16] loss: 0.6589, acc: 0.7144\n",
            "[epoch 16] loss: 0.6588, acc: 0.7140\n",
            "[epoch 16] loss: 0.6582, acc: 0.7147\n",
            "[epoch 16] loss: 0.6599, acc: 0.7135\n",
            "> val_acc: 0.6637, val_f1: 0.6637\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6637\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 17\n",
            "[epoch 17] loss: 0.6409, acc: 0.7100\n",
            "[epoch 17] loss: 0.6880, acc: 0.7086\n",
            "[epoch 17] loss: 0.6400, acc: 0.7300\n",
            "[epoch 17] loss: 0.6395, acc: 0.7224\n",
            "[epoch 17] loss: 0.6492, acc: 0.7209\n",
            "[epoch 17] loss: 0.6572, acc: 0.7104\n",
            "[epoch 17] loss: 0.6496, acc: 0.7106\n",
            "[epoch 17] loss: 0.6476, acc: 0.7103\n",
            "[epoch 17] loss: 0.6433, acc: 0.7167\n",
            "[epoch 17] loss: 0.6429, acc: 0.7162\n",
            "[epoch 17] loss: 0.6473, acc: 0.7119\n",
            "[epoch 17] loss: 0.6483, acc: 0.7179\n",
            "[epoch 17] loss: 0.6438, acc: 0.7187\n",
            "[epoch 17] loss: 0.6420, acc: 0.7197\n",
            "[epoch 17] loss: 0.6477, acc: 0.7211\n",
            "[epoch 17] loss: 0.6496, acc: 0.7210\n",
            "[epoch 17] loss: 0.6544, acc: 0.7200\n",
            "[epoch 17] loss: 0.6534, acc: 0.7214\n",
            "[epoch 17] loss: 0.6563, acc: 0.7202\n",
            "[epoch 17] loss: 0.6559, acc: 0.7200\n",
            "[epoch 17] loss: 0.6544, acc: 0.7202\n",
            "[epoch 17] loss: 0.6504, acc: 0.7222\n",
            "[epoch 17] loss: 0.6447, acc: 0.7243\n",
            "[epoch 17] loss: 0.6457, acc: 0.7236\n",
            "[epoch 17] loss: 0.6447, acc: 0.7234\n",
            "[epoch 17] loss: 0.6462, acc: 0.7222\n",
            "[epoch 17] loss: 0.6445, acc: 0.7220\n",
            "[epoch 17] loss: 0.6428, acc: 0.7228\n",
            "[epoch 17] loss: 0.6418, acc: 0.7221\n",
            "[epoch 17] loss: 0.6442, acc: 0.7208\n",
            "[epoch 17] loss: 0.6478, acc: 0.7183\n",
            "[epoch 17] loss: 0.6488, acc: 0.7177\n",
            "[epoch 17] loss: 0.6502, acc: 0.7158\n",
            "[epoch 17] loss: 0.6494, acc: 0.7158\n",
            "[epoch 17] loss: 0.6489, acc: 0.7166\n",
            "[epoch 17] loss: 0.6491, acc: 0.7167\n",
            "[epoch 17] loss: 0.6514, acc: 0.7163\n",
            "[epoch 17] loss: 0.6498, acc: 0.7172\n",
            "[epoch 17] loss: 0.6507, acc: 0.7168\n",
            "[epoch 17] loss: 0.6505, acc: 0.7168\n",
            "[epoch 17] loss: 0.6520, acc: 0.7160\n",
            "[epoch 17] loss: 0.6527, acc: 0.7153\n",
            "[epoch 17] loss: 0.6522, acc: 0.7154\n",
            "[epoch 17] loss: 0.6537, acc: 0.7145\n",
            "[epoch 17] loss: 0.6552, acc: 0.7136\n",
            "> val_acc: 0.6644, val_f1: 0.6644\n",
            ">> saved: drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/ATAE_LSTM_MAMS_val_acc_0.6644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 18\n",
            "[epoch 18] loss: 0.7102, acc: 0.7200\n",
            "[epoch 18] loss: 0.6456, acc: 0.7175\n",
            "[epoch 18] loss: 0.6466, acc: 0.7154\n",
            "[epoch 18] loss: 0.6398, acc: 0.7156\n",
            "[epoch 18] loss: 0.6301, acc: 0.7270\n",
            "[epoch 18] loss: 0.6405, acc: 0.7293\n",
            "[epoch 18] loss: 0.6426, acc: 0.7291\n",
            "[epoch 18] loss: 0.6537, acc: 0.7242\n",
            "[epoch 18] loss: 0.6540, acc: 0.7214\n",
            "[epoch 18] loss: 0.6632, acc: 0.7163\n",
            "[epoch 18] loss: 0.6576, acc: 0.7181\n",
            "[epoch 18] loss: 0.6548, acc: 0.7210\n",
            "[epoch 18] loss: 0.6568, acc: 0.7178\n",
            "[epoch 18] loss: 0.6504, acc: 0.7206\n",
            "[epoch 18] loss: 0.6503, acc: 0.7192\n",
            "[epoch 18] loss: 0.6481, acc: 0.7192\n",
            "[epoch 18] loss: 0.6523, acc: 0.7171\n",
            "[epoch 18] loss: 0.6501, acc: 0.7180\n",
            "[epoch 18] loss: 0.6516, acc: 0.7161\n",
            "[epoch 18] loss: 0.6507, acc: 0.7165\n",
            "[epoch 18] loss: 0.6470, acc: 0.7188\n",
            "[epoch 18] loss: 0.6490, acc: 0.7181\n",
            "[epoch 18] loss: 0.6494, acc: 0.7177\n",
            "[epoch 18] loss: 0.6471, acc: 0.7190\n",
            "[epoch 18] loss: 0.6446, acc: 0.7197\n",
            "[epoch 18] loss: 0.6449, acc: 0.7194\n",
            "[epoch 18] loss: 0.6466, acc: 0.7183\n",
            "[epoch 18] loss: 0.6456, acc: 0.7187\n",
            "[epoch 18] loss: 0.6457, acc: 0.7183\n",
            "[epoch 18] loss: 0.6452, acc: 0.7195\n",
            "[epoch 18] loss: 0.6431, acc: 0.7203\n",
            "[epoch 18] loss: 0.6425, acc: 0.7211\n",
            "[epoch 18] loss: 0.6421, acc: 0.7221\n",
            "[epoch 18] loss: 0.6428, acc: 0.7214\n",
            "[epoch 18] loss: 0.6440, acc: 0.7202\n",
            "[epoch 18] loss: 0.6442, acc: 0.7210\n",
            "[epoch 18] loss: 0.6430, acc: 0.7219\n",
            "[epoch 18] loss: 0.6441, acc: 0.7203\n",
            "[epoch 18] loss: 0.6436, acc: 0.7204\n",
            "[epoch 18] loss: 0.6451, acc: 0.7196\n",
            "[epoch 18] loss: 0.6459, acc: 0.7189\n",
            "[epoch 18] loss: 0.6469, acc: 0.7188\n",
            "[epoch 18] loss: 0.6470, acc: 0.7189\n",
            "[epoch 18] loss: 0.6470, acc: 0.7194\n",
            "[epoch 18] loss: 0.6460, acc: 0.7194\n",
            "> val_acc: 0.6644, val_f1: 0.6644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 19\n",
            "[epoch 19] loss: 0.6447, acc: 0.6850\n",
            "[epoch 19] loss: 0.6324, acc: 0.6933\n",
            "[epoch 19] loss: 0.6199, acc: 0.7057\n",
            "[epoch 19] loss: 0.6211, acc: 0.7221\n",
            "[epoch 19] loss: 0.6372, acc: 0.7167\n",
            "[epoch 19] loss: 0.6323, acc: 0.7200\n",
            "[epoch 19] loss: 0.6401, acc: 0.7171\n",
            "[epoch 19] loss: 0.6322, acc: 0.7174\n",
            "[epoch 19] loss: 0.6285, acc: 0.7186\n",
            "[epoch 19] loss: 0.6358, acc: 0.7143\n",
            "[epoch 19] loss: 0.6370, acc: 0.7133\n",
            "[epoch 19] loss: 0.6404, acc: 0.7142\n",
            "[epoch 19] loss: 0.6363, acc: 0.7147\n",
            "[epoch 19] loss: 0.6371, acc: 0.7151\n",
            "[epoch 19] loss: 0.6377, acc: 0.7157\n",
            "[epoch 19] loss: 0.6356, acc: 0.7165\n",
            "[epoch 19] loss: 0.6376, acc: 0.7155\n",
            "[epoch 19] loss: 0.6388, acc: 0.7160\n",
            "[epoch 19] loss: 0.6346, acc: 0.7183\n",
            "[epoch 19] loss: 0.6328, acc: 0.7188\n",
            "[epoch 19] loss: 0.6338, acc: 0.7190\n",
            "[epoch 19] loss: 0.6317, acc: 0.7213\n",
            "[epoch 19] loss: 0.6342, acc: 0.7200\n",
            "[epoch 19] loss: 0.6352, acc: 0.7207\n",
            "[epoch 19] loss: 0.6366, acc: 0.7198\n",
            "[epoch 19] loss: 0.6365, acc: 0.7206\n",
            "[epoch 19] loss: 0.6379, acc: 0.7196\n",
            "[epoch 19] loss: 0.6353, acc: 0.7224\n",
            "[epoch 19] loss: 0.6354, acc: 0.7231\n",
            "[epoch 19] loss: 0.6373, acc: 0.7221\n",
            "[epoch 19] loss: 0.6370, acc: 0.7218\n",
            "[epoch 19] loss: 0.6363, acc: 0.7224\n",
            "[epoch 19] loss: 0.6374, acc: 0.7216\n",
            "[epoch 19] loss: 0.6391, acc: 0.7211\n",
            "[epoch 19] loss: 0.6396, acc: 0.7207\n",
            "[epoch 19] loss: 0.6387, acc: 0.7217\n",
            "[epoch 19] loss: 0.6388, acc: 0.7218\n",
            "[epoch 19] loss: 0.6377, acc: 0.7229\n",
            "[epoch 19] loss: 0.6383, acc: 0.7233\n",
            "[epoch 19] loss: 0.6374, acc: 0.7239\n",
            "[epoch 19] loss: 0.6379, acc: 0.7239\n",
            "[epoch 19] loss: 0.6379, acc: 0.7242\n",
            "[epoch 19] loss: 0.6383, acc: 0.7248\n",
            "[epoch 19] loss: 0.6371, acc: 0.7247\n",
            "[epoch 19] loss: 0.6387, acc: 0.7240\n",
            "> val_acc: 0.6644, val_f1: 0.6644\n",
            ">> test_acc: 0.6564\n",
            ">> test_f1_macro: 0.6325\n",
            ">> test_f1_micro: 0.6564\n",
            ">> test_f1_weighted: 0.6495\n",
            ">> test_precision_macro: 0.6387\n",
            ">> test_precision_micro: 0.6564\n",
            ">> test_precision_weighted: 0.6499\n",
            ">> test_recall_macro: 0.6338\n",
            ">> test_recall_micro: 0.6564\n",
            ">> test_recall_weighted: 0.6564\n",
            "confusion matrix:\n",
            "[[216  64  49]\n",
            " [ 51 478  78]\n",
            " [ 81 136 183]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAD4CAYAAAAejHvMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVhV1frA8e/iMMkok6iAU4oKAg6ImhOmlqU5ZklZVy0r62raravVbdZf1u1Wdm/pNTOza1pZmpZpDjnkkIIzzgMqDigOICLjWb8/9gGRUA4KHIb38zzn4ex99t7n3bjdvq79rrWU1hohhBBCCCHENXa2DkAIIYQQQoiKRpJkIYQQQgghCpEkWQghhBBCiEIkSRZCCCGEEKIQSZKFEEIIIYQoxN7WARTm6+urGzRoYOswhBDilsTFxSVrrf1sHUd5kvu2EKKyutk9u8IlyQ0aNCA2NtbWYQghxC1RSh2zdQzlTe7bQojK6mb3bCm3EEIIIYQQohBJkoUQQgghhChEkmQhhBBCCCEKqXA1yUJUN9nZ2SQmJpKRkWHrUEQJODs7ExgYiIODg61DqZDkuhaFyd8ZUdlIkiyEjSUmJuLu7k6DBg1QStk6HGEFrTXnz58nMTGRhg0b2iwOpVQvYApgAmZorScX+vxDoJtl0QWopbWuqZRqCUwFPIBcYJLW+hvLPrOArkCKZb9hWuvtJY1NrmtRUEX5OyNESUiSLISNZWRkSCJRySil8PHx4dy5c7aMwQR8AvQEEoEtSqlFWus9edtorccV2H400MqymA48prU+qJSqC8QppZZprS9ZPn9Raz3/duKT61oUVBH+zghRUlKTLEQFIIlE5VMB/syigENa6yNa6yxgHtDvJtvHAHMBtNYHtNYHLe9PAWeBUh/buQL8jkQFIteDqGysSpKVUr2UUvuVUoeUUhOK+Ly+UmqlUmqnUmq1UiqwwGe5Sqntltei0gw+T2zCBd5buq8sDi2EEBVVAHCiwHKiZd2fKKXqAw2BVUV8FgU4AocLrJ5kuZ9/qJRyusExn1RKxSqlYqV1UAhhK1cyc/hl12mmrDhY6scuNkku8EjvXiAEiFFKhRTa7H1gttY6HHgLeKfAZ1e11i0tr76lFPd1diam8OnqwySlSgcRIUrq/PnztGzZkpYtW1K7dm0CAgLyl7Oysm66b2xsLGPGjCn2O+68885SiXX16tX06dOnVI5VzQwB5mutcwuuVErVAb4ChmutzZbVLwHNgLaANzC+qANqradrrSO11pF+fhVvgsHKdF3nGTt2LAEBAZjN5uI3FqIaO3s5g7mbjzP8i820ens5o+Zs5cuNCaRn5ZTq91hTk5z/SA9AKZX3SG9PgW1CgOct738DFpZmkMUJD/QEYFdiCv4hzuX51UJUej4+PmzfbvTLeuONN3Bzc+OFF17I/zwnJwd7+6JvFZGRkURGRhb7HRs2bCidYEVBJ4GgAsuBlnVFGQI8W3CFUsoD+Bl4RWu9KW+91vq05W2mUuoL4AUqocp2XZvNZhYsWEBQUBBr1qyhW7duxe90C2523kJUZIfOpvHrnjMs35PE9hOX0BqCvGswtF19eob407aBF/am0q0ituZo1jzS2wEMtLwfALgrpXwsy86WR3KblFL9i/qC231sF1LXAzsFu06mFL+xEKJYw4YN4+mnn6Zdu3b8/e9/Z/PmzXTo0IFWrVpx5513sn//fuD6lt033niDESNGEB0dTaNGjfj444/zj+fm5pa/fXR0NA888ADNmjXjkUceQWsNwJIlS2jWrBlt2rRhzJgxJWoxnjt3LmFhYbRo0YLx442Gz9zcXIYNG0aLFi0ICwvjww8/BODjjz8mJCSE8PBwhgwZcvu/LNvZAjRRSjVUSjliJMJ/KmlTSjUDvICNBdY5AgswngDOL7R9HctPBfQHdpfZGZSzinxdr169mtDQUEaNGsXcuXPz1yclJTFgwAAiIiKIiIjIT8xnz55NeHg4ERERPProo/nnN3/+tT/OgvF17tyZvn37EhJiPAju378/bdq0ITQ0lOnTp+fvs3TpUlq3bk1ERATdu3fHbDbTpEmT/A53ZrOZxo0bSwc8UeZyzZrYhAu8s2Qvd72/mh4frOG9pfvJydWM6xHM0rGdWftiN167P4QOd/iUeoIMpTe6xQvAf5RSw4C1GK0ZeY/16mutTyqlGgGrlFK7tNYFa9/QWk8HpgNERkbqkn65i6M9d/i5sVuSZFHJvbk4nj2nUkv1mCF1PXj9/tAS75eYmMiGDRswmUykpqaybt067O3tWbFiBS+//DLff//9n/bZt28fv/32G5cvX6Zp06aMGjXqT2Oibtu2jfj4eOrWrUvHjh1Zv349kZGRPPXUU6xdu5aGDRsSExNjdZynTp1i/PjxxMXF4eXlxd13383ChQsJCgri5MmT7N5t5HiXLhkDN0yePJmjR4/i5OSUv64y0lrnKKX+CizDGAJuptY6Xin1FhCrtc5LmIcA83Re1mZ4EOgC+Fju23BtqLc5Sik/QAHbgadvN1a5rou/rufOnUtMTAz9+vXj5ZdfJjs7GwcHB8aMGUPXrl1ZsGABubm5pKWlER8fz8SJE9mwYQO+vr5cuHCh2PPeunUru3fvzh9+bebMmXh7e3P16lXatm3LoEGDMJvNjBw5Mj/eCxcuYGdnx9ChQ5kzZw5jx45lxYoVREREUBFLbETll5Gdy+8Hk1m+J4mV+5JITsvCwaRo38iH4R0b0CPEnzqeNcotHmuS5GIf6Vl6Rw8EUEq5AYPyhhLSWp+0/DyilFqNMQTRdUlyaQgL9GTdweTSPqwQ1dbgwYMxmUwApKSk8Je//IWDBw+ilCI7O7vIfXr37o2TkxNOTk7UqlWLpKQkAgMDr9smKioqf13Lli1JSEjAzc2NRo0a5f8DHhMTc13r1s1s2bKF6Ojo/H+0H3nkEdauXcurr77KkSNHGD16NL179+buu+8GIDw8nEceeYT+/fvTv3+RD7cqDa31EmBJoXWvFVp+o4j9/gf87wbHvKsUQ6xwKuJ1nZWVxZIlS/jggw9wd3enXbt2LFu2jD59+rBq1Spmz54NgMlkwtPTk9mzZzN48GB8fX0B8Pb2Lva8o6Kirhuf+OOPP2bBggUAnDhxgoMHD3Lu3Dm6dOmSv13ecUeMGEG/fv0YO3YsM2fOZPjw4cV+nxDW0lqz/cQlvtlygsU7TnElKxd3J3uim9WiZ4g/0U398HC2zQQ01iTJ+Y/0MJLjIcDDBTdQSvkCFywdP14CZlrWewHpWutMyzYdgfdKMf58YQGe/LD1JEmpGfh7SF2yqJxupWWsrLi6uua/f/XVV+nWrRsLFiwgISGB6OjoIvdxcro2EILJZCIn58+dKKzZpjR4eXmxY8cOli1bxrRp0/j222+ZOXMmP//8M2vXrmXx4sVMmjSJXbt2SY1mGZPr+uaWLVvGpUuXCAsLAyA9PZ0aNWqUuJOqvb19fqc/s9l8XQfFgue9evVqVqxYwcaNG3FxcSE6OvqmMyMGBQXh7+/PqlWr2Lx5M3PmzClRXEIU5VJ6Fgu2neSbLSfYd+YyNRxM9Amvw/0RdWnfyAdHe9uPUlxsBFrrHCDvkd5e4Nu8R3pKqbzRKqKB/UqpA4A/MMmyvjkQq5TagdGhb3LBge5LU1jAtc57QojSlZKSQkCA0RVh1qxZpX78pk2bcuTIERISEgD45ptvrN43KiqKNWvWkJycTG5uLnPnzqVr164kJydjNpsZNGgQEydOZOvWrZjNZk6cOEG3bt149913SUlJIS0trdTPR1QOFeW6njt3LjNmzCAhIYGEhASOHj3K8uXLSU9Pp3v37kydOhUw6uxTUlK46667+O677zh//jxAfrlFgwYNiIuLA2DRokU3bBlPSUnBy8sLFxcX9u3bx6ZNRr/N9u3bs3btWo4ePXrdcQGeeOIJhg4del1LvBAlpbVm4+HzjJ23jaj/W8mbi/fgaG/H/w0IY/Mr3fnn4Ai6BPtViAQZrKxJLu6RnqXjx59mZ9JabwDCbjNGq+R13tt5MoUeIf7l8ZVCVBt///vf+ctf/sLEiRPp3bt3qR+/Ro0afPrpp/Tq1QtXV1fatm17w21Xrlx53aPu7777jsmTJ9OtWze01vTu3Zt+/fqxY8cOhg8fnt+y9s4775Cbm8vQoUNJSUlBa82YMWOoWbNmqZ+PqBwqwnWdnp7O0qVLmTZtWv46V1dXOnXqxOLFi5kyZQpPPvkkn3/+OSaTialTp9KhQwdeeeUVunbtislkolWrVsyaNYuRI0fSr18/IiIi8r+zKL169WLatGk0b96cpk2b0r59ewD8/PyYPn06AwcOxGw2U6tWLZYvXw5A3759GT58uJRaiFty7nIm8+MS+WbLcRLOp+PubM+QtkE81DaI0Lqetg7vhtT1fTlsLzIyUsfGxt7Svnd/uIZALxdmDrvxP7BCVDR79+6lefPmtg7D5tLS0nBzc0NrzbPPPkuTJk0YN25c8TvaUFF/dkqpOK118eOHVSFF3bflujZUxuu6KLGxsYwbN45169bd1nHkuqg+cs2atQfPMW/zcVbuPUuOWRPV0JshbYO4L6wOzg4V44nEze7ZVaoQr0WAdN4TorL67LPP+PLLL8nKyqJVq1Y89dRTtg5JiNtWFa7ryZMnM3XqVKlFFjdkNmtSM7JJTsviwpUs1h9K5rvYE5xKycDH1ZERnRryUNsg7vBzs3WoJVKlkuRw6bwnRKU1bty4StnCJsTNVIXresKECUyYMMHWYYhylpqRzdnUTC5cyeJ8WibnrxgJ8IUrWSSnZRZ4n8XF9CxyzdcqE5SCzk38+EefEHo0968wNcYlVaWS5DDLzHs7E1PoKTPvCSGEEEJYRWvNgaQ0lltmtdtxg4EQ3J3t8XF1xMfNiSBvF1oG1cTbsuzj6oi3qyONa7lRt2b5jWdcVqpUkhxSxzN/5r2e0nlPCCGEEOKGcnLNxB27yPI9Sfy6J4njF9IBiAiqyfM9g6nv44K3JfH1dXPCy8Wx0rYK34oqlSTXcDTRpJa7zLwnhBBCCFGE9Kwc1h1M5tf4JFbtS+JiejaOJjvubOzDU10b0aO5v5SsWlSpJBmMzntrDpxDa41SytbhCCGEEELYVHJaJiv3JrF8TxLrDiaTmWPGw9meu5rVomdIbbo29cPNqcqlhLetyrWZhwV4kJyWSVJqpq1DEaJS6NatG8uWLbtu3UcffcSoUaNuuE90dDR5Q37dd999XLp06U/bvPHGG7z//vs3/e6FCxeyZ8+1+YVee+01VqxYUZLwi7R69eoSz1YmqpaqeF3nGTt2LAEBAfljgAtRlLOXM5ix7ggPTN1A20krGP/9LvaevkxMVD2+fqIdca/25KMhregdXkcS5Buocr+VvM57u06mUNtTHhcIUZyYmBjmzZvHPffck79u3rx5vPeedTPIL1mypPiNbmDhwoX06dOHkJAQAN56661bPpYQBVXV69psNrNgwQKCgoJYs2YN3bp1K7VjF5STkyPTtVdC6Vk5/BqfxA/bTvL7wXOYNYTU8eC57k3oGeJPSB0PecpeAlWuJTm/817in1sAhBB/9sADD/Dzzz+TlZUFQEJCAqdOnaJz586MGjWKyMhIQkNDef3114vcv0GDBiQnG+OTT5o0ieDgYDp16sT+/fvzt/nss89o27YtERERDBo0iPT0dDZs2MCiRYt48cUXadmyJYcPH2bYsGHMn29M3rly5UpatWpFWFgYI0aMIDMzM//7Xn/9dVq3bk1YWBj79u2z+lznzp1LWFgYLVq0YPz48YAx1e+wYcNo0aIFYWFhfPjhhwB8/PHHhISEEB4ezpAhQ0r4WxW2VlWv69WrVxMaGsqoUaOYO3du/vqkpCQGDBhAREQEERERbNiwAYDZs2cTHh5OREQEjz76KMB18QC4ubnlH7tz58707ds3P8Hv378/bdq0ITQ0lOnTp+fvs3TpUlq3bk1ERATdu3fHbDbTpEkTzp07BxjJfOPGjfOXRdnJNWt+P5jM899up+3EFYz9ZjuHz6bxTHRjVjzflSXPdWZsj2BC63pKglxCVe6/iXmd93ZJ5z1RGf0yAc7sKt1j1g6Deyff8GNvb2+ioqL45Zdf6NevH/PmzePBBx9EKcWkSZPw9vYmNzeX7t27s3PnTsLDw4s8TlxcHPPmzWP79u3k5OTQunVr2rRpA8DAgQMZOXIkAP/4xz/4/PPPGT16NH379qVPnz488MAD1x0rIyODYcOGsXLlSoKDg3nssceYOnUqY8eOBcDX15etW7fy6aef8v777zNjxoxifw2nTp1i/PjxxMXF4eXlxd13383ChQsJCgri5MmT7N69GyD/EfvkyZM5evQoTk5ORT52FyUg1zVQOtf13LlziYmJoV+/frz88stkZ2fj4ODAmDFj6Nq1KwsWLCA3N5e0tDTi4+OZOHEiGzZswNfXlwsXLhT7a926dSu7d++mYcOGAMycORNvb2+uXr1K27ZtGTRoEGazmZEjR7J27VoaNmzIhQsXsLOzY+jQocyZM4exY8eyYsUKIiIi8PPzK/Y7xa3ZdyaVBVtPsnD7SZJSM3F3tuf+iLoMaBVA2wbe2NlJQny7qlxLMhid93adTKWiTbktREWV92gajEfSMTExAHz77be0bt2aVq1aER8ff12dZWHr1q1jwIABuLi44OHhQd++ffM/2717N507dyYsLIw5c+YQHx9/03j2799Pw4YNCQ4OBuAvf/kLa9euzf984MCBALRp04aEhASrznHLli1ER0fj5+eHvb09jzzyCGvXrqVRo0YcOXKE0aNHs3TpUjw8PAAIDw/nkUce4X//+588dq6kqtp1nZWVxZIlS+jfvz8eHh60a9cuv+561apV+fXWJpMJT09PVq1axeDBg/H19QWM/zgUJyoqKj9BBuOJSkREBO3bt+fEiRMcPHiQTZs20aVLl/zt8o47YsQIZs+eDRjJ9fDhw4v9PlEyZ1Mz+GztEe6dso5eH63j89+PEhbgyScPt2bLKz2YPCicdo18JEEuJVXyzh8e6Mn3WxM5k5pBHc/KP5i1qEZu0jJWlvr168e4cePYunUr6enptGnThqNHj/L++++zZcsWvLy8GDZsGBkZGbd0/GHDhrFw4UIiIiKYNWsWq1evvq14nZycACMZyMnJua1jeXl5sWPHDpYtW8a0adP49ttvmTlzJj///DNr165l8eLFTJo0iV27dlW4ZFkp1QuYApiAGVrryYU+/xDIK1p1AWpprWtaPvsL8A/LZxO11l9a1rcBZgE1gCXAc/p2WxzkurZKcdf1smXLuHTpEmFhYQCkp6dTo0aNEndStbe3z+/0Zzab80tSAFxdXfPfr169mhUrVrBx40ZcXFyIjo6+6e8qKCgIf39/Vq1axebNm2Ua65vQWpNr1mTnarJyzWTnvXI0Wbm5ZOXo/HVZuWZOXcrgx+0nWX8oGbOGlkE1eatfKL3D6uDj5mTr06myKtYdv5S0CLB03ktMkSRZCCu4ubnRrVs3RowYkd/alpqaiqurK56eniQlJfHLL78QHR19w2N06dKFYcOG8dJLL5GTk8PixYt56qmnALh8+TJ16tQhOzubOXPmEBAQAIC7uzuXL1/+07GaNm1KQkIChw4donHjxnz11Vd07dr1ts4xKiqKMWPGkJycjJeXF3PnzmX06NEkJyfj6OjIoEGDaNq0KUOHDsVsNnPixAm6detGp06dmDdvHmlpadSsWfO2YihNSikT8AnQE0gEtiilFmmt85tFtdbjCmw/Gmhlee8NvA5EAhqIs+x7EZgKjAT+wEiSewG/lMtJlbKqdl3PnTuXGTNm5J/LlStXaNiwIenp6XTv3j2/dCOv3OKuu+5iwIABPP/88/j4+HDhwgW8vb1p0KABcXFxPPjggyxatIjs7Owivy8lJQUvLy9cXFzYt28fmzZtAqB9+/Y888wzHD16NL/cIq81+YknnmDo0KE8+uijmEwmq8+tqjGbNccupLP3dGqB12VSrmbnJ8Ul/a9nkHcN/tqtMf1bBdDIz61sAhfXqZJJckgdD+wU7D6Zwt2htW0djhCVQkxMDAMGDMh/PB0REUGrVq1o1qwZQUFBdOzY8ab7t27dmoceeoiIiAhq1apF27Zt8z97++23adeuHX5+frRr1y4/gRgyZAgjR47k448/vq4jkbOzM1988QWDBw8mJyeHtm3b8vTTT5fofFauXElgYGD+8nfffcfkyZPp1q0bWmt69+5Nv3792LFjB8OHD89vWXvnnXfIzc1l6NChpKSkoLVmzJgxFSpBtogCDmmtjwAopeYB/YAb1Q7EYCTGAPcAy7XWFyz7Lgd6KaVWAx5a602W9bOB/lTSJBmqznWdnp7O0qVLmTZtWv46V1dXOnXqxOLFi5kyZQpPPvkkn3/+OSaTialTp9KhQwdeeeUVunbtislkolWrVsyaNYuRI0fSr18/IiIi6NWr13WtxwX16tWLadOm0bx5c5o2bUr79u0B8PPzY/r06QwcOBCz2UytWrVYvnw5AH379mX48OHVqtTiSmYO+85cvi4h3nfmMulZuQCY7BSNfF1pU98LP3cnHEx2OJoUDiY7HOztrlt2tCwb71X+e3dnexmZwgZURavbjYyM1HnjVN6OXh+tpbanM7OGR5VCVEKUnb1799K8eXNbhyFuQVF/dkqpOK11ZFl/t1LqAaCX1voJy/KjQDut9V+L2LY+sAkI1FrnKqVeAJy11hMtn78KXAVWA5O11j0s6zsD47XWN32eX9R9W67r6ik2NpZx48axbt26Ij+vzNeF1pqTl66y77QlIT5jtA4nnL+S3yrs7mxP8zoehFhezet40MTfDWeH6tuqXtHd7J5dJVuSwSi5WL3/rMy8J4QQMASYr7XOLa0DKqWeBJ4EqFevXmkdVlRikydPZurUqVWiFjk5LZMDZy6zP+ky+y0/DyalkZZ5rVa8vo8LzWt7MKBVAM3reNC8jjsBNWtIzlGFVNkkOSzAk/lx0nlPCFFlnQSCCiwHWtYVZQjwbKF9owvtu9qyPrDQ+iKPqbWeDkwHoyXZ+rBFVTVhwgQmTJhg6zBK5HJGNgeS0jiQlwyfucyBpMucv3KtM6OXiwNNa7szqHUAwbXdaVbbnaa1PWSWumqgyv4J58+8J533RCUgTzwqnwpQqrYFaKKUaoiRyA4BHi68kVKqGeAFbCywehnwf0opL8vy3cBLWusLSqlUpVR7jI57jwH/vtUA5boWBVWAvzPkmjW/7TvL/LhEdp1M4eSlq/mfuTiaCPZ3p0dz//xkONjfHV83R7mOq6kqmySH1PHAZKfYJZ33RAXn7OzM+fPn8fHxkRtxJaG15vz58zg7O9syhhyl1F8xEl4TMFNrHa+UeguI1Vovsmw6BJhXcBg3SzL8NkaiDfBWXic+4BmuDQH3C7fYaU+ua1GQrf/OJKVm8M2WE8zbfJxTKRn4uTtx5x0+POxfLz8ZDqhZQ8YXFtepskmys4OJJrXcZOY9UeEFBgaSmJgo07dWMs7OzteNnmELWuslGMO0FVz3WqHlN26w70xgZhHrY4EWtxubXNeisPL+O2M2a9YdSubrP46xYu9Zcs2azk18ee3+ULo3r4WDqUrOpyZKUZVNksGoS/5NOu+JCs7BweG6Ga6EqArkuha2kpyWyXexiczdfJzjF9LxcXVkZOdGxEQFUd+n6OHuhCiKVUmyFbM61cdokfADLgBDtdaJls+KnNWpPIQFevJdXCKnUzKoW1PqkoUQQoiqSGvNxiPn+fqP4yyLP0N2rqZ9I29evKcpd4f642QvQ7CJkis2SbZmVifgfWC21vpLpdRdwDvAo8XM6lTm8mfeO5kiSbIQQghRxVy8ksX3WxP5evNxjpy7gmcNBx7r0ICYqHo0riWz0onbY01LsjWzOoUAz1ve/wYstLwvclYnYO7th168vM57u0+mcI903hNCCCEqvXOXM/lt/1lW7T3Lqv1nycox06a+F/8a3Jje4XVk4g5RaqxJkgOAEwWWE4F2hbbZAQzEKMkYALgrpXxusG/ALUdbQnmd93YmSuc9IYQQojLSWrPndCor955l5b6z7DhxCYA6ns7EtA0ipl09mtX2sHGUoioqrY57LwD/UUoNA9ZijNlp9cxOZTlzU1iAJ6v2Sec9IYQQorK4mpXLhsPJrNxntBifSc1AKYgIrMnfegbTvbk/zeu4y7/rokxZkyQXO6uT1voURksySik3YJDW+pJS6kazOlFo/zKbuUk67wkhhBAV36lLV1m17yyr9p1l/aFkMnPMuDqa6BLsx13NahHdtBZ+7k62DlNUI9YkycXO6qSU8gUuaK3NwEtcG3uzyFmdSiNwa4VZOu/tTJTOe0IIIURFcj4tk9kbj7F8TxJ7TqcCEORdg5ioenRvXouoht4yMoWwmWKTZCtndYoG3lFKaYxyi2ct+95sVqdy0bxA571eLaTznhBCCGFrlzOymbHuKDPWHeFqdi6R9b156d5mdG9eizv83KSMQlQIVtUkFzerk9Z6PjD/BvsWOatTeZGZ94QQQoiKISM7lzl/HOeT3w5x4UoW94XV5vmeTWW4NlEhVekZ9/KEB3qyYq903hNCCCFsISfXzA/bTvLR8gOcSsmgU2NfXrynKRFBNW0dmhA3VC2S5LAAT76NTeRUSgYBUpcshBBClAutNcviz/DPZfs5fO4KEYGe/HNwBB0b+9o6NCGKVS2S5PyZ9xJTJEkWQgghysH6Q8m8t3QfOxJTaFzLjWlD23BPqL880RWVRrVIkpvX8cBeOu8JIYQQZW7HiUv8c9l+fj+UTF1PZ957IJyBrQKwN9nZOjQhSqRaJMnODiaa+LuzUzrvCSGEEDeUlJpBrlnjYLLD0WSHg73CwWSHvZ0qtgX40Nk0/vXrfn7ZfQZvV0de7RPCI+3qyTTRotKqFkkyQFiAh3TeE0IIIYpwPi2T1xfF89PO0zfcxtFkh6O9HQ4mI3F2KLBssrNj/5lUajiYGNujCY93aoi7s0M5noEQpa/6JMmBNaXznhCiSlFK9QKmYIxhP0NrPbmIbR4E3gA0sENr/bBSqhvwYYHNmgFDtNYLlVKzgP2PzzEAACAASURBVK5A3qO3YVrr7WV3FsLWft55mtd+3E1qRjbPRN9BPW8XsnPNZOVqsnPNZOeYr1+2vLJyrl/u0sSXJ7s0wsdNZsUTVUP1SZLzO+9dkiRZCFHpKaVMwCdATyAR2KKUWqS13lNgmyYYs5x21FpfVErVAtBa/wa0tGzjDRwCfi1w+Bct49+LKuzc5Uxe+3E3v+w+Q1iAJ3MGt6NZbQ9bhyVEhVFtkuRmtd2xt1PsOplCrxZ1bB2OEELcrijgkNb6CIBSah7QD9hTYJuRwCda64sAWuuzRRznAeAXrXV6GccrKgitNYt2nOL1RfGkZ+by915NebJzI+lYJ0Qh1eZvhLODiWB/d3adTLV1KEIIURoCgBMFlhMt6woKBoKVUuuVUpss5RmFDQHmFlo3SSm1Uyn1oVKqyGfnSqknlVKxSqnYc+fO3eo5iHJ2NjWDkbPjeG7edhr4uLLkuU48E91YEmQhilCt/laEBXiyK/ESWmtbhyKEEOXBHmgCRAMxwGdKqfwpzpRSdYAwYFmBfV7CqFFuC3gD44s6sNZ6utY6Umsd6efnVzbRi1KjtWZ+XCI9PljDuoPneOW+5nw/6k4a13K3dWhCVFjVKkluEejJxfRsTl66autQhBDidp0EggosB1rWFZQILNJaZ2utjwIHMJLmPA8CC7TW2XkrtNantSET+AKjrENUYqdTrjJi1hZe+G4Hwf7u/PJcZ0Z2aYTJTkZ6EuJmqlWSnNd5b7eMlyyEqPy2AE2UUg2VUo4YZROLCm2zEKMVGaWUL0b5xZECn8dQqNTC0rqMMsbK7A/sLovgRdnTWvPNluPc/cFaNh45z2t9QvjmqQ408nOzdWhCVArVpuMeXOu8tzNROu8JISo3rXWOUuqvGKUSJmCm1jpeKfUWEKu1XmT57G6l1B4gF2PUivMASqkGGC3Rawodeo5Syg9QwHbg6fI4H1G6Tl66yoTvd7LuYDLtGnrz3gPh1PdxtXVYQlQq1SpJvtZ5T1qShRCVn9Z6CbCk0LrXCrzXwPOWV+F9E/hzRz+01neVeqCi3GTlmPn6j2P8c9l+NPB2v1AeaVcfOymtEKLEqlWSDEbJxa97zsjMe0IIIaqMnFwzC7efYsrKA5y4cJVOjX15Z2AYQd4utg5NiEqr+iXJgZ58E3uCxItX5eYhhBCiUjObNUt2n+aD5Qc4cu4KYQGevD28BV2D/aQhSIjbVP2S5AKd9yRJFkIIURlprVm59yz/Wn6AvadTCfZ3Y9rQNtwT6i/JsRClpNolyU0LzLx3b5h03hNCCFF5aK1Zf+g87/+6n+0nLtHAx4UpQ1rSJ7yuDOkmSpfWkJ0OmWmQZXkV9b6odblZ5R+vyRFiCs+LdHuqXZLs7GCiaW3pvCeEEKJyiU24wD+X7eePoxcIqFmDdweFMbB1IA4yW17lcDEBHN3B1cfWkdzYpeNwYBns/wUSfofcTOv2s3cGRzdwdAUndyNhLe8nGqYiJwe9LdUuSQaj5GJpvHTeE0IIUfHtSkzh/V/3s+bAOfzcnXizbyhDooJwsjfZOjRhjdM7Yc27sO8nsHOA5vdDm2HQsEv5J5KFmc1wMg4O/GIkx0mWYdG9GxkxetSxJL9u4JT3071AQmxZZ3Kw6WmUlWqZJLcI8GTeFum8J4QQouLaf+YyHyzfz7L4JGq6OPDSvc14rEMDajhKcnzbcjLLvrWzYHLs5Ald/g6Zl2HH1xD/w7VENOJhcCvHqd0z0+DwKiMpPrgMrpwDZYJ67aHn29D0XvBtUvxxqoFqmSSHB0rnPSGEEBVPrlmzev9Z5vxxnN/2n8XN0Z5xPYIZ0akB7s5Vs7WuXFy9BMc3GiUECeuMBNbnDmjxAIQ9ULpJYeHkOPolaPc01KhpfN7jddjzI8TNguWvwcq3y751+boyinVGzbCTJzTpAcH3QuPu4OJd+t9byVmVJCulegFTMGZ1mqG1nlzo83rAl0BNyzYTtNZLLDM67QX2WzbdpLW2+exNTWu742BS7JTOe0IIISqApNQMvtlygnmbj3MqJQM/dydGd2vM8I4N8XJ1tHV4lU9RSTHaqFsNioKOzxllBmvehTWToU4EhA2G0IHg+ac5dqxzegesea9AcvwytHvqWnKcx6EGRAwxXmf3QtyXsGNu6bQum81w+RScPwTJB42fR9fB2Xjjc+87IOpJCO5ltBxX0TKJ0qKMCZlusoFSJuAA0BNIBLYAMVrrPQW2mQ5s01pPVUqFAEu01g0sSfJPWusW1gYUGRmpY2NjS3wiJdX743V4uzry1ePtyvy7hBDVh1IqTmsdaes4ylN53berGrNZ8/uhZOb8cYwVe8+Sa9Z0buLLw1H16BHiLx3ySqK4pLhBJ+MVEAkOztf2Sz0F8Qtg13dwahugoH5HCBsEIf2ta10tnBx3eLbo5Phmsq9ea10+vtFSu9wH2gyHBp3BrtC1cPUinD98fTKct5xz9dp2Dq5QtxU07WUkxlJG8Sc3u2db05IcBRzSWh+xHGwe0A/YU2AbDXhY3nsCp2493PIRHujJL7ul854QQojylZyWyXexiczdfJzjF9LxdnXkic4NiWlbjwa+rrYOr+LTGi4dg1PbIXFL0Ulx9ISik+LCPOoaSW2HZ40kc9d8I2H+aRwseREa9zBKMprea3RSK+j0Dlj9Luz/+eYtx9a4rnV5H2z9ErZ/bSTw3o2geV9IT4bkQ0YinJ58bV9lAq/64NPYKNfwucNIhn0ag3sd23cOrMSsSZIDgBMFlhOBws2vbwC/KqVGA65AjwKfNVRKbQNSgX9ordcV/gKl1JPAkwD16tWzOvjb0SLAk7mbpfOeEEKIsqe1ZuOR83z9x3GWxZ8hO1fTrqE3L9zTlHtC/WWkihspmBCf3n7t59WLxuclTYpvxucOiB4PXf8OZ3YayfLuH+DAUnBwMRLlsMHgWgvW/ctIjp1vMzkuSq1m0Osd6P4a7FkEcV/A+o+M7/VtAs3uMxJgH0si7NUA7KUkpyyUVse9GGCW1vpfSqkOwFdKqRbAaaCe1vq8UqoNsFApFaq1Ti24s9Z6OjAdjMd2pRTTTeXNvLdLOu8JIYQoI5fSs5gfl8jXm49z5NwVPGs48Gj7BjzcLojGtdxtHV7FUlxCbGcPtUKMTm51WkLdluDfAuxLeXxcpYwa5ToR0OMto/xh93yjVXf398Y2ZZEcF+ZQAyIeMl45WZII24A1SfJJIKjAcqBlXUGPA70AtNYblVLOgK/W+iyQaVkfp5Q6DAQDNi9ey+u8t+tkCvdJ5z0hhBCl6MKVLP679jCzNxzjanYurevV5F+DI+gdXgdnB2k1Boyk+PQO2LvY6ERXXEJcK/TWW4lvlZ0dNOhovHq9C0d+M0aKCBtcdslxUSRBtglrkuQtQBOlVEOM5HgI8HChbY4D3YFZSqnmgDNwTinlB1zQWucqpRoBTYAjpRb9bXCyt8y8lygz7wkhhCgdF69k8dm6I8zakMDV7Fz6RdTlqa530LyOR/E7VxfJh4yW2V3z4fzBipMQF8feEYLvsXUUohwVmyRrrXOUUn8FlmEM7zZTax2vlHoLiNVaLwL+BnymlBqH0YlvmNZaK6W6AG8ppbIBM/C01vpCmZ1NCYUFeLJkl3TeE0JUTsUNz2nZ5kGMfiMa2KG1ftiyPhfYZdnsuNa6r2V9Q2Ae4APEAY9qrbPK+FQqvZT0bGb8foQv1idwJSuHPuF1ea57YympyJN6yqjv3fWd0WKMMmqIOzwLIf1kjF5RIVlVk6y1XgIsKbTutQLv9wAdi9jve+D724yxzEjnPSFEZWUZnvMTCgzPqZRaVGh4zibAS0BHrfVFpVStAoe4qrVuWcSh3wU+1FrPU0pNwyinm1pmJ1LJpVzNZubvR5n5+1EuZ+bQO6wOz/VoQrC/JMekX4C9i4wW44TfAW20FN89CVoMNEaWEKICqxoz7mVnwPENcMddJdotPMCoJ9qZKJ33hBCVjjXDc44EPtFaXwSw9BO5IWU8UruLayV1X2K0QkuSXMjljGy+WJ/AjHVHSM3IoVdobZ7r0eTPZRWZl8HRrfoMw5V1xZjVbdd8OLQCzNnGKAzRE4yh1Hwb2zpCIaxWNZLkNe/Cho/hqbXgH2r1bsG13XAwKTYfPU/vcOm8J4SoVKwZnjMYQCm1HqMk4w2t9VLLZ85KqVggB5istV6IUWJxSWudU+CYRU4/ZouhOyuCtMwcvtyQwPS1R0i5mk3PEH/G9mhCaF3Paxvl5hjDhsXNMhLFoCh4cDa417ZZ3DeVmw0XE4zxdy+dAJ1b8mNobXS+278EstPBvS60f9pIjOtEVJ//JIgqpWokyXeONgbeXjwWRiz788w0N+Bkb+L+8Lr874/jDGgdSMugcuypKoQQZc8eo8N0NMbIRGuVUmFa60tAfa31SUun6lVKqV2A1T2ZbTF0py1dycxh9sZjTF97mIvp2XRvVouxPYIJCyyQHF88Btu+gq1fQdoZYyKHto8bk0JMj4YHv4KgtrY5Aa3h8hnLzGwHjYkz8mZqu5hwa4lxYTW8IPwhY+SHeh2s/rdYiIqqaiTJLt5GjdPCp41kOXK41bu+3jeUP45e4Ll52/h5TGfcnKrGr0QIUeVZMzxnIvCH1jobOKqUOoCRNG/RWp8E0FofUUqtBlph9CGpqZSyt7QmF3XMaufH7Sd5a/Eezl/JIrqpH2N7BF9rVMnNhgPLjAkfDq001jXpCW0+hCZ3g8nemFp43sPwxb3Q+31oM6xsA76YACe2WJLhQ9emLM5Ku7aNfQ1j8ozaLSB0gGVyCsvEFKZb/HfQ0f3W9xWiAqo6V3PEENg+B1a8Ds16g1ut4vcBPGs48OFDLRkyfSNvLornn4MjyjhQIYQoFdYMz7kQY7KnL5RSvhjlF0eUUl5AutY607K+I/CeZVSi34AHMEa4+AvwY/mcTsVjNmve/3U/n64+TOt6NZn+WCRt6nsZH/6p1biuMVNbq0ehZtD1B6rdAp5cDd8/DoufMybJuPe90h/7NvOyUX64aSqYcwAFNesZs7TV63AtEfZpDB4B0tIrRDGqTpKsFPT+AKbeCctegUGfWb1rVENvnu3WmH+vOkR001pSnyyEqPCsHJ5zGXC3UmoPkAu8aJkB9U7gv0opM2CHUZOc1+FvPDBPKTUR2AZ8Xs6nViFcycxh3Dfb+XVPEjFRQbzZtwWOKhf2/lSo1fju61uNb8TFGx6ZDyvfhPVT4Oye0qtT1hrifzD+7bt82kjU2z8D3o0q3ljDQlQiSuuKVUoWGRmpY2NvY0K+VZNg7Xvw2I/QKNrq3bJzzQyetpEj59JYOrYLdWvWuPUYhBDVllIqTmsdaes4ytNt37crmJOXrvLEl7HsP5PCpB7+DLkjE3X4N9j2v2utxq0fLbrV2Bq7f4AfnwUnD3joK6Nj3606uw9+eRGOrjU6yN33L9vVPQtRCd3snl31kuTsDJjaAVAwakOJ/hd97PwV7puyjhYBnnw9sj0mO+mNK4QoGUmSK6HMtPza3VNHdrNjeyxB5lM0c0zCPjuvjldZWo2HFd9qbI0zu+GbRyDl5K3VKRcsrXB0g+6vGrXPdjLltRAlcbN7dtUpt8jj4GyUXXzVH37/ELq9ZPWu9X1ceaNvKC/O38l/1x7mmWgZz1EIIaqMi8fg7N4Cndksr8un8zeprRXKzhfPeiHY1+l+rYbXvwW4+5deLLVbwMjfSl6nXFRpRY83wNW39GITQgBVMUkGuKObMQTN7x9A2ANGpwUrPdAmkNUHzvHBrwfo1NiX8EAZFk4IISqtlJOw+3vYPR9O77i2voa3kfzecRdm7ztYdKIGn+62w79BMz4eeicurqXcqa4o+XXKb8H6j4qvUy5cWmHLIeWEqAaqXrlFnrSz8J9I40by2KISDWSekp7NvVPW4uRg4qfRnXCVYeGEEFaScosKIP0C7FlozPp2bAOgoW5raDHIqP/1aWwkqBTuoFePN/uG4mhvg1EfblanLKUVQpSZ6lVukcetlvEI6qdxsPMbY4g4K3m6OPDBQy2J+WwTb/+0h8mDwsssTCGEEKUgM82Y7W3XfDi80hgCzTcYur1sJMc+d/xpl2sd9FJ5/f4Qht3ZAGWrmeFaDAS/ppbxlO+D+/5p1ClLaYUQNlN1k2SA1sOMmY6WvWJ0tLC0HFijfSMfRnW9g09XH6ZrsB/3hsmwcEIIUaHkZBnTPu/6Dvb/AjlXwSPQGP4sbDDUDrvhU8S4Yxd56qs4MrNzmTmsLdFNrRtbv0z5h1rqlJ+An8bCxv8YNdNSWiGETVTtJNnODvp8BP/tAivegL4fl2j3sT2C+f1QMhN+2EXLejWp4ynDwgkhhE2Zc+HYeiMx3rMIMi4Z9cUtHzb6oAS1L3aSjAXbEhk/fxd1ajoz78l2NK7lXk7BW8HFGx75DlZNhB3zjI7obYZJaYUQNlC1k2QwehB3eAY2/Nu4idZrb/WujvZ2TBnSivumrONv3+7gf4+3w06GhRNCiPJjzoWk3ZDwu/E6th4yUsDBFZr3gRYPGJ21TQ7FH6rADHrtG3kz9ZE2eJVHB72SsjNBj9eNlxDCZqp+kgwQ/RLELzTqk59aa9XNNE9DX1fe6BvC+O938dm6IzzV9c91bUIIIUrJjZJiMGaQC+kHjbpBcC9wdLH6sJk5uYyZu41l8UYHvbf6heJgkmmZhRA3Vj2SZEdXoxPE3CFGjVencSXa/cHIIH7bd473f91Px8a+tAjwLKNAhRCimrEmKW7QGep3BM+AW/oKrTXj5+9kWXwSr/YJYURHG3bQE0JUGtUjSQZoei806wOr34XQAeDVwOpdlVJMHhRGr48uMWbeNn4a3QkXx+rzqxNCiFKVetoYtaGMkuLCPlx+gIXbT/HC3cE83qlhqRxTCFH1Va9nTfe+a9R6LXnRmLWoBGq6OPLBgxEcTb7CxJ/3llGAQghRDaSegmUvw7l9RlI88DMYtwfGbIO+/4bwB0stQf429gQfrzrEg5GBPNtNZlEVQlivejWHegYaY2Yuexn2LjJuziVwZ2NfnuzSiP+uOULXYD/uCb3BrEhCCCFurE6EkRSXUiJ8I+sPJfPyD7vo1NiXSQPCpMRCCFEi1aslGSDqKWPszF/GQ0ZqiXf/W8+mtAjwYML3O0lKzSiDAIUQoooz2Zd5grz/zGWe/iqOO/zc+HRoa+mkJ4Qosep31zDZQ58pcPkM/DapxLs72tvx0UOtuJqdy9++3UFOrrkMghRCCHGrzqZmMGLWFpwdTcwc3hYPZ+tHNBJCiDxWJclKqV5Kqf1KqUNKqQlFfF5PKfWbUmqbUmqnUuq+Ap+9ZNlvv1LqntIM/pYFtoG2T8Dm6XBqW4l3b1zLjdfvD+X3Q8kMn7WFlKvZZRCkEELcXHH3Zss2Dyql9iil4pVSX1vWtVRKbbSs26mUeqjA9rOUUkeVUtstr5bldT6lIT0rh8e/jOViehZfDGtLQE2ZBEoIcWuKTZKVUibgE+BeIASIUUqFFNrsH8C3WutWwBDgU8u+IZblUKAX8KnleLbX/VVw9YPFY40hiEooJqoekweGsenIeQZ8sp7D59LKIEghhCiaNfdmpVQT4CWgo9Y6FBhr+SgdeMyyrhfwkVKqZoFdX9Rat7S8tpf1uZSWXLNmzNxtxJ9K4d8xrWS4TiHEbbGmJTkKOKS1PqK1zgLmAYV7vGnAw/LeEzhled8PmKe1ztRaHwUOWY5ne86e0OsdOL0dFjwNiXElHvFiSFQ95jzRnktXs+n/yXrWHjhXRsEKIcSfWHNvHgl8orW+CKC1Pmv5eUBrfdDy/hRwFvArt8jLgNaatxbHs2LvWd7oG0r35v62DkkIUclZkyQHACcKLCda1hX0BjBUKZUILAFGl2BflFJPKqVilVKx586VY6IZOhDajYI9P8KMu+A/kbDmn3DxmNWHiGrozY/PdiSgZg2GfbGZz38/ii5hsi2EELfAmvtrMBCslFqvlNqklOpV+CBKqSjAEThcYPUkSxnGh0opp6K+3Gb37RuYuT6BLzce44lODXmsQwNbhyOEqAJKq+NeDDBLax0I3Ad8pZSy+tha6+la60itdaSfXzk2ZigF906GFw8aY3O61YbfJsKUcJh5L8TNgquXij1MkLcL34+6kx7N/Xn7pz2M/34nmTklL+EQQohSZg80AaIx7tOfFSyrUErVAb4Chmut83ohvwQ0A9oC3sD4og5ss/t2EZbuPsPEn/fQK7Q2L9/X3KaxCCGqDmsS2ZNAUIHlQMu6gh4HvgXQWm8EnAFfK/e1PWdPaP0YDP8Zxu6Cu16F9GRY/By8HwzfPgb7lkBO1g0P4epkz7ShbRh9V2O+jU1k6Iw/SE7LLMeTEEJUM9bcXxOBRVrrbEvJ2wGMpBmllAfwM/CK1npT3g5a69PakAl8QUUpkbuB7ScuMfabbYQH1uTDh1piZydjIQshSoc1SfIWoIlSqqFSyhGjI96iQtscB7oDKKWaYyTJ5yzbDVFKOSmlGmLcnDeXVvBlomY96PICPLsZRv4GbYYZU6fOi4F/NTVm67tB/bKdneJvdzfl3zGt2JmYQr//rGfPqZKPxSyEEFaw5t68EKMVGaWUL0b5xRHL9guA2Vrr+QV3sLQuo4yZN/oDu8vyJG7HiQvpPPHlFvzcnZjxWCQ1HCtGv3AhRNVQbJKstc4B/gosA/ZijGIRr5R6SynV17LZ34CRSqkdwFxgmKUlIh6jhXkPsBR4VmtdOeoQlIKA1nDfe/C3/RDzDTTqCnFfXqtfXvs+ZF7+0673R9Rl/tN3kmvWDJq6gaW7z9jgBIQQVZmV9+ZlwHml1B7gN4xRK84DDwJdgGFFDPU2Rym1C9iF8URwYjmeltVS0rMZ9sVmsnLMfDEsCj/3IkunhRDilqmK1sksMjJSx8bG2jqMG8tIMTr67fgGjv0Obv7Q400Ifwjsrv8/x9nUDEZ+FceOE5d4vmcwo+9qLNOiClHFKaXitNaRto6jPJX3fTsrx8xjM/8g7thFvnq8He0b+ZTbdwshqpab3bOr34x7t6tg/fITK8EzEBY+DZ/3NMowCqjl4cw3T7ZnQKsAPlh+gL/O3cbVrMrRkC6EEBWR1poJ3+9k05ELvPdAuCTIQogyI0ny7QiMhMdXQP+pkHLCKMNY+AxcTsrfxNnBxAcPRjDh3mYs2XWawf/dwOmUqzYMWgghKq/Pfz/KD9tO8nzPYAa0CrR1OEKIKkyS5NtlZwctH4bRcdDxOdj5Lfy7Dayfkj8ahlKKp7vewYzHIklITuf+f68nNuGCjQMXQojK59f4JMIDPRl9V2NbhyKEqOIkSS4tTu7Q8y149g9o0BGWvwaftocDv+Zv0r25Pz88cycujiYG/3cjbyyKJy0zx4ZBCyFE5XL4XBohdTykf4cQosxJklzafO6Ah7+BR+YbI2R8PRjmDIbkQwAE+7vz85hOPNa+Pl9uTKDnB2tYvifp5scUQgjBpfQszl/JopGfq61DEUJUA5Ikl5UmPWHURrh7IhzbaLQq//oPyEjF3dmBN/u14PtRd+Lh7MDI2bGM+l8cZ1MzbB21EEJUWIfPXQHgDj83G0cihKgOJEkuS/aOcOdoGLMVIh6CDf826pW3zQGzmdb1vPhpTCdevKcpK/edpfsHa5jzxzHM5oo1LJ8QQlQEh8+lAZIkCyHKh72tA6gW3GpBv08gcgT8Mh5+fAYWjQalcACeBZ5xhFyzRi8B8y+g7BRFVtzZ2YNPE6gdVuDVAmp4le85CSFEOTty7gqOJjsCvWrYOhQhRDUgSXJ5CmgDI36FPQsh6fqZXhVg0rDvTCrrDyWTla1pU9+LNvW9sLcrkC7nZMK5fXB4Jez4+tp6z3p/Tpxr1jfqooUQogo4fC6N+j4u2JvkIagQouxJklze7OygxUDjVYgCmgN+aZlM/GkP720/RaMUV94ZEEa7ogbMv5wESbvgTIHX/iWApVzDydNIlvMS55r1weRgtEbbmUCZLO8ty/nvC68zGcl5TgZkp0N2BuRcheyrxvvs9Bt8dhVcvKHNMGnpFkLctsPn0giu5W7rMIQQ1YQkyRWQr5sTHw1pRf9WAfxj4W4emr6JmKggJvRqjqeLw7UN3f2NV+Me19ZlXYGze+HMzmuJ89bZRgJb3uydjeR53YdGbXb7p42h8oQQooSyc80cP5/OvS1q2zoUIUQ1IUlyBRbdtBa/juvCRysOMmPdEZbvOcsbfUPoHVbnxmOEOroaMwEGFpiG3JwLF45A6inQucayOafAK7fQz8Lvc8DeyUh6HVzAwfLT3hkcahgv+xrXr7d3NlrNz+yCVZPgt4nwx1To9Dy0fdzYRwghrHT8Qjo5Zk0jX+m0J4QoH5IkV3Aujva8fF9z+kbUZcIPO/nr19uY1/gEr98fQhN/K1tl7Uzg28R4lbfaYfDwPEiMhVVvw6+vwMb/QJcXoNVjxgggpSErHQ4th/gFcHon1AmHoPZQrx34h4FJLnUhKrPDZy0jW9SSJFkIUT4kc6gkWgR4svCZjny16RgfLj9ArynreKxDfcb2CMazhkPxB7C1wEh47Ec4ug5WTYSf/2ZM3d11AoQ/dGtJbPZVOGhJjA8sg+wr4OJrfNeJzcZ6AAdXCGxzLWkObAvOnqV7fkKIMpU3RrJMJCKEKC+SJFci9iY7hndsSN+Iuvxr+QFmbUjgx+2nePGepjwYGYTJrhKMZNGwM4xYCodWGi3LPz4Dv38I3V6CkAFGicbNZGfAoRWWxHgpZKWBiw+EPwihA6B+x2sJ96UTcOIPOL4JTmyCde+DNgMK/EMhqB3Ua2/8rFnv5iOB5GRBxiW4etF4pV+49v7qRchIMbZTdsZxlJ3xyl9XYD3q+nXejaBZH3B0ud3fbvWmtdHBNDvdqM3PTjeWTQ5gcjRKhkxOxtMLk5OxLKO/VBpHzqXh5+6Eh3MlaBQQQlQJSuuKNXFFZGSkjo2NJ+tjjQAAIABJREFUtXUYlcLukym8uTieLQkXCa3rwZt9Q4ls4G3rsKynNez7yahZ/v/27jw+qur+//jrk52wJCEhISRAEkAgEtaIyGIREcEFbF0KdpHalvbb+rXa77etfNtaq11cqnbzZ6WKWqtFu6i0YhVRRJFdkVUwCVsCkhgWTUAg4fz+uDd1HBMywCQzgffz8biPzNx77tzP3JmcfHLuuedUbYSsAXDeD6HvpE8mL0c+8oa8W/80bHreS4zbdYbCyVB4GeSNCa0l+tCHUL4Cti/zkubyld5rAXTM9pLl5M5BSbCfGB/+sOnXtRhI7OT9dEe994XzH/vPGx4Hrg+U2AmKroShX4Zug4/3TIbmg12w7u+w9invcd4oKBjrLWl5LXPMk1V/BFY8BJUb/OT3gHfF4PCBTybDDeuDz2tzYhOCEueAn91HwMW/Ou6QzWyVc664+ZInz8wmAr8BYoEHnXO3N1LmKuAWvGFv3nbOXe2vvwb4kV/sZ865R/31w4BHgHbAPOA7rpk/FK1Rb3/u/y0mIS6GOTPOadHjiMjp5Vh1tpLkNs45xz/X7OIXz23kvQ8+Ysrgbtw0qR/ZKW3oxrij9V7y9sovYO8WyCn2Wpbrj3iJ8TvzvCS1XRr0v9RrMc4b47UQnoz6Oqhc/3HSvGO514WjXZqXLLdLC1g6Q7vUT65rKJPQsfkW8MY0JM/bl3gjkGx41hsNpGsRDL0Giq44+aHzPtoPG+Z6ifGW1wAH3YZ4E9JsfQ0+3OWVS8v7OGHO/4z33iLtvbXwzH95P9t3gYQO3o2p8cleq3t8e/9nctPr45Kg/rC31B0K+HnIuzpQ99Gn1zX8zDoTzv/xcYfdWkmymcUCm4ELgHJgBTDNObchoEwf4ClgnHNur5llOucqzawzsBIoxkueVwHD/DLLgeuBZXhJ8m+dc88fK5aWrredcwy+dT6XDMzm558tarHjiMjpR0nyaeDA4TruX1jKA4vKiDXj2+f14mtjCkiKj410aKGrPwKrn4BX74QPyr11Sal+YnyZl7ydbGIczQ7ug7V/hTcf9RLDuCToP9lrXc4bHXrXgLpD8O6LsOYpr692/SFIy/e6pBRd+fENnM7B+5uhbKG3bHnNbzE3yB70cdLcY0TrjkZSdxhevwcW3eX9k3DJvd53oI1oxST5HOAW59yF/vOZAM65XwaUuRPY7Jx7MGjfacBY59w3/OcPAAv95RXnXL/GyjWlpevt6ppDDPvZS/z4kkK+Ojq/xY4jIqefY9XZ6pN8ikhOiON/Jnh9k3/23AZ+9eJmnly5gx9dXMiEwqymh4yLJrHxMOwaGDQV1j/jtWYWjD21E+NA7VJh+Ne9ZedqeOsxWPNXrxW4cwEM+SIM/gJ0bGSc2KNHYdvrXpK94VmvBbl9F28il4FXebM9Bn8HzKBLX285+xtey/rONz9OmpfcB4t/7XU/6DHi46Q5e/CJtZyHYtcaeOZb3iQ5RVfCpDujo1U7OuUAOwKelwNnB5U5A8DMFuN1ybjFOffvJvbN8ZfyRtZ/ipnNAGYA9OjR44TfRCgabtrrpZv2RKQVKUk+xXTvnMwDXypmccn7/PSf6/nGY6sY0yeDmy85jiHjIi0uEQZ9PtJRRFa3wd5ywW2wca7XHWPBrV7/7TMu9FqXe1/g9dVd+xSs/Tt8uNPrktDvEhh4JeSPPb5RQ2LjoPtwb/nM9+FQDWx74+OkecFPvSW9NwyfAYOmQVKn8LzfusPw2t3ezZXtOsPnH4f+l4TntU9vcUAfYCyQCywys7D0V3DOzQJmgdeSHI7XbEpplT/8WxcN/yYirUdJ8ilqVO8Mnrt+DH8OGDLuSyN68q3zepHZMSnS4UmoEpK9lvVBU+H9Eq91efUT3vTj8e29m9Vi4rxZFyfcBn0vCt8oGYkd4IwJ3gLeNOglL8HK2fD892HBbTD4ai9hzuh94sfZ9bbferwOiq6CSXeo9Tg0FUD3gOe5/rpA5cAy59wRYIuZbcZLmivwEufAfRf663Obec1WV1ZVQ2JcDDmpbeheCxFp89Qn+TRQXXOIu+dv5skVO0iIjeGakXl849wC0tqHaSIPaV31R7w+x5vmeTfhFX4W2qe3bgzlq2D5A7DuH3D0iJekn/1N6HV+6F0x6g57/Y5fv8cbxu+Se6HfxS0bdytoxT7JcXg37p2Pl8iuAK52zq0PKDMR72a+a8wsA3gLGMzHN+sN9Yu+iXfj3p5Gbtz7nXNu3rFiael6+9pHVrBz30H+fcO5LXYMETk9qU/yaS69QyK/+GwRXx9TwG9e2swDi0r589JtfHV0Pl8dk69xR9ua2HgvmYxkQpk7DHJned1BVj0CKx+Cx6+Azr28luXBVx+7K8bO1V7rceV6bzKZiber9fg4OefqzOw64AW8/saznXPrzexWYKVzbq6/bYKZbQDqge8556oBzOw2vMQa4Fbn3B7/8bf4eAi45/0lokqrahiQowmARKR1hdSS3NxYnGZ2L3Ce/zQZyHTOpfrb6oG1/rbtzrnJxzqWWpJb3ubdH3Lv/M08v+49UtrF843PFDB9ZB7JCfqfSU5Q3WHvhsHlD3hjUSd0COiKETAdet0hr/X4tXugfQZc8mvod1Hk4m4BrTlOcrRoyXr7UF09/X/8b647rzffndC3RY4hIqevk2pJ9sfivI+AsTjNbG7gWJzOuRsDyv83MCTgJQ4651podgQ5EWdkdeT+Lw5jXcV+7pm/mTv/vYnZr2/hv8b25gtn92hbw8ZJdIhL8G4WHHglVKyCZbO8Fubls7wuGGd/00uKn/22d7PhwKkw8ZdqPZZmbas+wFEHvTJ1056ItK5QOg8OB0qcc2XOucPAHGDKMcpPA/4SjuCkZQ3ISWH29LP4+3+NpG/Xjtz2rw2MvWshjy3dxuG645y5TKRBzjD43ANw43pvBsXd6+GJK+GP53kzGU6b421XgiwhKK3UyBYiEhmhJMlNjaf5KWbWE8gHXg5YnWRmK81sqZld1sR+M/wyK6uqqkIMXcJlWM80Hv/aCJ74+tnkprXjx8+sY9zdC3lq5Q7q6pUsywnqkOkNJXfDWrj8ITjvR/Dtpd604yIhahj+LT9DYySLSOsKdyfUqcDfnHP1Aet6OucqzKwAeNnM1jrnSgN3as3xNqVpI3tlcM4303l1cxV3v7iZ7/9tDX9YWMp3xvfhkoHdiI1pAxOSSPSJS/Cm2BY5AWVVtWSnJNE+UfdMiEjrCqUlOZSxOBtMJairhXOuwv9ZhjcO55BP7ybRwswY2zeTudeN4oEvDSM+NobvzFnN+Hte5c9Lt/HRkfrmX0REJExKq2rU1UJEIiKUJHkF0MfM8s0sAS8RnhtcyMz6AWnAkoB1aWaW6D/OAEYBG4L3lehjZlx4Zlee/84Y7rt6KJ2S4vjRM+sYefvL/PqlzeypPRzpEEXkFOeco7SqVtNRi0hENHv9KsSxOMFLnue4T44p1x94wMyO4iXktweOiiHRLybGuHhgNhcVdWX5lj3MWlTGr196lz+8WsoVw3L52ugC8tRXUERaQNWHh6g5VEeBWpJFJAJC6uTlz7Y0L2jdzUHPb2lkvzeAopOIT6KEmXF2QTpnF6RTUvkhf1y0hadWlPP4su1cWNiVGZ8pYGiPtEiHKSKnkJIqjWwhIpGjOyHkuPXO7MgdVwzkfyacwaNLtvLYkm38e/17nJWXxtfHFDC+fxYxuslPRE5SaVUtAL0ydbVKRFpfKH2SRRqV2SmJ713YjyUzz+cnlxayc99HzHhsFePvfZW/LN+um/xE5KSUVdWQnBBL105JkQ5FRE5DSpLlpLVPjOMro/J59Xtj+e20ISQnxDLzH2sZfcfL/OqFTSwrq+ZQnRJmETk+pVW1FHRpj5muTIlI61N3CwmbuNgYJg/qxqUDs1lSVs2sRWXct7CE379SQmJcDMN6pnFOQTojeqUzKDeVhDj9jyYiTSutrKE4T/c6iEhkKEmWsDMzRvbKYGSvDPYdOMzyLXtYUlbN0rI93D1/M8yHpPgYint2ZkRBZ0YUpDNQSbOIBDh4uJ6d+w9SkNG9+cIiIi1ASbK0qNTkBCac2ZUJZ3YFYG/tYZZt2cPSsmqWllXzqxc3A9AuPpbivDRGFKT7SXMK8bFKmkVOV1ver8U53bQnIpGjJFlaVVr7BCYO6MrEAV7SvKf2MMu3eK3MS8uqueuFTQAkJ8Ty+bO6890LzqBjUnwkQxaRCCjV8G8iEmFKkiWiOrdPYOKAbCYOyAaguuYQy7fs4aWNlTzyxlaeW7OLmy8t5OKibN28I3IaKa2qwQzyNVmRiESIrmdLVEnvkMikomzuvmoQz3xrFJmdErnuibe45uEVbKuujXR4IlHFzCaa2SYzKzGzmxrZPt3Mqsxstb98zV9/XsC61Wb2kZld5m97xMy2BGwb3NrvC6Csqpac1HYkxcdG4vAiIkqSJXoN6p7Ks98ezS2XFvLmtr1MuHcRv1vwroaTEwHMLBa4D5gEFALTzKywkaJPOucG+8uDAM65VxrWAeOAA8CLAft8L2Cf1S38VhpVWlWjrhYiElFKkiWqxcYY00fls+B/PsP4wizunr+ZSb95jTdK3o90aCKRNhwocc6VOecOA3OAKSfwOlcAzzvnDoQ1upNw9KijzB8jWUQkUpQkS5uQ1SmJ+64eyqPXDqeu3nH1g8u48cnVVH14KNKhiURKDrAj4Hm5vy7Y5Wa2xsz+ZmaNjac2FfhL0Lqf+/vca2aJjR3czGaY2UozW1lVVXVCb6Ap733wEQeP1KslWUQiSkmytCmfOaMLL954LteP682/1uzk/LsX8uel2zh61EU6NJFo9E8gzzk3EJgPPBq40cyygSLghYDVM4F+wFlAZ+AHjb2wc26Wc67YOVfcpUuXsAatkS1EJBooSZY2Jyk+lu9O6Mvz3zmXM7ul8KNn1vG5+99g/c79kQ5NpDVVAIEtw7n+uv9wzlU75xoutzwIDAt6jauAp51zRwL22eU8h4CH8bp1tKrSSj9J1hjJIhJBSpKlzeqd2YEnvn42v/78YMr3HuDS373Obf/aQM2hukiHJtIaVgB9zCzfzBLwuk3MDSzgtxQ3mAxsDHqNaQR1tWjYx7wxFy8D1oU57maVvV9Lx8Q4unRotKeHiEir0DjJ0qaZGZcNyeG8vpnc+cI7zF68haffquCcXukM65HG0J5pFGZ30pTXcspxztWZ2XV4XSVigdnOufVmdiuw0jk3F7jezCYDdcAeYHrD/maWh9cS/WrQSz9uZl0AA1YD32zht/IppVU1FGR20NjoIhJRSpLllJCSHM/PP1vE5cNyeXjxVt7ctpfn1uwCIDEuhkG5qQztmcawnmkM7ZFKulqo5BTgnJsHzAtad3PA45l4fYwb23crjdzo55wbF94oj19pZS0je6dHOgwROc0pSZZTytAeaQztkQbArv0HeXPbPt7cvpdV2/by0Otl/OFV7wa//Iz2DO3hJc3DeqbRJ7MDMTFqtRKJtJpDdbz3wUe6aU9EIk5JspyyslPacfHAdlw80OuW+dGRetZW7GfVNi9pXripkr+/WQ5Ax6Q4hvRIY3z/TC4fmkv7RP1qiETClipvZs1eGiNZRCJMmYCcNpLiYzkrrzNn5XUGwDnHtuoDXtK8fS8rtuzh5mfX86sXNjFteA++PDKPnNR2EY5a5PSi4d9EJFooSZbTlpmRl9GevIz2XD4sF4BV2/Yye/EWHnzdWyYO6Mq1o/IZ1jMtwtGKnB5Kq2qIMeiRnhzpUETkNKckWSRAQx/lin0H+dMbW3li+XaeW7OLwd1TuXZ0PpMGdCU+ViNliLSUsqpaenROJjEuNtKhiMhpLqS/9mY20cw2mVmJmd3UyPZ7zWy1v2w2s30B264xs3f95ZpwBi/SUnJS2zHzov4snXk+P518JvsOHOb6v7zFuXe+wv0LS9l/4EjzLyIix620qkZdLUQkKjTbkmxmscB9wAVAObDCzOY65zY0lHHO3RhQ/r+BIf7jzsBPgGLAAav8ffeG9V2ItJD2iXFcMzKPL43oycvvVDJ78Rbu+Pc7/HbBu1wxLJfpo/L0B10kTOqPOsrer2VMn4xIhyIiElJ3i+FAiXOuDMDM5gBTgA1NlJ+GlxgDXAjMd87t8fedD0wkaIYnkWgXE2OML8xifGEWG3Z+wMOLt/Dkih08tnQb4/plcu2ofEb1TtfkByInYee+gxyuO6p/PEUkKoTS3SIH2BHwvJxGBqAHMLOeQD7w8vHsa2YzzGylma2sqqoKJW6RiCns1om7rhzE4pvGccP4Pqwp38cXH1rGhb9exBPLtnPwcH2kQxRpk0oaRrbIVJIsIpEX7juQpgJ/c84dV5bgnJvlnCt2zhV36dIlzCGJtIwuHRO5YfwZvP6Dcdx1xUDiYmL4v6fXMuKXC/jl8xup2Hcw0iGKtCmllV6SXJChMZJFJPJC6W5RAXQPeJ7rr2vMVODbQfuODdp3YejhiUS/pPhYrizuzhXDclmxdS8PL97CHxeV8eBrW7jwzCymj8znrLw0dcUQaUbZ+7WkJsfTuX1CpEMREQkpSV4B9DGzfLykdypwdXAhM+sHpAFLAla/APzCzBoGmZ0AzDypiEWilJkxPL8zw/M7U773AI8t3cac5TuYt/Y9zuzWia+MyufSQdka2kqkCaWV3sgW+odSRKJBs90tnHN1wHV4Ce9G4Cnn3Hozu9XMJgcUnQrMcc65gH33ALfhJdorgFsbbuITOZXlpiUzc1J/lswcx88/O4DDdUf537++zajbX+aeFzdR+cFHkQ5RJOqUVtVqOmoRiRohTSbinJsHzAtad3PQ81ua2Hc2MPsE4xNp05IT4vjC2T25engPFpdU8/DiLfzulRLuf7WUi4uymT4qn8HdUyMdpkjE7T94hPdrDlGgkS1EJEpoxj2RVmBmjO6Tweg+GWx9v5ZHl2zlryvLeWb1TopyUpgyuBuXDupGVqekSIcqEhFlDSNbKEkWkSih+XVFWlleRnt+cumZLJk5jlsuLcTh+NlzGxnxywVMm7WUOcu3a0Y/CUkIs6FON7OqgBlRvxawrT5g/dyA9flmtsx/zSfNrFXuoiutqgVQdwsRiRpqSRaJkI5J8Uwflc/0UfmUVtUwd/VO5r69k5v+sZYfP7uOz5yRyZTB3RjfP4t2CbrZTz4plNlQfU86565r5CUOOucGN7L+DuBe59wcM/sD8FXg/nDG3pjSqhriYozunZNb+lAiIiFRkiwSBXp16cCNF5zBDeP7sK7iA55dXcE/1+zkpY27SU6IZUJhFlMG5zC6TwbxsboAJMDxz4baLPOGlRjHxyMYPQrcQiskyWVVNfRMT9b3W0SihpJkkShiZhTlplCUm8LMi/qzfMse5r5dwby17/HM6p2kJcdzUVE2UwbnUNwzjZgYDZV1GmtsRtOzGyl3uZmdC2wGbnTONeyTZGYrgTrgdufcM0A6sM8f1ajhNZuaYXUGMAOgR48eJ/te/JEt1B9ZRKKHkmSRKBUbY5zTK51zeqXz08kDWLS5imff3sk/3qzg8WXbyU5JYnTvDIrz0ijO60xBRnuNLyvB/gn8xTl3yMy+gdcyPM7f1tM5V2FmBcDLZrYW2B/qCzvnZgGzAIqLi10zxY+prv4o26prGd8/62ReRkQkrJQki7QBCXExjC/MYnxhFrWH6nhp426eW7OLlzbu5q+rygHo3D6B4p5p/0maB3RLISFOl65PYc3Ohuqcqw54+iBwZ8C2Cv9nmZktBIYAfwdSzSzOb00+1gyrYbNj70GO1DvdtCciUUVJskgb0z4xjimDc5gyOAfnHKVVtazcuocVW/eyctseXtywG4DEuBgGd0/9T9I8tEcaKe3iIxy9hFGzs6GaWbZzbpf/dDLehFD4s6Ae8FuYM4BRwJ3OOWdmrwBXAHOAa4BnW/qNlFb6w79lqruFiEQPJckibZiZ0TuzA70zOzB1uNcvtPLDj1i1dS8rtu5l1bY9/OHVMupfKcUM+mZ1pDgvjWE90xiYm0p+env1a26jnHN1ZtYwG2osMLthNlRgpXNuLnC9PzNqHbAHmO7v3h94wMyO4g0FenvAqBg/AOaY2c+At4CHWvq9lDaMkZyhJFlEooeSZJFTTGbHJCYVZTOpKBuAA4frWL19Hyu37WXF1j0889ZO/rx0OwAdEuMYkNOJgbmpDMhJYWBOCj3Tk9W3uY1objZU59xMYGYj+70BFDXxmmV4I2e0mrKqWjI6JJCSrCsdIhI9lCSLnOKSE+IY2TuDkb0zAKg/6ni38kPWlu9nbcV+1pTv55E3tnK47igAnZLivBE2clIZmJtCUU4KuWntlDhLiymtqtF01CISdZQki5xmYmOMfl070a9rJ64s9u77OlJ/lM27vcR5TcV+1pbv56HXyzhS7w1akJYcT1FuKgNzUrh4YDb9sztF8i3IKaa0qoaJA7pGOgwRkU9QkiwixMfGcGa3FM7slsJUf92huno2vfcha8r3/yd5vv/VUn7/Sglj+mQw49wCRvfOUAuznJQ9tYfZe+CIxkgWkaijJFlEGpUYF8vA3FQG5qb+Z93+A0d4fPk2Hlm8lS89tJx+XTvy9TEFXDqom4abkxNS1nDTnpJkEYky+qsmIiFLSY7nW2N789oPzuOuKwZy1Dn+569vM+bOl/nDq6XsP3gk0iFKG9MwskWBxkgWkSijJFlEjltiXCxXFnfnhRvO5ZGvnEXvzA7c/vw7jPzlAm771wbK9x6IdIjSRpRV1ZIQG0NuWnKkQxER+QR1txCRE2ZmjO2bydi+mayr2M+Dr5XxyBtbeeSNrVxclM2McwsYkJMS6TAlipVW1ZCf0Z5YjdctIlFGLckiEhYDclL49dQhLPr+eVw7Ko+X36nkkt+9zrRZS3nlnUqOHnWRDlGiUGlVLb0y1dVCRKKPWpJFJKxyUtvxw4sL+e/z+zBn+XZmv76Vrzyygrz0ZMb2zWRkr3TOLkjXFNnC4bqjbN9zgIv9iW9ERKKJkmQRaRGdkuKZcW4vpo/M519rdvL0WxXMWbGdR97YSoxBUW4qI3ulM6pXBsV5aSTFx0Y6ZGll2/fUUn/UqSVZRKKSkmQRaVEJcTF8bmgunxuay6G6et7avo83St5ncWk1sxaVcf/CUhLiYhjWI41RvdMZ2TuDgTkpxMWqN9iprqSyFtDwbyISnZQki0irSYyLZURBOiMK0vkuUHOojuVbqnmjpJrFpdX86sXN8OJmOiTGcXZ+Z0b2zmBU73T6ZnXUpCWnoIbh3/Iz1JIsItFHSbKIREyHxDjG9ctiXL8sAKprDrGkrJrFJdUsKX2fBe9UAl4S9fmzunP50Fy6dEyMZMgSRmVVtWR1SqRjkvqni0j0CSlJNrOJwG+AWOBB59ztjZS5CrgFcMDbzrmr/fX1wFq/2Hbn3OQwxC0ip6D0DolcMrAblwzsBkD53gO8/u77/OPNCm5//h1+9cImLijMYurwHozpnUGMhg1r00qratTVQkSiVrNJspnFAvcBFwDlwAozm+uc2xBQpg8wExjlnNtrZpkBL3HQOTc4zHGLyGkgNy2ZqcN7MHV4D0oqa3hyxXb+tqqc59e9R05qOz5/VneuKu5O15SkSIcqx8k5R2lVDVMGd4t0KCIijQrlzpjhQIlzrsw5dxiYA0wJKvN14D7n3F4A51xleMMUkdNd78wO/PDiQpb+3/n8btoQ8jKSuWf+ZkbevoCvPbqClzbspq7+aKTDbFVmNtHMNplZiZnd1Mj26WZWZWar/eVr/vrBZrbEzNab2Roz+3zAPo+Y2ZaAfVqkkeP9msN8+FGdWpJFJGqF0t0iB9gR8LwcODuozBkAZrYYr0vGLc65f/vbksxsJVAH3O6ceyb4AGY2A5gB0KNHj+N6AyJyekmMi+XSQd24dFA3tlXX8uSKHTy1spyXNq4kq1MiVxV7rcvdO5/a0xyHcpXP96Rz7rqgdQeALzvn3jWzbsAqM3vBObfP3/4959zfWjL+hpv2lCSLSLQK1417cUAfYCyQCywysyK/wu3pnKswswLgZTNb65wrDdzZOTcLmAVQXFysablEJCQ909vz/Yn9uPGCM1iwsZI5K7bz+1dK+P0rJYzuncHUs3pwXr8uJCeckvco/+cqH4CZNVzlC06SP8U5tzng8U4zqwS6APua3iu8GpLkgi4a2UJEolMofzkqgO4Bz3P9dYHKgWXOuSPAFjPbjJc0r3DOVQA458rMbCEwBChFRCRM4mNjmDigKxMHdKVi30GeWrGDp1bu4NtPvEliXAyje2dwQWEW4/pnktnxlOm/HMpVPoDLzexcYDNwo3MucB/MbDiQwCfr5Z+b2c3AAuAm59yh4Bc92SuAZVW1JMXH0C2l3XHvKyLSGkJJklcAfcwsHy85ngpcHVTmGWAa8LCZZeB1vygzszTggHPukL9+FHBn2KIXEQmSk9qOGy84g+vP78PSsmrmb9jN/A27WfBOJWYwuHsq4/tnMaEwi96ZHU718Zf/CfzFr4O/ATwKjGvYaGbZwGPANc65hg7dM4H38BLnWcAPgFuDX/hkrwCWVtVQkNFBI5SISNRqNkl2ztWZ2XXAC3j9jWc759ab2a3ASufcXH/bBDPbANTj9WerNrORwANmdhTvJsHbG+kvJyISdrExxqjeGYzqncFPLi3knfc+ZP6G3by0cTd3vbCJu17YRM/0ZC7on8X4wiyKe6a1tVn+mr3K55yrDnj6IAGNFGbWCXgO+KFzbmnAPrv8h4fM7GHgf8McN+AlyYNyU1vipUVEwiKkjnrOuXnAvKB1Nwc8dsB3/SWwzBtA0cmHKSJy4syM/tmd6J/dievP78Ou/QdZsLGS+Rt286cl23jw9S2kJsczrm8mFxRmMeaMLnRIjPp+zM1e5TOz7ICkdzKw0V+fADwN/Cn4Br2GfcxrYr8MWBfuwD86Uk/53oN8bkhuuF9aRCRsov4U1NoQAAAHZElEQVSvgIhIuGWntOOLI3ryxRE9qTlUx2ubq5i/YTcvb6rkH29VkBAbw2eH5HDHFQMjHWqTQrzKd72ZTcYbXWgPMN3f/SrgXCDdzBrWTXfOrQYeN7MugAGrgW+GO/at1bU4B70yNbKFiEQvJckiclrrkBjHpKJsJhVlU1d/lJXb9vLSht2kJkf/VMkhXOWbidfHOHi/PwN/buI1xzW2PpwMY9KArhRmd2zpQ4mInDAlySIivrjYGEYUpDOiID3SoZzS+nbtyP1fHBbpMEREjqlN3aUiIiIiItIalCSLiIiIiARRkiwiIiIiEkRJsoiIiIhIECXJIiIiIiJBlCSLiIiIiARRkiwiIiIiEkRJsoiIiIhIEHPORTqGTzCzKmDbCeyaAbwf5nBOVrTFFG3xQPTFpHiaF20xRVs8PZ1zXSIdRGs6heptxdO8aIsp2uKB6ItJ8Rxbk3V21CXJJ8rMVjrniiMdR6Boiyna4oHoi0nxNC/aYoq2eCR00fbZKZ7mRVtM0RYPRF9MiufEqbuFiIiIiEgQJckiIiIiIkFOpSR5VqQDaES0xRRt8UD0xaR4mhdtMUVbPBK6aPvsFE/zoi2maIsHoi8mxXOCTpk+ySIiIiIi4XIqtSSLiIiIiISFkmQRERERkSBtLkk2s4lmtsnMSszspka2J5rZk/72ZWaW14KxdDezV8xsg5mtN7PvNFJmrJntN7PV/nJzS8UTcMytZrbWP97KRrabmf3WP0drzGxoC8bSN+C9rzazD8zshqAyLX6OzGy2mVWa2bqAdZ3NbL6Zvev/TGti32v8Mu+a2TUtGM9dZvaO/5k8bWapTex7zM83zDHdYmYVAZ/NRU3se8zfyzDG82RALFvNbHUT+7bIOZLjF011tn+8qKu3o6nO9o8X8XpbdfYJx6Q6O5ycc21mAWKBUqAASADeBgqDynwL+IP/eCrwZAvGkw0M9R93BDY3Es9Y4F+tfJ62AhnH2H4R8DxgwAhgWSt+fu/hDdzdqucIOBcYCqwLWHcncJP/+Cbgjkb26wyU+T/T/MdpLRTPBCDOf3xHY/GE8vmGOaZbgP8N4XM95u9luOIJ2n43cHNrniMtx/0ZRlWd7R8j6urtaK2zAz7DVq+3VWefcEyqs8O4tLWW5OFAiXOuzDl3GJgDTAkqMwV41H/8N+B8M7OWCMY5t8s596b/+ENgI5DTEscKsynAn5xnKZBqZtmtcNzzgVLn3InMzHVSnHOLgD1BqwO/K48ClzWy64XAfOfcHufcXmA+MLEl4nHOveicq/OfLgVyT/Y4JxtTiEL5vQxrPP7v9FXAX072ONKioqrOhjZbb0eqzoYI1duqs08sphCpzg5RW0uSc4AdAc/L+XTl9p8y/pd3P5De0oH5lwiHAMsa2XyOmb1tZs+b2ZktHQvggBfNbJWZzWhkeyjnsSVMpelfkNY+RwBZzrld/uP3gKxGykTqXF2L13LUmOY+33C7zr+cOLuJy5uROEdjgN3OuXeb2N7a50gaF7V1NkRVvR2tdTZEV72tOjs0qrPDpK0lyVHJzDoAfwducM59ELT5TbzLVIOA3wHPtEJIo51zQ4FJwLfN7NxWOOYxmVkCMBn4ayObI3GOPsF513uiYjxEM/shUAc83kSR1vx87wd6AYOBXXiXy6LBNI7dIhF1vwMSXaKs3o7K72s019uqs5ukOjuM2lqSXAF0D3ie669rtIyZxQEpQHVLBWRm8XgV7ePOuX8Eb3fOfeCcq/EfzwPizSyjpeLxj1Ph/6wEnsa7tBIolPMYbpOAN51zu4M3ROIc+XY3XLL0f1Y2UqZVz5WZTQcuAb7g/xH4lBA+37Bxzu12ztU7544Cf2ziWK19juKAzwFPNlWmNc+RHFPU1dn+caKq3o7SOhuir95Wnd0M1dnh1daS5BVAHzPL9//DnQrMDSozF2i4m/UK4OWmvrgny+9j8xCw0Tl3TxNlujb0rzOz4XjnvCWT9vZm1rHhMd6NBeuCis0FvmyeEcD+gEtYLaXJ/yJb+xwFCPyuXAM820iZF4AJZpbmX7aa4K8LOzObCHwfmOycO9BEmVA+33DGFNjv8bNNHCuU38twGg+845wrb2xja58jOaaoqrMh+urtKK6zIfrqbdXZzcekOjucQr3DL1oWvLt8N+PdmflDf92teF9SgCS8S0MlwHKgoAVjGY13uWcNsNpfLgK+CXzTL3MdsB7v7tGlwMgWPj8F/rHe9o/bcI4CYzLgPv8crgWKWzim9niVZ0rAulY9R3gV/S7gCF7/q6/i9XtcALwLvAR09ssWAw8G7Hut/30qAb7SgvGU4PUTa/guNdzx3w2Yd6zPtwVjesz/jqzBq0Szg2Pyn3/q97Il4vHXP9Lw3Qko2yrnSMsJfY5RU2f7x4uqerup7ysRrLP9Y0a03m6iPlKd3XxMqrPDuGhaahERERGRIG2tu4WIiIiISItTkiwiIiIiEkRJsoiIiIhIECXJIiIiIiJBlCSLiIiIiARRkiwiIiIiEkRJsoiIiIhIkP8Pb29zqQJejwUAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}