{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ACKNOWLEDGEMENT**"
      ],
      "metadata": {
        "id": "qVNZSg3UV5sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is inspired by https://github.com/songyouwei/ABSA-PyTorch."
      ],
      "metadata": {
        "id": "x6ytAZbKWC1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SETUP**"
      ],
      "metadata": {
        "id": "5zXJrSMo495u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "8_7Yx7y-5BOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tYF42jlo5Q4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfca0923-f3f6-4c09-d112-7b77d3e9b208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "eTERBhPf5Ykk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import strftime, localtime\n",
        "from sklearn import metrics\n",
        "from xml.etree.ElementTree import parse\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "ZEPQFq-e6gVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "6674vF4n6le1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASSES**"
      ],
      "metadata": {
        "id": "IDenOuxLpJFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**"
      ],
      "metadata": {
        "id": "USX4iChw67d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, max_len, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(max_len) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-max_len:]\n",
        "    else:\n",
        "        trunc = sequence[:max_len]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "L5p9tWvxkY2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self, max_len, lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_len = max_len\n",
        "        self.word_to_index = {}\n",
        "        self.index_to_word = {}\n",
        "        self.idx = 1\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word_to_index:\n",
        "                self.word_to_index[word] = self.idx\n",
        "                self.index_to_word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unk = len(self.word_to_index)+1\n",
        "        sequence = [self.word_to_index[w] if w in self.word_to_index else unk for w in words]\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_len, padding=padding, truncating=truncating)"
      ],
      "metadata": {
        "id": "4y8GnOALjcjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, file_name, tokenizer):\n",
        "        with open(file_name, 'r', newline='\\n', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "        with open(file_name + '.graph', 'rb') as f:\n",
        "            index_to_graph = pickle.load(f)\n",
        "        \n",
        "        self.data = []\n",
        "\n",
        "        # Create tokens with tokenizer\n",
        "        for i in range(0, len(lines), 3):\n",
        "            left, right = [s.lower().strip() for s in lines[i].split(\"$T$\")]\n",
        "            aspect = lines[i+1].lower().strip()\n",
        "            polarity = lines[i+2].strip()\n",
        "\n",
        "            text_token = tokenizer.text_to_sequence(\" \".join([left, aspect, right]))\n",
        "            context_token = tokenizer.text_to_sequence(\" \".join([left, right]))\n",
        "\n",
        "            left_token = tokenizer.text_to_sequence(left)\n",
        "            left_aspect_token = tokenizer.text_to_sequence(\" \".join([left, aspect]))\n",
        "\n",
        "            right_token = tokenizer.text_to_sequence(right, reverse=True)\n",
        "            right_aspect_token = tokenizer.text_to_sequence(\" \".join([aspect, right]), reverse=True)\n",
        "\n",
        "            aspect_token = tokenizer.text_to_sequence(aspect)\n",
        "\n",
        "            left_len = np.sum(left_token != 0)\n",
        "            aspect_len = np.sum(aspect_token != 0)\n",
        "            aspect_boundary = np.asarray([left_len, left_len + aspect_len - 1], dtype = np.int64)\n",
        "            polarity = int(polarity) + 1\n",
        "            \n",
        "            text_len = np.sum(text_token != 0)\n",
        "            concat_segments_tokens = pad_and_truncate([0] * (text_len + 1) + [1] * (aspect_len + 1), tokenizer.max_len)\n",
        "            \n",
        "            dependency_graph = np.pad(index_to_graph[i],\n",
        "                                      ((0, tokenizer.max_len-index_to_graph[i].shape[0]),\n",
        "                                       (0, tokenizer.max_len-index_to_graph[i].shape[0])),\n",
        "                                      'constant')\n",
        "            \n",
        "            self.data.append({\n",
        "                'text_token' : text_token,\n",
        "                'context_token' : context_token, \n",
        "                'left_token' : left_token, \n",
        "                'left_aspect_token' : left_aspect_token,\n",
        "                'right_token' : right_token,\n",
        "                'right_aspect_token' : right_aspect_token,\n",
        "                'aspect_token' : aspect_token, \n",
        "                'aspect_boundary' : aspect_boundary, \n",
        "                'polarity' : polarity, \n",
        "                'dependency_graph' : dependency_graph\n",
        "            })\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "Q5FagHQ9cZGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split()\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "gjHh_czJ0Rbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "fuiAR0ng6vID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.squeeze_embedding = SqueezeEmbedding()\n",
        "        self.lstm = nn.LSTM(config2[\"embed_dim\"]*2, config2[\"hidden_dim\"], num_layers=1, batch_first=True)\n",
        "        self.dense = nn.Linear(config2[\"hidden_dim\"], config2[\"polarities_dim\"])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        text_indices, aspect_indices = inputs[0], inputs[1]\n",
        "        x_len = torch.sum(text_indices != 0, dim=-1)\n",
        "        x_len_max = torch.max(x_len)\n",
        "        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()\n",
        "\n",
        "        x = self.embed(text_indices)\n",
        "        x = self.squeeze_embedding(x, x_len)\n",
        "        aspect = self.embed(aspect_indices)\n",
        "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))\n",
        "        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)\n",
        "        x = torch.cat((aspect, x), dim=-1)\n",
        "\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # global ht\n",
        "        out_pack, (ht, ct) = self.lstm(x_emb_p, None)\n",
        "\n",
        "        return self.dense(ht[0])"
      ],
      "metadata": {
        "id": "xPQIKGdi0qXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze sequence embedding length to the longest one in the batch\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_first=True):\n",
        "        super(SqueezeEmbedding, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        sequence -> sort -> pad and pack -> unpack ->unsort\n",
        "        :param x: sequence embedding vectors\n",
        "        :param x_len: numpy/tensor list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \"\"\"sort\"\"\"\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "        \"\"\"pack\"\"\"\n",
        "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)\n",
        "        \"\"\"unpack: out\"\"\"\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)  # (sequence, lengths)\n",
        "        out = out[0]  #\n",
        "        \"\"\"unsort\"\"\"\n",
        "        out = out[x_unsort_idx]\n",
        "        return out"
      ],
      "metadata": {
        "id": "tFr-dJ6B1E55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main**"
      ],
      "metadata": {
        "id": "6BmgF1FSp32r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Instructor:\n",
        "    def __init__(self):\n",
        "        tokenizer = build_tokenizer(\n",
        "            fnames=[config2[\"dataset_file\"]['train'], config2[\"dataset_file\"]['test']],\n",
        "            max_seq_len=config2[\"max_seq_len\"],\n",
        "            dat_fname='{0}_tokenizer.dat'.format(config2[\"dataset\"]))\n",
        "        embedding_matrix = build_embedding_matrix(\n",
        "            word2idx=tokenizer.word_to_index,\n",
        "            embed_dim=config2[\"embed_dim\"],\n",
        "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(config2[\"embed_dim\"]), config2[\"dataset\"]))\n",
        "        self.model = config2[\"model_class\"](embedding_matrix).to(config2[\"device\"])\n",
        "\n",
        "        self.trainset = ABSADataset(config2[\"dataset_file\"]['train'], tokenizer)\n",
        "        self.testset = ABSADataset(config2[\"dataset_file\"]['test'], tokenizer)\n",
        "        self.valset = ABSADataset(config2[\"dataset_file\"]['val'], tokenizer)\n",
        "\n",
        "        if config2[\"device\"].type == 'cuda':\n",
        "            print('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=config2[\"device\"].index)))\n",
        "        self._print_args()\n",
        "\n",
        "    def _print_args(self):\n",
        "        n_trainable_params, n_nontrainable_params = 0, 0\n",
        "        for p in self.model.parameters():\n",
        "            n_params = torch.prod(torch.tensor(p.shape))\n",
        "            if p.requires_grad:\n",
        "                n_trainable_params += n_params\n",
        "            else:\n",
        "                n_nontrainable_params += n_params\n",
        "        print('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
        "\n",
        "\n",
        "    def _reset_params(self):\n",
        "        for child in self.model.children():\n",
        "            for p in child.parameters():\n",
        "                if p.requires_grad:\n",
        "                    if len(p.shape) > 1:\n",
        "                        config2[\"initializer\"](p)\n",
        "                    else:\n",
        "                        stdv = 1. / math.sqrt(p.shape[0])\n",
        "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "\n",
        "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
        "        max_val_acc = 0\n",
        "        max_val_f1 = 0\n",
        "        max_val_epoch = 0\n",
        "        global_step = 0\n",
        "        path = None\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        for i_epoch in range(config2[\"num_epoch\"]):\n",
        "            print('>' * 100)\n",
        "            print('epoch: {}'.format(i_epoch))\n",
        "            n_correct, n_total, loss_total = 0, 0, 0\n",
        "            # switch model to training mode\n",
        "            self.model.train()\n",
        "            for i_batch, batch in enumerate(train_data_loader):\n",
        "                global_step += 1\n",
        "                # clear gradient accumulators\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                inputs = [batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                global outputs, targets\n",
        "                outputs = self.model(inputs)\n",
        "                targets = batch['polarity'].to(config2[\"device\"])\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
        "                n_total += len(outputs)\n",
        "                loss_total += loss.item() * len(outputs)\n",
        "                if global_step % config2[\"log_step\"] == 0:\n",
        "                    train_acc = n_correct / n_total\n",
        "                    train_loss = loss_total / n_total\n",
        "                    print('[epoch {}] loss: {:.4f}, acc: {:.4f}'.format(i_epoch, train_loss, train_acc))\n",
        "\n",
        "            val_acc, val_loss, _, val_f1, _, _, _, _, _, _, _, _ = self._evaluate(criterion, val_data_loader)\n",
        "            print('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                max_val_epoch = i_epoch\n",
        "                if not os.path.exists('state_dict'):\n",
        "                    os.mkdir('state_dict')\n",
        "                path = '{0}/{1}_{2}_val_acc_{3}'.format(config[\"model_path\"], config2[\"model_name\"], config2[\"dataset\"], round(val_acc, 4))\n",
        "                torch.save(self.model.state_dict(), path)\n",
        "                print('>> saved: {}'.format(path))\n",
        "            if val_f1 > max_val_f1:\n",
        "                max_val_f1 = val_f1\n",
        "            if i_epoch - max_val_epoch >= config2[\"patience\"]:\n",
        "                print('>> early stop.')\n",
        "                break\n",
        "            train_losses.append(loss_total / n_total)\n",
        "            train_accuracies.append(n_correct / n_total)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        return path, train_losses, train_accuracies, val_losses, val_accuracies\n",
        "\n",
        "    def _evaluate(self, criterion, data_loader):\n",
        "        n_correct, n_total, loss_total = 0, 0, 0\n",
        "        t_targets_all, t_outputs_all = None, None\n",
        "        # switch model to evaluation mode\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i_batch, t_batch in enumerate(data_loader):\n",
        "                t_inputs = [t_batch[col].to(config2[\"device\"]) for col in config2[\"inputs_cols\"]]\n",
        "                t_targets = t_batch['polarity'].to(config2[\"device\"])\n",
        "                t_outputs = self.model(t_inputs)\n",
        "\n",
        "                loss = criterion(t_outputs, t_targets)\n",
        "\n",
        "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
        "                n_total += len(t_outputs)\n",
        "                loss_total += loss.item() * len(t_outputs)\n",
        "\n",
        "                if t_targets_all is None:\n",
        "                    t_targets_all = t_targets\n",
        "                    t_outputs_all = t_outputs\n",
        "                else:\n",
        "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
        "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
        "\n",
        "        acc = n_correct / n_total\n",
        "        loss = loss_total / n_total\n",
        "        f1_macro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
        "        f1_micro = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='micro')\n",
        "        f1_weighted = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='weighted')\n",
        "        precision_macro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        precision_micro = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        precision_weighted = metrics.precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        recall_macro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'macro')\n",
        "        recall_micro = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'micro')\n",
        "        recall_weighted = metrics.recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average = 'weighted')\n",
        "        confusion_matrix = metrics.confusion_matrix(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2])\n",
        "        return acc, loss, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix\n",
        "\n",
        "    def run(self):\n",
        "        # Loss and Optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        optimizer = config2[\"optimizer\"](_params, lr=config2[\"lr\"], weight_decay=config2[\"l2reg\"])\n",
        "\n",
        "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=config2[\"batch_size\"], shuffle=True)\n",
        "        test_data_loader = DataLoader(dataset=self.testset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "        val_data_loader = DataLoader(dataset=self.valset, batch_size=config2[\"batch_size\"], shuffle=False)\n",
        "\n",
        "        self._reset_params()\n",
        "        best_model_path, train_losses, train_accuracies, val_losses, val_accuracies = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "        acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = self._evaluate(nn.CrossEntropyLoss(), test_data_loader)\n",
        "        print('>> test_acc: {:.4f}'.format(acc))\n",
        "        print('>> test_f1_macro: {:.4f}'.format(f1_macro))\n",
        "        print('>> test_f1_micro: {:.4f}'.format(f1_micro))\n",
        "        print('>> test_f1_weighted: {:.4f}'.format(f1_weighted))\n",
        "        print('>> test_precision_macro: {:.4f}'.format(precision_macro))\n",
        "        print('>> test_precision_micro: {:.4f}'.format(precision_micro))\n",
        "        print('>> test_precision_weighted: {:.4f}'.format(precision_weighted))\n",
        "        print('>> test_recall_macro: {:.4f}'.format(recall_macro))\n",
        "        print('>> test_recall_micro: {:.4f}'.format(recall_micro))\n",
        "        print('>> test_recall_weighted: {:.4f}'.format(recall_weighted))\n",
        "        print('confusion matrix:')\n",
        "        print(confusion_matrix)\n",
        "\n",
        "        epochs = len(train_losses)\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        for i, metrics in enumerate(zip([train_losses, train_accuracies], [val_losses, val_accuracies], ['Loss', 'Accuracy'])):\n",
        "            plt.subplot(1, 2, i + 1)\n",
        "            plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "            plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "            plt.legend()\n",
        "        plt.show()\n",
        "        plt.savefig('lstm_accuracy.png')\n",
        "        return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "mPRxQS-Jp5vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "bv09Mo677CkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_format_sentences(source_path, target_path):\n",
        "    f = open(target_path, \"w\")\n",
        "\n",
        "    sentences = parse(source_path).getroot()\n",
        "    preprocessed = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            if polarity == \"positive\":\n",
        "                polarity = '1'\n",
        "            elif polarity == \"neutral\":\n",
        "                polarity = '0'\n",
        "            elif polarity == \"negative\":\n",
        "                polarity = '-1'\n",
        "            else:\n",
        "                raise Exception(\"invalid polarity!\")\n",
        "            start = int(aspectTerm.get('from'))\n",
        "            end = int(aspectTerm.get('to'))\n",
        "            preprocessed.append(text[:start] + \"$T$\" + text[end:])\n",
        "            preprocessed.append(text[start:end])\n",
        "            preprocessed.append(polarity)\n",
        "    f.write(\"\\n\".join(preprocessed))"
      ],
      "metadata": {
        "id": "Mlhjev_wrkDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading tokenizer:', dat_fname)\n",
        "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        text = ''\n",
        "        for fname in fnames:\n",
        "            fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "            lines = fin.readlines()\n",
        "            fin.close()\n",
        "            for i in range(0, len(lines), 3):\n",
        "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "                aspect = lines[i + 1].lower().strip()\n",
        "                text_raw = text_left + \" \" + aspect + \" \" + text_right\n",
        "                text += text_raw + \" \"\n",
        "\n",
        "        tokenizer = Tokenizer(max_seq_len)\n",
        "        tokenizer.fit_on_text(text)\n",
        "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "pV174mP37UZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_word_vec(path, word2idx=None, embed_dim=300):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n",
        "        if word in word2idx.keys():\n",
        "            word_vec[word] = np.asarray(vec, dtype='float32')\n",
        "    return word_vec"
      ],
      "metadata": {
        "id": "2h-2UjGs7fu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix:', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = config[\"glove_path\"]\n",
        "        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "nB3jz5Fk7dZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "7JH84-Kx7Y-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dependency_adj_matrix(text):\n",
        "    # https://spacy.io/docs/usage/processing-text\n",
        "    tokens = nlp(text)\n",
        "    words = text.split()\n",
        "    matrix = np.zeros((len(words), len(words))).astype('float32')\n",
        "    assert len(words) == len(list(tokens))\n",
        "\n",
        "    for token in tokens:\n",
        "        matrix[token.i][token.i] = 1\n",
        "        for child in token.children:\n",
        "            matrix[token.i][child.i] = 1\n",
        "            matrix[child.i][token.i] = 1\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "VemwD4FB7sY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "    change_format_sentences(config[\"raw_train_path\"], config[\"processed_train_path\"])\n",
        "    change_format_sentences(config[\"raw_val_path\"], config[\"processed_val_path\"])\n",
        "    change_format_sentences(config[\"raw_test_path\"], config[\"processed_test_path\"])\n",
        "    process(config[\"processed_train_path\"])\n",
        "    process(config[\"processed_val_path\"])\n",
        "    process(config[\"processed_test_path\"])"
      ],
      "metadata": {
        "id": "vC45U4k21oPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(filename):\n",
        "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    lines = fin.readlines()\n",
        "    fin.close()\n",
        "    idx2graph = {}\n",
        "    fout = open(filename+'.graph', 'wb')\n",
        "    for i in range(0, len(lines), 3):\n",
        "        text_left, _, text_right = [s.strip() for s in lines[i].partition(\"$T$\")]\n",
        "        aspect = lines[i + 1].strip()\n",
        "        adj_matrix = dependency_adj_matrix(text_left+' '+aspect+' '+text_right)\n",
        "        idx2graph[i] = adj_matrix\n",
        "    pickle.dump(idx2graph, fout)        \n",
        "    fout.close() "
      ],
      "metadata": {
        "id": "HlLsFiUJ7mPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    ins = Instructor()\n",
        "    best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = ins.run()\n",
        "    return best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix"
      ],
      "metadata": {
        "id": "ANtAieVU8lwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **MAIN**"
      ],
      "metadata": {
        "id": "cxb8D05u8UN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "5QyUuFxj8XuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "AF3DYep7AW8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"base_path\": \"/content/drive/MyDrive/CS4248/MAMS-ATSA\",\n",
        "    \"glove_path\": \"/content/drive/MyDrive/CS4248/glove.42B.300d.txt\"\n",
        "}\n",
        "\n",
        "config[\"model_path\"] = os.path.join(config[\"base_path\"], 'lstm_models')\n",
        "config[\"raw_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train.xml')\n",
        "config[\"raw_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val.xml')\n",
        "config[\"raw_test_path\"] = os.path.join(config['base_path'], 'raw/test.xml')\n",
        "config[\"processed_train_path\"] = os.path.join(config[\"base_path\"], 'raw/train_processed.xml.seg')\n",
        "config[\"processed_val_path\"] = os.path.join(config[\"base_path\"], 'raw/val_processed.xml.seg')\n",
        "config[\"processed_test_path\"] = os.path.join(config['base_path'], 'raw/test_processed.xml.seg')"
      ],
      "metadata": {
        "id": "9y9MO-RW8bcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_files = {\n",
        "    'train': config[\"processed_train_path\"],\n",
        "    'test': config[\"processed_test_path\"],\n",
        "    'val': config[\"processed_val_path\"]\n",
        "}\n",
        "\n",
        "config2 = {\n",
        "    \"model_name\" : \"LSTM\",\n",
        "    \"lr\" : 3e-5,\n",
        "    \"dropout\" : 0.1,\n",
        "    \"l2reg\" : 0.001,\n",
        "    \"num_epoch\" : 20,\n",
        "    \"batch_size\" : 25,\n",
        "    \"log_step\" : 10,\n",
        "    \"embed_dim\" : 300,\n",
        "    \"hidden_dim\" : 300,\n",
        "    \"model_class\" : LSTM,\n",
        "    \"dataset\": \"MAMS\",\n",
        "    \"dataset_file\" : dataset_files,\n",
        "    \"inputs_cols\" : ['text_token', 'aspect_token'],\n",
        "    \"initializer\" : torch.nn.init.xavier_uniform_,\n",
        "    \"optimizer\" : torch.optim.Adam,\n",
        "    \"max_seq_len\" : 85,\n",
        "    \"polarities_dim\" : 3,\n",
        "    \"patience\" : 5, # for early stopping\n",
        "    \"device\" : None,\n",
        "    \"seed\" : 1234,\n",
        "    \"valset_ratio\" : 0\n",
        "}\n",
        "\n",
        "config2[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
        "        if config2[\"device\"] is None else torch.device(config2[\"device\"])\n",
        "\n",
        "if config2[\"seed\"] is not None:\n",
        "    random.seed(config2[\"seed\"])\n",
        "    np.random.seed(config2[\"seed\"])\n",
        "    torch.manual_seed(config2[\"seed\"])\n",
        "    torch.cuda.manual_seed(config2[\"seed\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(config2[\"seed\"])"
      ],
      "metadata": {
        "id": "clOIqT523tWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**"
      ],
      "metadata": {
        "id": "3fT6RqeiAGmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this line only for the first time you run this file\n",
        "# process_data()"
      ],
      "metadata": {
        "id": "vT4xfKSC-Xpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path, train_losses, train_accuracies, val_losses, val_accuracies, acc, test_losses, f1_macro, f1_micro, f1_weighted, precision_macro, precision_micro, precision_weighted, recall_macro, recall_micro, recall_weighted, confusion_matrix = run()"
      ],
      "metadata": {
        "id": "8gEIkSn38pHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a96df5e-6bb5-462e-ee76-10867684a81a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading tokenizer: MAMS_tokenizer.dat\n",
            "loading embedding_matrix: 300_MAMS_embedding_matrix.dat\n",
            "> n_trainable_params: 1083303, n_nontrainable_params: 3994500\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 0\n",
            "[epoch 0] loss: 1.1110, acc: 0.3920\n",
            "[epoch 0] loss: 1.0863, acc: 0.4160\n",
            "[epoch 0] loss: 1.0679, acc: 0.4360\n",
            "[epoch 0] loss: 1.0548, acc: 0.4380\n",
            "[epoch 0] loss: 1.0487, acc: 0.4400\n",
            "[epoch 0] loss: 1.0386, acc: 0.4533\n",
            "[epoch 0] loss: 1.0303, acc: 0.4651\n",
            "[epoch 0] loss: 1.0256, acc: 0.4740\n",
            "[epoch 0] loss: 1.0199, acc: 0.4844\n",
            "[epoch 0] loss: 1.0141, acc: 0.4896\n",
            "[epoch 0] loss: 1.0087, acc: 0.4956\n",
            "[epoch 0] loss: 1.0027, acc: 0.5047\n",
            "[epoch 0] loss: 1.0001, acc: 0.5071\n",
            "[epoch 0] loss: 0.9940, acc: 0.5131\n",
            "[epoch 0] loss: 0.9888, acc: 0.5171\n",
            "[epoch 0] loss: 0.9858, acc: 0.5202\n",
            "[epoch 0] loss: 0.9850, acc: 0.5219\n",
            "[epoch 0] loss: 0.9822, acc: 0.5231\n",
            "[epoch 0] loss: 0.9778, acc: 0.5274\n",
            "[epoch 0] loss: 0.9735, acc: 0.5316\n",
            "[epoch 0] loss: 0.9708, acc: 0.5326\n",
            "[epoch 0] loss: 0.9679, acc: 0.5345\n",
            "[epoch 0] loss: 0.9680, acc: 0.5343\n",
            "[epoch 0] loss: 0.9649, acc: 0.5357\n",
            "[epoch 0] loss: 0.9624, acc: 0.5371\n",
            "[epoch 0] loss: 0.9593, acc: 0.5395\n",
            "[epoch 0] loss: 0.9568, acc: 0.5413\n",
            "[epoch 0] loss: 0.9532, acc: 0.5434\n",
            "[epoch 0] loss: 0.9499, acc: 0.5461\n",
            "[epoch 0] loss: 0.9491, acc: 0.5464\n",
            "[epoch 0] loss: 0.9464, acc: 0.5488\n",
            "[epoch 0] loss: 0.9455, acc: 0.5481\n",
            "[epoch 0] loss: 0.9456, acc: 0.5479\n",
            "[epoch 0] loss: 0.9430, acc: 0.5492\n",
            "[epoch 0] loss: 0.9420, acc: 0.5505\n",
            "[epoch 0] loss: 0.9407, acc: 0.5513\n",
            "[epoch 0] loss: 0.9395, acc: 0.5514\n",
            "[epoch 0] loss: 0.9376, acc: 0.5534\n",
            "[epoch 0] loss: 0.9349, acc: 0.5552\n",
            "[epoch 0] loss: 0.9343, acc: 0.5555\n",
            "[epoch 0] loss: 0.9318, acc: 0.5562\n",
            "[epoch 0] loss: 0.9311, acc: 0.5573\n",
            "[epoch 0] loss: 0.9296, acc: 0.5588\n",
            "[epoch 0] loss: 0.9290, acc: 0.5586\n",
            "> val_acc: 0.6156, val_f1: 0.6156\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 1\n",
            "[epoch 1] loss: 0.7822, acc: 0.6200\n",
            "[epoch 1] loss: 0.8469, acc: 0.6200\n",
            "[epoch 1] loss: 0.8330, acc: 0.6218\n",
            "[epoch 1] loss: 0.8168, acc: 0.6338\n",
            "[epoch 1] loss: 0.8490, acc: 0.6181\n",
            "[epoch 1] loss: 0.8629, acc: 0.6038\n",
            "[epoch 1] loss: 0.8790, acc: 0.5903\n",
            "[epoch 1] loss: 0.8704, acc: 0.5994\n",
            "[epoch 1] loss: 0.8627, acc: 0.6049\n",
            "[epoch 1] loss: 0.8579, acc: 0.6096\n",
            "[epoch 1] loss: 0.8577, acc: 0.6078\n",
            "[epoch 1] loss: 0.8490, acc: 0.6118\n",
            "[epoch 1] loss: 0.8549, acc: 0.6072\n",
            "[epoch 1] loss: 0.8558, acc: 0.6091\n",
            "[epoch 1] loss: 0.8577, acc: 0.6076\n",
            "[epoch 1] loss: 0.8558, acc: 0.6092\n",
            "[epoch 1] loss: 0.8551, acc: 0.6101\n",
            "[epoch 1] loss: 0.8541, acc: 0.6116\n",
            "[epoch 1] loss: 0.8537, acc: 0.6125\n",
            "[epoch 1] loss: 0.8510, acc: 0.6146\n",
            "[epoch 1] loss: 0.8514, acc: 0.6147\n",
            "[epoch 1] loss: 0.8513, acc: 0.6140\n",
            "[epoch 1] loss: 0.8503, acc: 0.6133\n",
            "[epoch 1] loss: 0.8490, acc: 0.6129\n",
            "[epoch 1] loss: 0.8486, acc: 0.6134\n",
            "[epoch 1] loss: 0.8478, acc: 0.6132\n",
            "[epoch 1] loss: 0.8488, acc: 0.6127\n",
            "[epoch 1] loss: 0.8479, acc: 0.6128\n",
            "[epoch 1] loss: 0.8466, acc: 0.6140\n",
            "[epoch 1] loss: 0.8473, acc: 0.6134\n",
            "[epoch 1] loss: 0.8475, acc: 0.6121\n",
            "[epoch 1] loss: 0.8466, acc: 0.6113\n",
            "[epoch 1] loss: 0.8472, acc: 0.6112\n",
            "[epoch 1] loss: 0.8490, acc: 0.6107\n",
            "[epoch 1] loss: 0.8489, acc: 0.6105\n",
            "[epoch 1] loss: 0.8501, acc: 0.6088\n",
            "[epoch 1] loss: 0.8491, acc: 0.6091\n",
            "[epoch 1] loss: 0.8505, acc: 0.6077\n",
            "[epoch 1] loss: 0.8519, acc: 0.6061\n",
            "[epoch 1] loss: 0.8513, acc: 0.6069\n",
            "[epoch 1] loss: 0.8504, acc: 0.6080\n",
            "[epoch 1] loss: 0.8499, acc: 0.6082\n",
            "[epoch 1] loss: 0.8492, acc: 0.6085\n",
            "[epoch 1] loss: 0.8495, acc: 0.6088\n",
            "[epoch 1] loss: 0.8490, acc: 0.6090\n",
            "> val_acc: 0.6231, val_f1: 0.6231\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 2\n",
            "[epoch 2] loss: 0.8067, acc: 0.6300\n",
            "[epoch 2] loss: 0.8474, acc: 0.6029\n",
            "[epoch 2] loss: 0.8405, acc: 0.5983\n",
            "[epoch 2] loss: 0.8404, acc: 0.6094\n",
            "[epoch 2] loss: 0.8443, acc: 0.6036\n",
            "[epoch 2] loss: 0.8305, acc: 0.6059\n",
            "[epoch 2] loss: 0.8284, acc: 0.6119\n",
            "[epoch 2] loss: 0.8397, acc: 0.6092\n",
            "[epoch 2] loss: 0.8404, acc: 0.6133\n",
            "[epoch 2] loss: 0.8371, acc: 0.6191\n",
            "[epoch 2] loss: 0.8346, acc: 0.6200\n",
            "[epoch 2] loss: 0.8345, acc: 0.6200\n",
            "[epoch 2] loss: 0.8360, acc: 0.6219\n",
            "[epoch 2] loss: 0.8336, acc: 0.6212\n",
            "[epoch 2] loss: 0.8329, acc: 0.6217\n",
            "[epoch 2] loss: 0.8306, acc: 0.6226\n",
            "[epoch 2] loss: 0.8346, acc: 0.6193\n",
            "[epoch 2] loss: 0.8354, acc: 0.6193\n",
            "[epoch 2] loss: 0.8327, acc: 0.6198\n",
            "[epoch 2] loss: 0.8308, acc: 0.6208\n",
            "[epoch 2] loss: 0.8322, acc: 0.6194\n",
            "[epoch 2] loss: 0.8320, acc: 0.6176\n",
            "[epoch 2] loss: 0.8320, acc: 0.6166\n",
            "[epoch 2] loss: 0.8306, acc: 0.6190\n",
            "[epoch 2] loss: 0.8307, acc: 0.6193\n",
            "[epoch 2] loss: 0.8270, acc: 0.6225\n",
            "[epoch 2] loss: 0.8277, acc: 0.6223\n",
            "[epoch 2] loss: 0.8296, acc: 0.6212\n",
            "[epoch 2] loss: 0.8301, acc: 0.6203\n",
            "[epoch 2] loss: 0.8275, acc: 0.6220\n",
            "[epoch 2] loss: 0.8272, acc: 0.6224\n",
            "[epoch 2] loss: 0.8263, acc: 0.6219\n",
            "[epoch 2] loss: 0.8246, acc: 0.6230\n",
            "[epoch 2] loss: 0.8241, acc: 0.6225\n",
            "[epoch 2] loss: 0.8237, acc: 0.6220\n",
            "[epoch 2] loss: 0.8225, acc: 0.6225\n",
            "[epoch 2] loss: 0.8210, acc: 0.6237\n",
            "[epoch 2] loss: 0.8197, acc: 0.6245\n",
            "[epoch 2] loss: 0.8216, acc: 0.6242\n",
            "[epoch 2] loss: 0.8212, acc: 0.6250\n",
            "[epoch 2] loss: 0.8210, acc: 0.6249\n",
            "[epoch 2] loss: 0.8194, acc: 0.6263\n",
            "[epoch 2] loss: 0.8201, acc: 0.6258\n",
            "[epoch 2] loss: 0.8204, acc: 0.6256\n",
            "[epoch 2] loss: 0.8201, acc: 0.6261\n",
            "> val_acc: 0.6284, val_f1: 0.6284\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6284\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 3\n",
            "[epoch 3] loss: 0.7816, acc: 0.6400\n",
            "[epoch 3] loss: 0.8119, acc: 0.6300\n",
            "[epoch 3] loss: 0.7903, acc: 0.6400\n",
            "[epoch 3] loss: 0.8018, acc: 0.6400\n",
            "[epoch 3] loss: 0.8036, acc: 0.6348\n",
            "[epoch 3] loss: 0.7994, acc: 0.6321\n",
            "[epoch 3] loss: 0.8013, acc: 0.6339\n",
            "[epoch 3] loss: 0.8045, acc: 0.6332\n",
            "[epoch 3] loss: 0.8058, acc: 0.6284\n",
            "[epoch 3] loss: 0.8003, acc: 0.6304\n",
            "[epoch 3] loss: 0.8011, acc: 0.6306\n",
            "[epoch 3] loss: 0.7994, acc: 0.6341\n",
            "[epoch 3] loss: 0.7979, acc: 0.6365\n",
            "[epoch 3] loss: 0.8012, acc: 0.6350\n",
            "[epoch 3] loss: 0.7985, acc: 0.6386\n",
            "[epoch 3] loss: 0.8031, acc: 0.6359\n",
            "[epoch 3] loss: 0.7953, acc: 0.6419\n",
            "[epoch 3] loss: 0.7971, acc: 0.6411\n",
            "[epoch 3] loss: 0.7979, acc: 0.6411\n",
            "[epoch 3] loss: 0.7975, acc: 0.6408\n",
            "[epoch 3] loss: 0.7955, acc: 0.6421\n",
            "[epoch 3] loss: 0.7949, acc: 0.6424\n",
            "[epoch 3] loss: 0.7945, acc: 0.6421\n",
            "[epoch 3] loss: 0.7944, acc: 0.6420\n",
            "[epoch 3] loss: 0.7954, acc: 0.6397\n",
            "[epoch 3] loss: 0.7964, acc: 0.6386\n",
            "[epoch 3] loss: 0.7986, acc: 0.6374\n",
            "[epoch 3] loss: 0.7993, acc: 0.6365\n",
            "[epoch 3] loss: 0.7984, acc: 0.6376\n",
            "[epoch 3] loss: 0.7998, acc: 0.6364\n",
            "[epoch 3] loss: 0.8002, acc: 0.6362\n",
            "[epoch 3] loss: 0.7989, acc: 0.6368\n",
            "[epoch 3] loss: 0.8002, acc: 0.6356\n",
            "[epoch 3] loss: 0.7991, acc: 0.6367\n",
            "[epoch 3] loss: 0.7980, acc: 0.6368\n",
            "[epoch 3] loss: 0.7994, acc: 0.6360\n",
            "[epoch 3] loss: 0.7991, acc: 0.6364\n",
            "[epoch 3] loss: 0.7984, acc: 0.6370\n",
            "[epoch 3] loss: 0.7994, acc: 0.6365\n",
            "[epoch 3] loss: 0.8001, acc: 0.6362\n",
            "[epoch 3] loss: 0.7996, acc: 0.6364\n",
            "[epoch 3] loss: 0.7985, acc: 0.6370\n",
            "[epoch 3] loss: 0.7998, acc: 0.6362\n",
            "[epoch 3] loss: 0.7993, acc: 0.6364\n",
            "[epoch 3] loss: 0.7982, acc: 0.6378\n",
            "> val_acc: 0.6359, val_f1: 0.6359\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6359\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 4\n",
            "[epoch 4] loss: 0.6855, acc: 0.6900\n",
            "[epoch 4] loss: 0.7638, acc: 0.6467\n",
            "[epoch 4] loss: 0.7969, acc: 0.6343\n",
            "[epoch 4] loss: 0.7835, acc: 0.6453\n",
            "[epoch 4] loss: 0.7922, acc: 0.6358\n",
            "[epoch 4] loss: 0.7816, acc: 0.6414\n",
            "[epoch 4] loss: 0.7812, acc: 0.6406\n",
            "[epoch 4] loss: 0.7770, acc: 0.6426\n",
            "[epoch 4] loss: 0.7793, acc: 0.6432\n",
            "[epoch 4] loss: 0.7739, acc: 0.6445\n",
            "[epoch 4] loss: 0.7709, acc: 0.6456\n",
            "[epoch 4] loss: 0.7738, acc: 0.6447\n",
            "[epoch 4] loss: 0.7799, acc: 0.6406\n",
            "[epoch 4] loss: 0.7781, acc: 0.6449\n",
            "[epoch 4] loss: 0.7787, acc: 0.6435\n",
            "[epoch 4] loss: 0.7825, acc: 0.6428\n",
            "[epoch 4] loss: 0.7797, acc: 0.6460\n",
            "[epoch 4] loss: 0.7779, acc: 0.6458\n",
            "[epoch 4] loss: 0.7824, acc: 0.6464\n",
            "[epoch 4] loss: 0.7821, acc: 0.6479\n",
            "[epoch 4] loss: 0.7813, acc: 0.6488\n",
            "[epoch 4] loss: 0.7832, acc: 0.6486\n",
            "[epoch 4] loss: 0.7848, acc: 0.6481\n",
            "[epoch 4] loss: 0.7837, acc: 0.6482\n",
            "[epoch 4] loss: 0.7836, acc: 0.6476\n",
            "[epoch 4] loss: 0.7817, acc: 0.6501\n",
            "[epoch 4] loss: 0.7811, acc: 0.6516\n",
            "[epoch 4] loss: 0.7787, acc: 0.6532\n",
            "[epoch 4] loss: 0.7782, acc: 0.6529\n",
            "[epoch 4] loss: 0.7770, acc: 0.6532\n",
            "[epoch 4] loss: 0.7789, acc: 0.6516\n",
            "[epoch 4] loss: 0.7771, acc: 0.6527\n",
            "[epoch 4] loss: 0.7772, acc: 0.6515\n",
            "[epoch 4] loss: 0.7756, acc: 0.6516\n",
            "[epoch 4] loss: 0.7769, acc: 0.6500\n",
            "[epoch 4] loss: 0.7784, acc: 0.6492\n",
            "[epoch 4] loss: 0.7774, acc: 0.6499\n",
            "[epoch 4] loss: 0.7781, acc: 0.6496\n",
            "[epoch 4] loss: 0.7800, acc: 0.6484\n",
            "[epoch 4] loss: 0.7790, acc: 0.6481\n",
            "[epoch 4] loss: 0.7799, acc: 0.6474\n",
            "[epoch 4] loss: 0.7804, acc: 0.6478\n",
            "[epoch 4] loss: 0.7796, acc: 0.6487\n",
            "[epoch 4] loss: 0.7789, acc: 0.6480\n",
            "[epoch 4] loss: 0.7801, acc: 0.6479\n",
            "> val_acc: 0.6359, val_f1: 0.6359\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 5\n",
            "[epoch 5] loss: 0.6797, acc: 0.6800\n",
            "[epoch 5] loss: 0.7436, acc: 0.6560\n",
            "[epoch 5] loss: 0.7555, acc: 0.6560\n",
            "[epoch 5] loss: 0.7569, acc: 0.6560\n",
            "[epoch 5] loss: 0.7615, acc: 0.6512\n",
            "[epoch 5] loss: 0.7554, acc: 0.6600\n",
            "[epoch 5] loss: 0.7528, acc: 0.6606\n",
            "[epoch 5] loss: 0.7551, acc: 0.6610\n",
            "[epoch 5] loss: 0.7634, acc: 0.6551\n",
            "[epoch 5] loss: 0.7624, acc: 0.6580\n",
            "[epoch 5] loss: 0.7630, acc: 0.6575\n",
            "[epoch 5] loss: 0.7658, acc: 0.6570\n",
            "[epoch 5] loss: 0.7654, acc: 0.6591\n",
            "[epoch 5] loss: 0.7712, acc: 0.6563\n",
            "[epoch 5] loss: 0.7708, acc: 0.6597\n",
            "[epoch 5] loss: 0.7672, acc: 0.6613\n",
            "[epoch 5] loss: 0.7641, acc: 0.6621\n",
            "[epoch 5] loss: 0.7656, acc: 0.6604\n",
            "[epoch 5] loss: 0.7638, acc: 0.6598\n",
            "[epoch 5] loss: 0.7676, acc: 0.6588\n",
            "[epoch 5] loss: 0.7659, acc: 0.6592\n",
            "[epoch 5] loss: 0.7647, acc: 0.6595\n",
            "[epoch 5] loss: 0.7635, acc: 0.6595\n",
            "[epoch 5] loss: 0.7666, acc: 0.6575\n",
            "[epoch 5] loss: 0.7661, acc: 0.6573\n",
            "[epoch 5] loss: 0.7696, acc: 0.6534\n",
            "[epoch 5] loss: 0.7707, acc: 0.6532\n",
            "[epoch 5] loss: 0.7690, acc: 0.6553\n",
            "[epoch 5] loss: 0.7678, acc: 0.6568\n",
            "[epoch 5] loss: 0.7668, acc: 0.6576\n",
            "[epoch 5] loss: 0.7662, acc: 0.6588\n",
            "[epoch 5] loss: 0.7644, acc: 0.6599\n",
            "[epoch 5] loss: 0.7647, acc: 0.6593\n",
            "[epoch 5] loss: 0.7630, acc: 0.6601\n",
            "[epoch 5] loss: 0.7634, acc: 0.6601\n",
            "[epoch 5] loss: 0.7632, acc: 0.6596\n",
            "[epoch 5] loss: 0.7652, acc: 0.6589\n",
            "[epoch 5] loss: 0.7623, acc: 0.6598\n",
            "[epoch 5] loss: 0.7619, acc: 0.6604\n",
            "[epoch 5] loss: 0.7626, acc: 0.6597\n",
            "[epoch 5] loss: 0.7623, acc: 0.6600\n",
            "[epoch 5] loss: 0.7624, acc: 0.6594\n",
            "[epoch 5] loss: 0.7619, acc: 0.6597\n",
            "[epoch 5] loss: 0.7613, acc: 0.6597\n",
            "> val_acc: 0.6381, val_f1: 0.6381\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6381\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 6\n",
            "[epoch 6] loss: 0.5936, acc: 0.8200\n",
            "[epoch 6] loss: 0.7564, acc: 0.6833\n",
            "[epoch 6] loss: 0.7585, acc: 0.6818\n",
            "[epoch 6] loss: 0.7539, acc: 0.6675\n",
            "[epoch 6] loss: 0.7442, acc: 0.6724\n",
            "[epoch 6] loss: 0.7501, acc: 0.6715\n",
            "[epoch 6] loss: 0.7592, acc: 0.6626\n",
            "[epoch 6] loss: 0.7487, acc: 0.6656\n",
            "[epoch 6] loss: 0.7502, acc: 0.6659\n",
            "[epoch 6] loss: 0.7509, acc: 0.6687\n",
            "[epoch 6] loss: 0.7499, acc: 0.6694\n",
            "[epoch 6] loss: 0.7584, acc: 0.6650\n",
            "[epoch 6] loss: 0.7557, acc: 0.6682\n",
            "[epoch 6] loss: 0.7536, acc: 0.6691\n",
            "[epoch 6] loss: 0.7548, acc: 0.6670\n",
            "[epoch 6] loss: 0.7547, acc: 0.6668\n",
            "[epoch 6] loss: 0.7586, acc: 0.6654\n",
            "[epoch 6] loss: 0.7596, acc: 0.6623\n",
            "[epoch 6] loss: 0.7563, acc: 0.6653\n",
            "[epoch 6] loss: 0.7501, acc: 0.6671\n",
            "[epoch 6] loss: 0.7482, acc: 0.6667\n",
            "[epoch 6] loss: 0.7469, acc: 0.6675\n",
            "[epoch 6] loss: 0.7464, acc: 0.6690\n",
            "[epoch 6] loss: 0.7492, acc: 0.6667\n",
            "[epoch 6] loss: 0.7504, acc: 0.6655\n",
            "[epoch 6] loss: 0.7470, acc: 0.6665\n",
            "[epoch 6] loss: 0.7463, acc: 0.6673\n",
            "[epoch 6] loss: 0.7453, acc: 0.6684\n",
            "[epoch 6] loss: 0.7435, acc: 0.6702\n",
            "[epoch 6] loss: 0.7426, acc: 0.6703\n",
            "[epoch 6] loss: 0.7402, acc: 0.6715\n",
            "[epoch 6] loss: 0.7374, acc: 0.6733\n",
            "[epoch 6] loss: 0.7411, acc: 0.6718\n",
            "[epoch 6] loss: 0.7409, acc: 0.6713\n",
            "[epoch 6] loss: 0.7384, acc: 0.6724\n",
            "[epoch 6] loss: 0.7406, acc: 0.6711\n",
            "[epoch 6] loss: 0.7406, acc: 0.6710\n",
            "[epoch 6] loss: 0.7419, acc: 0.6700\n",
            "[epoch 6] loss: 0.7429, acc: 0.6692\n",
            "[epoch 6] loss: 0.7445, acc: 0.6683\n",
            "[epoch 6] loss: 0.7452, acc: 0.6674\n",
            "[epoch 6] loss: 0.7445, acc: 0.6675\n",
            "[epoch 6] loss: 0.7455, acc: 0.6665\n",
            "[epoch 6] loss: 0.7462, acc: 0.6666\n",
            "[epoch 6] loss: 0.7469, acc: 0.6663\n",
            "> val_acc: 0.6269, val_f1: 0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 7\n",
            "[epoch 7] loss: 0.6694, acc: 0.6500\n",
            "[epoch 7] loss: 0.6997, acc: 0.6829\n",
            "[epoch 7] loss: 0.7263, acc: 0.6833\n",
            "[epoch 7] loss: 0.7300, acc: 0.6776\n",
            "[epoch 7] loss: 0.7153, acc: 0.6836\n",
            "[epoch 7] loss: 0.7133, acc: 0.6815\n",
            "[epoch 7] loss: 0.7187, acc: 0.6719\n",
            "[epoch 7] loss: 0.7253, acc: 0.6697\n",
            "[epoch 7] loss: 0.7220, acc: 0.6738\n",
            "[epoch 7] loss: 0.7234, acc: 0.6736\n",
            "[epoch 7] loss: 0.7249, acc: 0.6769\n",
            "[epoch 7] loss: 0.7264, acc: 0.6751\n",
            "[epoch 7] loss: 0.7265, acc: 0.6735\n",
            "[epoch 7] loss: 0.7289, acc: 0.6731\n",
            "[epoch 7] loss: 0.7300, acc: 0.6708\n",
            "[epoch 7] loss: 0.7281, acc: 0.6719\n",
            "[epoch 7] loss: 0.7294, acc: 0.6712\n",
            "[epoch 7] loss: 0.7290, acc: 0.6729\n",
            "[epoch 7] loss: 0.7284, acc: 0.6737\n",
            "[epoch 7] loss: 0.7313, acc: 0.6730\n",
            "[epoch 7] loss: 0.7292, acc: 0.6733\n",
            "[epoch 7] loss: 0.7306, acc: 0.6738\n",
            "[epoch 7] loss: 0.7334, acc: 0.6736\n",
            "[epoch 7] loss: 0.7323, acc: 0.6740\n",
            "[epoch 7] loss: 0.7348, acc: 0.6730\n",
            "[epoch 7] loss: 0.7333, acc: 0.6732\n",
            "[epoch 7] loss: 0.7348, acc: 0.6720\n",
            "[epoch 7] loss: 0.7332, acc: 0.6731\n",
            "[epoch 7] loss: 0.7325, acc: 0.6737\n",
            "[epoch 7] loss: 0.7320, acc: 0.6736\n",
            "[epoch 7] loss: 0.7345, acc: 0.6725\n",
            "[epoch 7] loss: 0.7354, acc: 0.6724\n",
            "[epoch 7] loss: 0.7357, acc: 0.6722\n",
            "[epoch 7] loss: 0.7356, acc: 0.6722\n",
            "[epoch 7] loss: 0.7349, acc: 0.6715\n",
            "[epoch 7] loss: 0.7340, acc: 0.6719\n",
            "[epoch 7] loss: 0.7354, acc: 0.6708\n",
            "[epoch 7] loss: 0.7376, acc: 0.6694\n",
            "[epoch 7] loss: 0.7369, acc: 0.6696\n",
            "[epoch 7] loss: 0.7370, acc: 0.6693\n",
            "[epoch 7] loss: 0.7360, acc: 0.6705\n",
            "[epoch 7] loss: 0.7366, acc: 0.6705\n",
            "[epoch 7] loss: 0.7363, acc: 0.6708\n",
            "[epoch 7] loss: 0.7348, acc: 0.6721\n",
            "[epoch 7] loss: 0.7337, acc: 0.6728\n",
            "> val_acc: 0.6539, val_f1: 0.6539\n",
            ">> saved: /content/drive/MyDrive/CS4248/MAMS-ATSA/lstm_models/LSTM_MAMS_val_acc_0.6539\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 8\n",
            "[epoch 8] loss: 0.6871, acc: 0.7200\n",
            "[epoch 8] loss: 0.7300, acc: 0.7025\n",
            "[epoch 8] loss: 0.7174, acc: 0.7015\n",
            "[epoch 8] loss: 0.7123, acc: 0.6989\n",
            "[epoch 8] loss: 0.7255, acc: 0.6861\n",
            "[epoch 8] loss: 0.7241, acc: 0.6886\n",
            "[epoch 8] loss: 0.7209, acc: 0.6861\n",
            "[epoch 8] loss: 0.7257, acc: 0.6826\n",
            "[epoch 8] loss: 0.7332, acc: 0.6758\n",
            "[epoch 8] loss: 0.7403, acc: 0.6704\n",
            "[epoch 8] loss: 0.7345, acc: 0.6709\n",
            "[epoch 8] loss: 0.7427, acc: 0.6676\n",
            "[epoch 8] loss: 0.7499, acc: 0.6606\n",
            "[epoch 8] loss: 0.7449, acc: 0.6647\n",
            "[epoch 8] loss: 0.7428, acc: 0.6677\n",
            "[epoch 8] loss: 0.7399, acc: 0.6692\n",
            "[epoch 8] loss: 0.7398, acc: 0.6704\n",
            "[epoch 8] loss: 0.7441, acc: 0.6675\n",
            "[epoch 8] loss: 0.7455, acc: 0.6665\n",
            "[epoch 8] loss: 0.7459, acc: 0.6663\n",
            "[epoch 8] loss: 0.7429, acc: 0.6668\n",
            "[epoch 8] loss: 0.7428, acc: 0.6665\n",
            "[epoch 8] loss: 0.7405, acc: 0.6676\n",
            "[epoch 8] loss: 0.7394, acc: 0.6697\n",
            "[epoch 8] loss: 0.7378, acc: 0.6717\n",
            "[epoch 8] loss: 0.7371, acc: 0.6723\n",
            "[epoch 8] loss: 0.7371, acc: 0.6716\n",
            "[epoch 8] loss: 0.7353, acc: 0.6722\n",
            "[epoch 8] loss: 0.7350, acc: 0.6706\n",
            "[epoch 8] loss: 0.7321, acc: 0.6722\n",
            "[epoch 8] loss: 0.7308, acc: 0.6725\n",
            "[epoch 8] loss: 0.7275, acc: 0.6747\n",
            "[epoch 8] loss: 0.7276, acc: 0.6753\n",
            "[epoch 8] loss: 0.7281, acc: 0.6754\n",
            "[epoch 8] loss: 0.7256, acc: 0.6776\n",
            "[epoch 8] loss: 0.7267, acc: 0.6780\n",
            "[epoch 8] loss: 0.7258, acc: 0.6787\n",
            "[epoch 8] loss: 0.7251, acc: 0.6798\n",
            "[epoch 8] loss: 0.7250, acc: 0.6800\n",
            "[epoch 8] loss: 0.7241, acc: 0.6808\n",
            "[epoch 8] loss: 0.7241, acc: 0.6807\n",
            "[epoch 8] loss: 0.7233, acc: 0.6810\n",
            "[epoch 8] loss: 0.7238, acc: 0.6800\n",
            "[epoch 8] loss: 0.7241, acc: 0.6792\n",
            "[epoch 8] loss: 0.7240, acc: 0.6791\n",
            "> val_acc: 0.6509, val_f1: 0.6509\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 9\n",
            "[epoch 9] loss: 0.6488, acc: 0.7150\n",
            "[epoch 9] loss: 0.6753, acc: 0.6978\n",
            "[epoch 9] loss: 0.6938, acc: 0.6929\n",
            "[epoch 9] loss: 0.6921, acc: 0.6863\n",
            "[epoch 9] loss: 0.6936, acc: 0.6808\n",
            "[epoch 9] loss: 0.6945, acc: 0.6828\n",
            "[epoch 9] loss: 0.6925, acc: 0.6888\n",
            "[epoch 9] loss: 0.6935, acc: 0.6923\n",
            "[epoch 9] loss: 0.6946, acc: 0.6918\n",
            "[epoch 9] loss: 0.6932, acc: 0.6951\n",
            "[epoch 9] loss: 0.6982, acc: 0.6941\n",
            "[epoch 9] loss: 0.7028, acc: 0.6905\n",
            "[epoch 9] loss: 0.7013, acc: 0.6913\n",
            "[epoch 9] loss: 0.7029, acc: 0.6907\n",
            "[epoch 9] loss: 0.7043, acc: 0.6905\n",
            "[epoch 9] loss: 0.6990, acc: 0.6929\n",
            "[epoch 9] loss: 0.6963, acc: 0.6950\n",
            "[epoch 9] loss: 0.6980, acc: 0.6933\n",
            "[epoch 9] loss: 0.7036, acc: 0.6889\n",
            "[epoch 9] loss: 0.7034, acc: 0.6883\n",
            "[epoch 9] loss: 0.7056, acc: 0.6873\n",
            "[epoch 9] loss: 0.7057, acc: 0.6859\n",
            "[epoch 9] loss: 0.7091, acc: 0.6846\n",
            "[epoch 9] loss: 0.7079, acc: 0.6844\n",
            "[epoch 9] loss: 0.7099, acc: 0.6848\n",
            "[epoch 9] loss: 0.7114, acc: 0.6840\n",
            "[epoch 9] loss: 0.7092, acc: 0.6854\n",
            "[epoch 9] loss: 0.7062, acc: 0.6872\n",
            "[epoch 9] loss: 0.7068, acc: 0.6875\n",
            "[epoch 9] loss: 0.7089, acc: 0.6867\n",
            "[epoch 9] loss: 0.7080, acc: 0.6870\n",
            "[epoch 9] loss: 0.7090, acc: 0.6864\n",
            "[epoch 9] loss: 0.7077, acc: 0.6876\n",
            "[epoch 9] loss: 0.7091, acc: 0.6872\n",
            "[epoch 9] loss: 0.7119, acc: 0.6856\n",
            "[epoch 9] loss: 0.7119, acc: 0.6847\n",
            "[epoch 9] loss: 0.7116, acc: 0.6847\n",
            "[epoch 9] loss: 0.7124, acc: 0.6851\n",
            "[epoch 9] loss: 0.7142, acc: 0.6842\n",
            "[epoch 9] loss: 0.7151, acc: 0.6831\n",
            "[epoch 9] loss: 0.7157, acc: 0.6826\n",
            "[epoch 9] loss: 0.7155, acc: 0.6833\n",
            "[epoch 9] loss: 0.7156, acc: 0.6830\n",
            "[epoch 9] loss: 0.7154, acc: 0.6826\n",
            "[epoch 9] loss: 0.7144, acc: 0.6835\n",
            "> val_acc: 0.6486, val_f1: 0.6486\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 10\n",
            "[epoch 10] loss: 0.7319, acc: 0.6640\n",
            "[epoch 10] loss: 0.7012, acc: 0.6820\n",
            "[epoch 10] loss: 0.6894, acc: 0.6960\n",
            "[epoch 10] loss: 0.6944, acc: 0.6810\n",
            "[epoch 10] loss: 0.6903, acc: 0.6880\n",
            "[epoch 10] loss: 0.7019, acc: 0.6853\n",
            "[epoch 10] loss: 0.6970, acc: 0.6909\n",
            "[epoch 10] loss: 0.6997, acc: 0.6910\n",
            "[epoch 10] loss: 0.7091, acc: 0.6858\n",
            "[epoch 10] loss: 0.7114, acc: 0.6852\n",
            "[epoch 10] loss: 0.7139, acc: 0.6844\n",
            "[epoch 10] loss: 0.7098, acc: 0.6863\n",
            "[epoch 10] loss: 0.7054, acc: 0.6898\n",
            "[epoch 10] loss: 0.7032, acc: 0.6923\n",
            "[epoch 10] loss: 0.7054, acc: 0.6923\n",
            "[epoch 10] loss: 0.7022, acc: 0.6955\n",
            "[epoch 10] loss: 0.7024, acc: 0.6951\n",
            "[epoch 10] loss: 0.7020, acc: 0.6951\n",
            "[epoch 10] loss: 0.7031, acc: 0.6947\n",
            "[epoch 10] loss: 0.7052, acc: 0.6914\n",
            "[epoch 10] loss: 0.7069, acc: 0.6897\n",
            "[epoch 10] loss: 0.7062, acc: 0.6898\n",
            "[epoch 10] loss: 0.7092, acc: 0.6877\n",
            "[epoch 10] loss: 0.7091, acc: 0.6873\n",
            "[epoch 10] loss: 0.7100, acc: 0.6859\n",
            "[epoch 10] loss: 0.7066, acc: 0.6880\n",
            "[epoch 10] loss: 0.7059, acc: 0.6880\n",
            "[epoch 10] loss: 0.7096, acc: 0.6860\n",
            "[epoch 10] loss: 0.7076, acc: 0.6862\n",
            "[epoch 10] loss: 0.7080, acc: 0.6853\n",
            "[epoch 10] loss: 0.7087, acc: 0.6844\n",
            "[epoch 10] loss: 0.7070, acc: 0.6857\n",
            "[epoch 10] loss: 0.7064, acc: 0.6859\n",
            "[epoch 10] loss: 0.7082, acc: 0.6852\n",
            "[epoch 10] loss: 0.7078, acc: 0.6862\n",
            "[epoch 10] loss: 0.7081, acc: 0.6863\n",
            "[epoch 10] loss: 0.7084, acc: 0.6864\n",
            "[epoch 10] loss: 0.7072, acc: 0.6871\n",
            "[epoch 10] loss: 0.7077, acc: 0.6870\n",
            "[epoch 10] loss: 0.7073, acc: 0.6869\n",
            "[epoch 10] loss: 0.7069, acc: 0.6880\n",
            "[epoch 10] loss: 0.7077, acc: 0.6872\n",
            "[epoch 10] loss: 0.7063, acc: 0.6881\n",
            "[epoch 10] loss: 0.7048, acc: 0.6890\n",
            "> val_acc: 0.6456, val_f1: 0.6456\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 11\n",
            "[epoch 11] loss: 0.7253, acc: 0.6800\n",
            "[epoch 11] loss: 0.6701, acc: 0.7100\n",
            "[epoch 11] loss: 0.6777, acc: 0.6982\n",
            "[epoch 11] loss: 0.6796, acc: 0.6913\n",
            "[epoch 11] loss: 0.6909, acc: 0.6933\n",
            "[epoch 11] loss: 0.6822, acc: 0.6962\n",
            "[epoch 11] loss: 0.6743, acc: 0.7000\n",
            "[epoch 11] loss: 0.6765, acc: 0.6989\n",
            "[epoch 11] loss: 0.6912, acc: 0.6941\n",
            "[epoch 11] loss: 0.6868, acc: 0.6974\n",
            "[epoch 11] loss: 0.6820, acc: 0.7020\n",
            "[epoch 11] loss: 0.6847, acc: 0.7011\n",
            "[epoch 11] loss: 0.6817, acc: 0.7020\n",
            "[epoch 11] loss: 0.6857, acc: 0.6994\n",
            "[epoch 11] loss: 0.6899, acc: 0.6946\n",
            "[epoch 11] loss: 0.6888, acc: 0.6942\n",
            "[epoch 11] loss: 0.6922, acc: 0.6919\n",
            "[epoch 11] loss: 0.6941, acc: 0.6891\n",
            "[epoch 11] loss: 0.6920, acc: 0.6899\n",
            "[epoch 11] loss: 0.6890, acc: 0.6902\n",
            "[epoch 11] loss: 0.6899, acc: 0.6897\n",
            "[epoch 11] loss: 0.6921, acc: 0.6887\n",
            "[epoch 11] loss: 0.6928, acc: 0.6888\n",
            "[epoch 11] loss: 0.6972, acc: 0.6876\n",
            "[epoch 11] loss: 0.6974, acc: 0.6878\n",
            "[epoch 11] loss: 0.6975, acc: 0.6879\n",
            "[epoch 11] loss: 0.6937, acc: 0.6910\n",
            "[epoch 11] loss: 0.6930, acc: 0.6924\n",
            "[epoch 11] loss: 0.6936, acc: 0.6929\n",
            "[epoch 11] loss: 0.6961, acc: 0.6919\n",
            "[epoch 11] loss: 0.6949, acc: 0.6927\n",
            "[epoch 11] loss: 0.6943, acc: 0.6942\n",
            "[epoch 11] loss: 0.6925, acc: 0.6945\n",
            "[epoch 11] loss: 0.6934, acc: 0.6942\n",
            "[epoch 11] loss: 0.6933, acc: 0.6940\n",
            "[epoch 11] loss: 0.6934, acc: 0.6943\n",
            "[epoch 11] loss: 0.6950, acc: 0.6929\n",
            "[epoch 11] loss: 0.6937, acc: 0.6937\n",
            "[epoch 11] loss: 0.6944, acc: 0.6935\n",
            "[epoch 11] loss: 0.6963, acc: 0.6933\n",
            "[epoch 11] loss: 0.6972, acc: 0.6932\n",
            "[epoch 11] loss: 0.6969, acc: 0.6937\n",
            "[epoch 11] loss: 0.6960, acc: 0.6947\n",
            "[epoch 11] loss: 0.6983, acc: 0.6932\n",
            "[epoch 11] loss: 0.6977, acc: 0.6933\n",
            "> val_acc: 0.6494, val_f1: 0.6494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "epoch: 12\n",
            "[epoch 12] loss: 0.7019, acc: 0.7100\n",
            "[epoch 12] loss: 0.7212, acc: 0.6657\n",
            "[epoch 12] loss: 0.6937, acc: 0.6900\n",
            "[epoch 12] loss: 0.6796, acc: 0.6918\n",
            "[epoch 12] loss: 0.6788, acc: 0.6936\n",
            "[epoch 12] loss: 0.6729, acc: 0.6978\n",
            "[epoch 12] loss: 0.6835, acc: 0.6956\n",
            "[epoch 12] loss: 0.6864, acc: 0.6962\n",
            "[epoch 12] loss: 0.6775, acc: 0.7000\n",
            "[epoch 12] loss: 0.6798, acc: 0.6987\n",
            "[epoch 12] loss: 0.6837, acc: 0.6973\n",
            "[epoch 12] loss: 0.6802, acc: 0.7021\n",
            "[epoch 12] loss: 0.6864, acc: 0.6984\n",
            "[epoch 12] loss: 0.6865, acc: 0.6991\n",
            "[epoch 12] loss: 0.6850, acc: 0.7008\n",
            "[epoch 12] loss: 0.6854, acc: 0.6997\n",
            "[epoch 12] loss: 0.6893, acc: 0.6983\n",
            "[epoch 12] loss: 0.6854, acc: 0.7002\n",
            "[epoch 12] loss: 0.6867, acc: 0.6998\n",
            "[epoch 12] loss: 0.6833, acc: 0.7010\n",
            "[epoch 12] loss: 0.6814, acc: 0.7010\n",
            "[epoch 12] loss: 0.6788, acc: 0.7030\n",
            "[epoch 12] loss: 0.6780, acc: 0.7032\n",
            "[epoch 12] loss: 0.6802, acc: 0.7024\n",
            "[epoch 12] loss: 0.6823, acc: 0.6997\n",
            "[epoch 12] loss: 0.6829, acc: 0.6994\n",
            "[epoch 12] loss: 0.6841, acc: 0.6997\n",
            "[epoch 12] loss: 0.6848, acc: 0.7003\n",
            "[epoch 12] loss: 0.6868, acc: 0.6989\n",
            "[epoch 12] loss: 0.6867, acc: 0.6989\n",
            "[epoch 12] loss: 0.6864, acc: 0.6980\n",
            "[epoch 12] loss: 0.6854, acc: 0.6983\n",
            "[epoch 12] loss: 0.6852, acc: 0.6994\n",
            "[epoch 12] loss: 0.6871, acc: 0.6994\n",
            "[epoch 12] loss: 0.6868, acc: 0.6986\n",
            "[epoch 12] loss: 0.6876, acc: 0.6985\n",
            "[epoch 12] loss: 0.6873, acc: 0.6988\n",
            "[epoch 12] loss: 0.6872, acc: 0.6984\n",
            "[epoch 12] loss: 0.6879, acc: 0.6986\n",
            "[epoch 12] loss: 0.6883, acc: 0.6976\n",
            "[epoch 12] loss: 0.6864, acc: 0.6983\n",
            "[epoch 12] loss: 0.6877, acc: 0.6980\n",
            "[epoch 12] loss: 0.6881, acc: 0.6979\n",
            "[epoch 12] loss: 0.6886, acc: 0.6973\n",
            "[epoch 12] loss: 0.6883, acc: 0.6985\n",
            "> val_acc: 0.6486, val_f1: 0.6486\n",
            ">> early stop.\n",
            ">> test_acc: 0.6542\n",
            ">> test_f1_macro: 0.6342\n",
            ">> test_f1_micro: 0.6542\n",
            ">> test_f1_weighted: 0.6498\n",
            ">> test_precision_macro: 0.6422\n",
            ">> test_precision_micro: 0.6542\n",
            ">> test_precision_weighted: 0.6494\n",
            ">> test_recall_macro: 0.6298\n",
            ">> test_recall_micro: 0.6542\n",
            ">> test_recall_weighted: 0.6542\n",
            "confusion matrix:\n",
            "[[198  77  54]\n",
            " [ 38 472  97]\n",
            " [ 65 131 204]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}